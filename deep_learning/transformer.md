# Transformer

## 한 줄 정의
> Self-Attention 메커니즘을 기반으로 순차 데이터를 병렬 처리하는 신경망 아키텍처

## 직관적 이해

Transformer는 "회의" 비유로 이해할 수 있다:
- 각 토큰이 회의 참석자
- Self-Attention: 각 참석자가 다른 참석자들의 의견을 참고해서 자기 의견 업데이트
- 모든 참석자가 동시에 소통 가능 (병렬 처리)

RNN과의 차이:
- RNN: 순차적으로 한 명씩 발언 → 느림, 긴 문장에서 앞 내용 잊음
- Transformer: 모두 동시에 소통 → 빠름, 긴 문장도 OK

---

## 1. Self-Attention

### 핵심 개념
각 토큰이 다른 토큰들을 얼마나 참고할지 가중치를 계산

- **Q (Query)**: 내가 찾고 싶은 것
- **K (Key)**: 다른 토큰들의 라벨
- **V (Value)**: 실제 정보

### 수학적 원리

```
Attention(Q, K, V) = softmax(QK^T / √d_k) · V
```

**차원 변화:**
```
입력 X: (seq_len, d_model)  예: (10, 512)

Q = X · W_Q: (10, 512) · (512, 64) = (10, 64)
K = X · W_K: (10, 512) · (512, 64) = (10, 64)
V = X · W_V: (10, 512) · (512, 64) = (10, 64)

QK^T: (10, 64) · (64, 10) = (10, 10)  ← Attention Score 행렬
softmax(QK^T / √64): (10, 10)  ← 각 행의 합 = 1
· V: (10, 10) · (10, 64) = (10, 64)  ← 최종 출력
```

### √d_k로 나누는 이유

Q, K의 각 원소가 평균 0, 분산 1이면:
- Q·K 내적의 분산 = d_k
- d_k가 크면 내적값이 커짐
- softmax 입력이 크면 → 극단적 출력 (거의 one-hot)
- gradient가 0에 가까워짐 (학습 불안정)

**해결**: √d_k로 나눠서 분산을 1로 정규화

---

## 2. Multi-Head Attention

### 핵심 개념
여러 관점에서 attention 수행

```
Head 1: 주어-동사 관계
Head 2: 시간 관계
Head 3: 수식어 관계
...
```

### 구조
```
입력 X (512차원)
    ↓
┌─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┐
│Head1│Head2│Head3│Head4│Head5│Head6│Head7│Head8│  (각 64차원)
└──┬──┴──┬──┴──┬──┴──┬──┴──┬──┴──┬──┴──┬──┴──┬──┘
                Concat (512차원)
                    ↓
              Linear (512 → 512)
                    ↓
                  출력
```

### 왜 여러 Head?
- 다양한 관계를 병렬로 포착
- 단일 Head는 한 종류의 관계만 학습
- 8개 Head = 8가지 다른 관점

---

## 3. Positional Encoding

### 왜 필요한가?
- Attention은 순서 정보가 없음 (병렬 처리)
- "나는 밥을 먹는다" vs "밥을 나는 먹는다" 구분 불가
- 위치 정보를 별도로 주입해야 함

### sin/cos PE

**수식:**
```
PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

**특징:**
- 각 위치마다 고유한 패턴 (신분증)
- 저차원: 저주파 (멀리 떨어진 위치 구별)
- 고차원: 고주파 (가까운 위치 정밀 구별)

**상대 위치 계산:**
- PE(pos+k)는 PE(pos)의 선형 변환으로 표현 가능
- 삼각함수 덧셈 정리 활용
- 모델이 상대 위치를 학습할 수 있음

### RoPE (Rotary Position Embedding)

**sin/cos PE의 문제:**
- PE가 입력에 더해진 후 W_Q 거침
- 위치 정보가 의미 정보와 섞여서 희석

**RoPE 해결:**
- W_Q 이후에 위치별 회전 적용
- 위치 정보가 정확하게 보존
- Q·K 결과가 상대 위치에만 의존

**비교:**
| | sin/cos PE | RoPE |
|--|-----------|------|
| 적용 시점 | 입력에 더함 | Q, K에 회전 |
| 위치 정보 | 간접적 | 직접적 |
| 외삽 성능 | 약함 | 강함 |

---

## 4. Feed-Forward Network (FFN)

### 핵심 개념
Attention 이후 각 토큰에 독립적으로 적용되는 2층 MLP

### 수식
```
FFN(x) = ReLU(xW₁ + b₁)W₂ + b₂
```

### 차원
```
x: (seq_len, 512)
W₁: (512, 2048)  ← 4배 확장
W₂: (2048, 512)  ← 다시 축소
```

### 역할
- Attention: 토큰 간 관계 (회의에서 정보 교환)
- FFN: 토큰별 변환 (각자 방에서 정보 정리)

### 왜 4배 확장?
- 더 풍부한 표현을 중간에 만들고
- 정보를 압축해서 전달

---

## 5. Layer Normalization

### 왜 Batch Norm 안 쓰고 Layer Norm?

| | Batch Norm | Layer Norm |
|--|-----------|------------|
| 정규화 방향 | 배치 전체 | 각 샘플 내 |
| 시퀀스 길이 | 고정 필요 | 가변 OK |
| 추론 시 | 이동 평균 필요 | 필요 없음 |

텍스트는 길이가 다 다르니까 Layer Norm!

### 수식
```
LayerNorm(x) = γ · (x - μ) / σ + β

μ = mean(x)  (해당 토큰의 모든 feature)
σ = std(x)
γ, β = 학습 파라미터
```

---

## 6. Residual Connection

### 수식
```
output = LayerNorm(x + SubLayer(x))
```

### 왜 필요?
깊은 네트워크 문제:
- Gradient Vanishing: gradient가 0에 가까워짐
- Degradation: 층이 깊어져도 성능 안 오름

Residual 해결:
- gradient에 최소한 1이 보장됨
- "최소한 입력은 보존됨" → 학습 안정

---

## 7. Encoder Block 전체 구조

```
입력 X
   ↓
┌──────────────────┐
│  Multi-Head      │
│  Self-Attention  │
└────────┬─────────┘
         ↓
      Add (x + )
         ↓
    Layer Norm
         ↓
┌──────────────────┐
│      FFN         │
└────────┬─────────┘
         ↓
      Add (+ )
         ↓
    Layer Norm
         ↓
      출력
```

이걸 N번 쌓으면 Encoder (보통 6~12개)

---

## Q&A

**Q: Self-Attention이 뭐예요?**
A: 각 토큰이 다른 토큰들을 얼마나 참고할지 가중치를 계산하는 메커니즘

**Q: √d_k 왜 나눠요?**
A: 내적값의 분산을 정규화해서 softmax 안정화

**Q: Multi-Head 왜 써요?**
A: 다양한 관계(주어-동사, 시간, 수식어 등)를 병렬로 포착

**Q: PE 왜 필요해요?**
A: Attention은 순서 모름, 위치 정보 주입 필요

**Q: RoPE 장점은?**
A: 상대 위치 자동 계산, 긴 시퀀스 외삽 성능 좋음

**Q: FFN 역할은?**
A: 토큰별 비선형 변환 (Attention은 토큰 간 관계)

**Q: Layer Norm 왜 써요?**
A: 가변 길이 대응, 추론 시 추가 통계 불필요

**Q: Residual 왜 써요?**
A: Gradient 흐름 유지, 학습 안정화
