# Transformer 퀴즈 기록

> 날짜: 2024-12-19

## 결과 요약

| 점수 | 1/6 |
|------|-----|
| 상태 | 재학습 필요 |

---

## 상세 기록

### Q1. Self-Attention이 뭐야?
- **내 답변**: "RNN 소실문제 해결, 모든 단어간 연관성 계산"
- **판정**: ❌
- **부족한 점**: 병렬 처리 언급 없음, Q·K·V 동작 없음
- **정답 키워드**: 각 토큰이 다른 모든 토큰을 얼마나 참고할지 가중치 계산, 병렬 처리

### Q2. Q, K, V가 뭐야?
- **내 답변**: "query는 요청, key는 메타데이터, value는 실제값"
- **판정**: ❌
- **부족한 점**: 동작 흐름 없음
- **정답 키워드**: Q·K 내적 → attention score → softmax → V 가중합

### Q3. √d_k로 왜 나눠?
- **내 답변**: 모름
- **판정**: ❌
- **정답 키워드**: d_k 클수록 내적 분산 커짐 → softmax 극단화 (one-hot) → gradient ≈ 0 → 학습 불안정

### Q4. Multi-Head 왜 써?
- **내 답변**: "여러 패턴으로 봐야해서"
- **판정**: ❌
- **부족한 점**: 구체적 예시 없음
- **정답 키워드**: 다양한 관계(주어-동사, 시간, 수식어 등) 병렬 포착, Single Head는 한 가지 관계만 학습

### Q5. Positional Encoding 왜 필요해?
- **내 답변**: "위치정보 소실, 위치 전달 필요"
- **판정**: ✅
- **정답 키워드**: Self-Attention은 순서 개념 없음, 위치 정보 별도 주입 필요

### Q6. sin/cos PE vs RoPE 차이는?
- **내 답변**: "sin/cos은 희석됨"
- **판정**: ❌
- **부족한 점**: 적용 시점 차이 없음
- **정답 키워드**:
  - sin/cos: 입력에 더함 (W_Q 전) → 희석
  - RoPE: Q, K에 회전 (W_Q 후) → 직접 보존, 상대 위치 의존

---

## 다음 학습 계획

- [ ] Self-Attention 재학습 (Q·K·V 동작 흐름 중심)
- [ ] √d_k 스케일링 재학습
- [ ] Multi-Head 구체적 예시 암기
- [ ] sin/cos vs RoPE 적용 시점 차이 암기
