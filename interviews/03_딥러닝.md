# 딥러닝

> 면접에서 자주 나오는 딥러닝 질문들

## 질문 목록

### 기본 개념
- [ ] 딥러닝은 무엇인가요?
- [ ] 왜 갑자기 딥러닝이 부흥했을까요?
- [ ] 마지막으로 읽은 논문은 무엇인가요?
- [ ] Cost Function과 Activation Function은 무엇인가요?
- [ ] Data Normalization은 무엇이고 왜 필요한가요?

### Activation Function
- [ ] 알고있는 Activation Function에 대해 알려주세요
- [ ] 요즘 Sigmoid 보다 ReLU를 많이 쓰는 이유는?
- [ ] Non-Linearity라는 말의 의미와 그 필요성은?
- [ ] ReLU로 어떻게 곡선 함수를 근사하나?
- [ ] ReLU의 문제점은?
- [ ] Bias는 왜 있는걸까?

### 학습 과정
- [ ] Gradient Descent에 대해서 쉽게 설명한다면?
- [ ] 왜 꼭 Gradient를 써야 할까?
- [ ] GD 중에 때때로 Loss가 증가하는 이유는?
- [ ] 중학생이 이해할 수 있게 더 쉽게 설명 한다면?
- [ ] Back Propagation에 대해서 쉽게 설명 한다면?
- [ ] Local Minima 문제에도 불구하고 딥러닝이 잘 되는 이유는?
- [ ] GD가 Local Minima 문제를 피하는 방법은?
- [ ] 찾은 해가 Global Minimum인지 아닌지 알 수 있는 방법은?

### 데이터 분할
- [ ] Training 세트와 Test 세트를 분리하는 이유는?
- [ ] Validation 세트가 따로 있는 이유는?
- [ ] Test 세트가 오염되었다는 말의 뜻은?

### 정규화 기법
- [ ] 오버피팅일 경우 어떻게 대처해야 할까요?
- [ ] Regularization이란 무엇인가?
- [ ] Batch Normalization의 효과는?
- [ ] Dropout의 효과는?
- [ ] BN 적용해서 학습 이후 실제 사용시에 주의할 점은?
- [ ] GAN에서 Generator 쪽에도 BN을 적용해도 될까?

### Optimizer
- [ ] SGD, RMSprop, Adam에 대해서 아는대로 설명한다면?
- [ ] SGD에서 Stochastic의 의미는?
- [ ] 미니배치를 작게 할때의 장단점은?
- [ ] 모멘텀의 수식을 적어 본다면?

### 초기화/하이퍼파라미터
- [ ] 하이퍼 파라미터는 무엇인가요?
- [ ] Weight Initialization 방법에 대해 말해주세요
- [ ] 볼츠만 머신은 무엇인가요?

### 프레임워크/구현
- [ ] Tensorflow, Keras, PyTorch, Caffe, Mxnet 중 선호하는 프레임워크는?
- [ ] 간단한 MNIST 분류기를 MLP+CPU 버전으로 numpy로 만든다면?
- [ ] 어느 정도 돌아가는 녀석을 작성하기까지 몇시간 정도 걸릴까?
- [ ] Back Propagation은 몇줄인가?
- [ ] CNN으로 바꾼다면 얼마나 추가될까?
- [ ] 간단한 MNIST 분류기를 TF, Keras, PyTorch 등으로 작성하는데?
- [ ] CNN이 아닌 MLP로 해도 잘 될까?
- [ ] 마지막 레이어 부분에 대해서 설명 한다면?
- [ ] 학습은 BCE loss로 하되 상황을 MSE loss로 보고 싶다면?
- [ ] 만약 한글 (인쇄물) OCR을 만든다면 데이터 수집은?

### GPU/하드웨어
- [ ] 딥러닝할 때 GPU를 쓰면 좋은 이유는?
- [ ] 학습 중인데 GPU를 100% 사용하지 않고 있다
- [ ] GPU를 두개 다 쓰고 싶다
- [ ] 학습시 필요한 GPU 메모리는 어떻게 계산하는가?

### 디버깅/실무
- [ ] TF, Keras, PyTorch 등을 사용할 때 디버깅 노하우는?
- [ ] 뉴럴넷의 가장 큰 단점은 무엇인가?
- [ ] One-Shot Learning은 무엇인가?

---

## 답변

### 딥러닝은 무엇인가요?

**답변 예정**

---

### 왜 갑자기 딥러닝이 부흥했을까요?

**답변 예정**

---

### Cost Function과 Activation Function은 무엇인가요?

**답변 예정**

---

### Data Normalization은 무엇이고 왜 필요한가요?

**답변 예정**

---

### 알고있는 Activation Function에 대해 알려주세요

**답변 예정**

---

### 요즘 Sigmoid 보다 ReLU를 많이 쓰는 이유는?

**답변 예정**

---

### Non-Linearity라는 말의 의미와 그 필요성은?

**답변 예정**

---

### ReLU로 어떻게 곡선 함수를 근사하나?

**답변 예정**

---

### ReLU의 문제점은?

**답변 예정**

---

### Bias는 왜 있는걸까?

**답변 예정**

---

### Gradient Descent에 대해서 쉽게 설명한다면?

**답변 예정**

---

### 왜 꼭 Gradient를 써야 할까?

**답변 예정**

---

### GD 중에 때때로 Loss가 증가하는 이유는?

**답변 예정**

---

### Back Propagation에 대해서 쉽게 설명 한다면?

**답변 예정**

---

### Local Minima 문제에도 불구하고 딥러닝이 잘 되는 이유는?

**답변 예정**

---

### GD가 Local Minima 문제를 피하는 방법은?

**답변 예정**

---

### Training 세트와 Test 세트를 분리하는 이유는?

**답변 예정**

---

### Validation 세트가 따로 있는 이유는?

**답변 예정**

---

### Test 세트가 오염되었다는 말의 뜻은?

**답변 예정**

---

### 오버피팅일 경우 어떻게 대처해야 할까요?

**답변 예정**

---

### Regularization이란 무엇인가?

**답변 예정**

---

### Batch Normalization의 효과는?

**답변 예정**

---

### Dropout의 효과는?

**답변 예정**

---

### BN 적용해서 학습 이후 실제 사용시에 주의할 점은?

**답변 예정**

---

### SGD, RMSprop, Adam에 대해서 아는대로 설명한다면?

**답변 예정**

---

### SGD에서 Stochastic의 의미는?

**답변 예정**

---

### 미니배치를 작게 할때의 장단점은?

**답변 예정**

---

### 모멘텀의 수식을 적어 본다면?

**답변 예정**

---

### 하이퍼 파라미터는 무엇인가요?

**답변 예정**

---

### Weight Initialization 방법에 대해 말해주세요

**답변 예정**

---

### 딥러닝할 때 GPU를 쓰면 좋은 이유는?

**답변 예정**

---

### 학습시 필요한 GPU 메모리는 어떻게 계산하는가?

**답변 예정**

---

### 뉴럴넷의 가장 큰 단점은 무엇인가?

**답변 예정**

---

### One-Shot Learning은 무엇인가?

**답변 예정**
