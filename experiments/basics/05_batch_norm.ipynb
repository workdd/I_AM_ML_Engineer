{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 05. Batch Normalization\n",
        "\n",
        "## í•™ìŠµ ëª©í‘œ\n",
        "- Batch Normalizationì˜ ì›ë¦¬ ì´í•´\n",
        "- Internal Covariate Shift ê°œë…\n",
        "- í•™ìŠµ/ì¶”ë¡  ì‹œ ë™ì‘ ì°¨ì´\n",
        "- Layer Normalizationê³¼ì˜ ë¹„êµ\n",
        "\n",
        "## ê´€ë ¨ ë©´ì ‘ ì§ˆë¬¸\n",
        "- BatchNormì´ ì™œ í•™ìŠµì„ ë¹ ë¥´ê²Œ í•˜ë‚˜ìš”?\n",
        "- ì¶”ë¡  ì‹œ BatchNormì€ ì–´ë–»ê²Œ ë™ì‘í•˜ë‚˜ìš”?\n",
        "- BatchNorm vs LayerNorm ì–¸ì œ ê°ê° ì‚¬ìš©í•˜ë‚˜ìš”?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Pre-Quiz\n",
        "\n",
        "### Q1. Internal Covariate Shiftë€?\n",
        "```\n",
        "ë‹¹ì‹ ì˜ ë‹µ: \n",
        "```\n",
        "\n",
        "### Q2. BatchNormì—ì„œ Î³, Î²ì˜ ì—­í• ì€?\n",
        "```\n",
        "ë‹¹ì‹ ì˜ ë‹µ: \n",
        "```\n",
        "\n",
        "### Q3. ì¶”ë¡  ì‹œ batch í¬ê¸°ê°€ 1ì´ë©´ BatchNormì€ ì–´ë–»ê²Œ ë™ì‘í•˜ë‚˜ìš”?\n",
        "```\n",
        "ë‹¹ì‹ ì˜ ë‹µ: \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# matplotlib í•œê¸€ ì„¤ì • (macOS)\n",
        "plt.rcParams['font.family'] = 'AppleGothic'\n",
        "plt.rcParams['axes.unicode_minus'] = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Batch Normalization ìˆ˜ì‹\n",
        "\n",
        "### Forward\n",
        "$$\\mu_B = \\frac{1}{m} \\sum_{i=1}^{m} x_i$$\n",
        "$$\\sigma^2_B = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_B)^2$$\n",
        "$$\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma^2_B + \\epsilon}}$$\n",
        "$$y_i = \\gamma \\hat{x}_i + \\beta$$\n",
        "\n",
        "### í•µì‹¬ í¬ì¸íŠ¸\n",
        "- ê° **feature(channel)** ë³„ë¡œ ì •ê·œí™”\n",
        "- **batch** ì°¨ì›ì—ì„œ mean, variance ê³„ì‚°\n",
        "- Î³, Î²ëŠ” í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =================================================\n",
        "# TODO: Batch Normalization êµ¬í˜„\n",
        "# =================================================\n",
        "\n",
        "class BatchNorm1d:\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_features: feature ìˆ˜ (channel ìˆ˜)\n",
        "            eps: ë¶„ëª¨ 0 ë°©ì§€ìš© ì‘ì€ ê°’\n",
        "            momentum: running mean/var ì—…ë°ì´íŠ¸ ë¹„ìœ¨\n",
        "        \"\"\"\n",
        "        self.eps = eps\n",
        "        self.momentum = momentum\n",
        "        \n",
        "        # í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°\n",
        "        self.gamma = np.ones(num_features)   # scale\n",
        "        self.beta = np.zeros(num_features)   # shift\n",
        "        \n",
        "        # Running statistics (ì¶”ë¡  ì‹œ ì‚¬ìš©)\n",
        "        self.running_mean = np.zeros(num_features)\n",
        "        self.running_var = np.ones(num_features)\n",
        "        \n",
        "        # í•™ìŠµ/ì¶”ë¡  ëª¨ë“œ\n",
        "        self.training = True\n",
        "        \n",
        "        # Backwardìš© ìºì‹œ\n",
        "        self.cache = None\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch, features)\n",
        "        Returns:\n",
        "            out: (batch, features)\n",
        "        \"\"\"\n",
        "        # ============================================\n",
        "        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
        "        #\n",
        "        # training=True:\n",
        "        #   1. batch mean, var ê³„ì‚°\n",
        "        #   2. running mean/var ì—…ë°ì´íŠ¸\n",
        "        #   3. ì •ê·œí™”: x_hat = (x - mean) / sqrt(var + eps)\n",
        "        #   4. scale & shift: y = gamma * x_hat + beta\n",
        "        #   5. cache ì €ì¥ (backwardìš©)\n",
        "        #\n",
        "        # training=False:\n",
        "        #   running mean/var ì‚¬ìš©\n",
        "        # ============================================\n",
        "        pass\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dout: (batch, features)\n",
        "        Returns:\n",
        "            dx: (batch, features)\n",
        "        \"\"\"\n",
        "        # ============================================\n",
        "        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš” (ë‚œì´ë„ ë†’ìŒ)\n",
        "        #\n",
        "        # íŒíŠ¸: ìºì‹œì—ì„œ í•„ìš”í•œ ê°’ ê°€ì ¸ì˜¤ê¸°\n",
        "        # x_hat, std = self.cache\n",
        "        # \n",
        "        # dgamma = sum(dout * x_hat, axis=0)\n",
        "        # dbeta = sum(dout, axis=0)\n",
        "        # dx_hat = dout * gamma\n",
        "        # dx = (dx_hat - mean(dx_hat) - x_hat * mean(dx_hat * x_hat)) / std\n",
        "        # ============================================\n",
        "        pass\n",
        "    \n",
        "    def train(self):\n",
        "        self.training = True\n",
        "    \n",
        "    def eval(self):\n",
        "        self.training = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============ í…ŒìŠ¤íŠ¸ ============\n",
        "def test_batch_norm():\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    bn = BatchNorm1d(num_features=4)\n",
        "    \n",
        "    # ë‹¤ì–‘í•œ scaleì˜ ì…ë ¥\n",
        "    x = np.random.randn(32, 4) * np.array([1, 10, 100, 1000])\n",
        "    \n",
        "    # Training mode\n",
        "    bn.train()\n",
        "    out = bn.forward(x)\n",
        "    \n",
        "    # ì¶œë ¥ì´ ì •ê·œí™”ë˜ì—ˆëŠ”ì§€ í™•ì¸ (meanâ‰ˆ0, stdâ‰ˆ1)\n",
        "    out_mean = np.mean(out, axis=0)\n",
        "    out_std = np.std(out, axis=0)\n",
        "    \n",
        "    assert np.allclose(out_mean, 0, atol=1e-6), f\"Mean ì˜¤ë¥˜: {out_mean}\"\n",
        "    assert np.allclose(out_std, 1, atol=0.1), f\"Std ì˜¤ë¥˜: {out_std}\"\n",
        "    \n",
        "    print(\"âœ… BatchNorm í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
        "    print(f\"   Input std per feature: {np.std(x, axis=0)}\")\n",
        "    print(f\"   Output mean: {out_mean}\")\n",
        "    print(f\"   Output std: {out_std}\")\n",
        "\n",
        "test_batch_norm()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. BatchNorm íš¨ê³¼ ì‹œê°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_batchnorm_effect():\n",
        "    \"\"\"BatchNorm ì ìš© ì „í›„ ë¶„í¬ ë¹„êµ\"\"\"\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # ë‹¤ì–‘í•œ ë¶„í¬ì˜ ë°ì´í„°\n",
        "    x1 = np.random.randn(1000) * 5 + 10    # mean=10, std=5\n",
        "    x2 = np.random.randn(1000) * 0.1 - 3   # mean=-3, std=0.1\n",
        "    x3 = np.random.randn(1000) * 20        # mean=0, std=20\n",
        "    \n",
        "    x = np.stack([x1, x2, x3], axis=1)  # (1000, 3)\n",
        "    \n",
        "    bn = BatchNorm1d(3)\n",
        "    bn.train()\n",
        "    x_norm = bn.forward(x)\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
        "    \n",
        "    titles = ['Feature 1 (Î¼=10, Ïƒ=5)', 'Feature 2 (Î¼=-3, Ïƒ=0.1)', 'Feature 3 (Î¼=0, Ïƒ=20)']\n",
        "    \n",
        "    for i in range(3):\n",
        "        # Before\n",
        "        axes[0, i].hist(x[:, i], bins=50, alpha=0.7)\n",
        "        axes[0, i].set_title(f'Before BN: {titles[i]}')\n",
        "        axes[0, i].axvline(x=0, color='r', linestyle='--')\n",
        "        \n",
        "        # After\n",
        "        axes[1, i].hist(x_norm[:, i], bins=50, alpha=0.7, color='green')\n",
        "        axes[1, i].set_title(f'After BN (Î¼â‰ˆ0, Ïƒâ‰ˆ1)')\n",
        "        axes[1, i].axvline(x=0, color='r', linestyle='--')\n",
        "        axes[1, i].set_xlim(-4, 4)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_batchnorm_effect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. í•™ìŠµ vs ì¶”ë¡  ëª¨ë“œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training vs Inference mode ë¹„êµ\n",
        "def compare_train_eval():\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    bn = BatchNorm1d(2)\n",
        "    \n",
        "    # ì—¬ëŸ¬ batchë¡œ í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜\n",
        "    print(\"=== Training Phase ===\")\n",
        "    for i in range(5):\n",
        "        x = np.random.randn(16, 2) * (i + 1) + i  # ì ì  ë‹¤ë¥¸ ë¶„í¬\n",
        "        bn.train()\n",
        "        _ = bn.forward(x)\n",
        "        print(f\"  Batch {i+1}: running_mean = {bn.running_mean}\")\n",
        "    \n",
        "    print(f\"\\nìµœì¢… running_mean: {bn.running_mean}\")\n",
        "    print(f\"ìµœì¢… running_var: {bn.running_var}\")\n",
        "    \n",
        "    # ì¶”ë¡  ëª¨ë“œ\n",
        "    print(\"\\n=== Inference Phase ===\")\n",
        "    bn.eval()\n",
        "    \n",
        "    # ë‹¨ì¼ ìƒ˜í”Œë¡œ í…ŒìŠ¤íŠ¸ (batch=1)\n",
        "    x_single = np.array([[5.0, -2.0]])\n",
        "    out = bn.forward(x_single)\n",
        "    print(f\"  Input: {x_single}\")\n",
        "    print(f\"  Output: {out}\")\n",
        "    print(\"  (running statistics ì‚¬ìš©)\")\n",
        "\n",
        "compare_train_eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. BatchNorm vs LayerNorm ë¹„êµ\n",
        "\n",
        "### ì •ê·œí™” ë°©í–¥ì˜ ì°¨ì´\n",
        "\n",
        "| ì¢…ë¥˜ | ì •ê·œí™” ì¶• | ì‚¬ìš©ì²˜ |\n",
        "|------|----------|--------|\n",
        "| **BatchNorm** | batch ì°¨ì› (ê° featureë³„) | CNN, MLP |\n",
        "| **LayerNorm** | feature ì°¨ì› (ê° sampleë³„) | Transformer, RNN |\n",
        "\n",
        "### ì‹œê°ì  ì´í•´ (ì…ë ¥: batch Ã— features)\n",
        "```\n",
        "BatchNorm: ì„¸ë¡œ(â†“) ë°©í–¥ìœ¼ë¡œ ì •ê·œí™”\n",
        "LayerNorm: ê°€ë¡œ(â†’) ë°©í–¥ìœ¼ë¡œ ì •ê·œí™”\n",
        "\n",
        "       Feature1  Feature2  Feature3\n",
        "Batch1    â†“         â†“         â†“     â† BatchNorm\n",
        "Batch2    â†“         â†“         â†“\n",
        "Batch3    â†“         â†“         â†“\n",
        "         â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’     â† LayerNorm\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =================================================\n",
        "# TODO: Layer Normalization êµ¬í˜„\n",
        "# =================================================\n",
        "\n",
        "class LayerNorm:\n",
        "    def __init__(self, normalized_shape, eps=1e-5):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            normalized_shape: ì •ê·œí™”í•  feature í¬ê¸°\n",
        "            eps: ë¶„ëª¨ 0 ë°©ì§€\n",
        "        \"\"\"\n",
        "        self.eps = eps\n",
        "        self.gamma = np.ones(normalized_shape)\n",
        "        self.beta = np.zeros(normalized_shape)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch, features)\n",
        "        Returns:\n",
        "            out: (batch, features)\n",
        "        \"\"\"\n",
        "        # ============================================\n",
        "        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
        "        # 1. ê° sampleë³„ë¡œ mean, var ê³„ì‚° (axis=-1)\n",
        "        # 2. ì •ê·œí™”\n",
        "        # 3. scale & shift\n",
        "        # ============================================\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BatchNorm vs LayerNorm ì‹œê°í™”\n",
        "def compare_bn_ln():\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    x = np.random.randn(4, 8) * np.array([[1], [5], [10], [20]])  # ê° batchë§ˆë‹¤ ë‹¤ë¥¸ scale\n",
        "    \n",
        "    bn = BatchNorm1d(8)\n",
        "    ln = LayerNorm(8)\n",
        "    \n",
        "    bn.train()\n",
        "    x_bn = bn.forward(x)\n",
        "    x_ln = ln.forward(x)\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "    \n",
        "    im0 = axes[0].imshow(x, cmap='RdBu', aspect='auto')\n",
        "    axes[0].set_title('Original')\n",
        "    axes[0].set_xlabel('Features')\n",
        "    axes[0].set_ylabel('Batch')\n",
        "    plt.colorbar(im0, ax=axes[0])\n",
        "    \n",
        "    im1 = axes[1].imshow(x_bn, cmap='RdBu', aspect='auto', vmin=-2, vmax=2)\n",
        "    axes[1].set_title('BatchNorm (â†“ featureë³„)')\n",
        "    axes[1].set_xlabel('Features')\n",
        "    axes[1].set_ylabel('Batch')\n",
        "    plt.colorbar(im1, ax=axes[1])\n",
        "    \n",
        "    im2 = axes[2].imshow(x_ln, cmap='RdBu', aspect='auto', vmin=-2, vmax=2)\n",
        "    axes[2].set_title('LayerNorm (â†’ sampleë³„)')\n",
        "    axes[2].set_xlabel('Features')\n",
        "    axes[2].set_ylabel('Batch')\n",
        "    plt.colorbar(im2, ax=axes[2])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # í†µê³„ ì¶œë ¥\n",
        "    print(\"BatchNorm: ê° feature(column)ì˜ meanâ‰ˆ0, stdâ‰ˆ1\")\n",
        "    print(f\"  Column means: {x_bn.mean(axis=0)[:4].round(3)}...\")\n",
        "    print(f\"  Column stds:  {x_bn.std(axis=0)[:4].round(3)}...\")\n",
        "    \n",
        "    print(\"\\nLayerNorm: ê° sample(row)ì˜ meanâ‰ˆ0, stdâ‰ˆ1\")\n",
        "    print(f\"  Row means: {x_ln.mean(axis=1).round(3)}\")\n",
        "    print(f\"  Row stds:  {x_ln.std(axis=1).round(3)}\")\n",
        "\n",
        "compare_bn_ln()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. ì–¸ì œ ì–´ë–¤ Normalizationì„ ì‚¬ìš©í• ê¹Œ?\n",
        "\n",
        "| ìƒí™© | ì¶”ì²œ | ì´ìœ  |\n",
        "|------|------|------|\n",
        "| CNN (ì´ë¯¸ì§€) | BatchNorm | ê° feature mapì˜ ë¶„í¬ ì •ê·œí™” íš¨ê³¼ì  |\n",
        "| Transformer | LayerNorm | ê°€ë³€ ê¸¸ì´ ì‹œí€€ìŠ¤, batch ë…ë¦½ì„± í•„ìš” |\n",
        "| RNN | LayerNorm | ì‹œí€€ìŠ¤ ê¸¸ì´ ë³€í™”, batch í†µê³„ ë¶ˆì•ˆì • |\n",
        "| ì‘ì€ batch | LayerNorm/GroupNorm | BatchNormì€ ì‘ì€ batchì—ì„œ ë¶ˆì•ˆì • |\n",
        "| ì¶”ë¡  ì‹œ batch=1 | LayerNorm ìœ ë¦¬ | BatchNormì€ running stats ì˜ì¡´ |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Post-Quiz\n",
        "\n",
        "### Q1. BatchNormì˜ Î³, Î²ë¥¼ í•™ìŠµí•˜ëŠ” ì´ìœ ëŠ”?\n",
        "```\n",
        "ë‹¹ì‹ ì˜ ë‹µ: \n",
        "```\n",
        "\n",
        "### Q2. Transformerì—ì„œ BatchNorm ëŒ€ì‹  LayerNormì„ ì“°ëŠ” ì´ìœ ëŠ”?\n",
        "```\n",
        "ë‹¹ì‹ ì˜ ë‹µ: \n",
        "```\n",
        "\n",
        "### Q3. BatchNormì´ ì •ê·œí™” íš¨ê³¼(regularization)ë¥¼ ê°–ëŠ” ì´ìœ ëŠ”?\n",
        "```\n",
        "ë‹¹ì‹ ì˜ ë‹µ: \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## ì •ë‹µ\n",
        "\n",
        "<details>\n",
        "<summary>ğŸ‘‰ í´ë¦­í•˜ì—¬ ì •ë‹µ ì½”ë“œ ë³´ê¸°</summary>\n",
        "\n",
        "```python\n",
        "class BatchNorm1d:\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
        "        self.eps = eps\n",
        "        self.momentum = momentum\n",
        "        \n",
        "        self.gamma = np.ones(num_features)\n",
        "        self.beta = np.zeros(num_features)\n",
        "        \n",
        "        self.running_mean = np.zeros(num_features)\n",
        "        self.running_var = np.ones(num_features)\n",
        "        \n",
        "        self.training = True\n",
        "        self.cache = None\n",
        "    \n",
        "    def forward(self, x):\n",
        "        if self.training:\n",
        "            mean = np.mean(x, axis=0)\n",
        "            var = np.var(x, axis=0)\n",
        "            \n",
        "            # Running stats ì—…ë°ì´íŠ¸\n",
        "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean\n",
        "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var\n",
        "            \n",
        "            std = np.sqrt(var + self.eps)\n",
        "            x_hat = (x - mean) / std\n",
        "            \n",
        "            self.cache = (x_hat, std)\n",
        "        else:\n",
        "            std = np.sqrt(self.running_var + self.eps)\n",
        "            x_hat = (x - self.running_mean) / std\n",
        "        \n",
        "        return self.gamma * x_hat + self.beta\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        x_hat, std = self.cache\n",
        "        m = dout.shape[0]\n",
        "        \n",
        "        self.dgamma = np.sum(dout * x_hat, axis=0)\n",
        "        self.dbeta = np.sum(dout, axis=0)\n",
        "        \n",
        "        dx_hat = dout * self.gamma\n",
        "        dx = (dx_hat - np.mean(dx_hat, axis=0) - x_hat * np.mean(dx_hat * x_hat, axis=0)) / std\n",
        "        \n",
        "        return dx\n",
        "\n",
        "\n",
        "class LayerNorm:\n",
        "    def __init__(self, normalized_shape, eps=1e-5):\n",
        "        self.eps = eps\n",
        "        self.gamma = np.ones(normalized_shape)\n",
        "        self.beta = np.zeros(normalized_shape)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        mean = np.mean(x, axis=-1, keepdims=True)\n",
        "        var = np.var(x, axis=-1, keepdims=True)\n",
        "        x_hat = (x - mean) / np.sqrt(var + self.eps)\n",
        "        return self.gamma * x_hat + self.beta\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>ğŸ‘‰ Quiz ì •ë‹µ</summary>\n",
        "\n",
        "### Pre-Quiz\n",
        "\n",
        "**Q1. Internal Covariate Shift**\n",
        "í•™ìŠµ ì¤‘ ì´ì „ ë ˆì´ì–´ì˜ íŒŒë¼ë¯¸í„°ê°€ ë³€í•˜ë©´ì„œ í˜„ì¬ ë ˆì´ì–´ì˜ ì…ë ¥ ë¶„í¬ê°€ ê³„ì† ë°”ë€ŒëŠ” í˜„ìƒ. ê° ë ˆì´ì–´ê°€ ìƒˆë¡œìš´ ë¶„í¬ì— ì ì‘í•´ì•¼ í•´ì„œ í•™ìŠµì´ ëŠë ¤ì§.\n",
        "\n",
        "**Q2. Î³, Î²ì˜ ì—­í• **\n",
        "ì •ê·œí™” í›„ í‘œí˜„ë ¥ ë³µêµ¬. í•­ìƒ mean=0, std=1ì´ë©´ í‘œí˜„ë ¥ì´ ì œí•œë  ìˆ˜ ìˆìŒ. ëª¨ë¸ì´ í•„ìš”í•˜ë©´ ì›ë˜ ë¶„í¬ë¡œ ëŒë¦´ ìˆ˜ ìˆë„ë¡ scale(Î³)ê³¼ shift(Î²)ë¥¼ í•™ìŠµ.\n",
        "\n",
        "**Q3. ì¶”ë¡  ì‹œ batch=1**\n",
        "í•™ìŠµ ì¤‘ ì €ì¥í•œ running mean/varë¥¼ ì‚¬ìš©. í˜„ì¬ batchì˜ í†µê³„ê°€ ì•„ë‹Œ í•™ìŠµ ì „ì²´ì˜ í†µê³„ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ batch í¬ê¸°ì™€ ë¬´ê´€í•˜ê²Œ ë™ì‘.\n",
        "\n",
        "### Post-Quiz\n",
        "\n",
        "**Q1. Î³, Î² í•™ìŠµ ì´ìœ **\n",
        "í•­ë“± ë³€í™˜(identity)ì„ í¬í•¨í•˜ë„ë¡. Î³=std, Î²=meanì´ë©´ ì •ê·œí™” ì „ìœ¼ë¡œ ë³µì›ë¨. ëª¨ë¸ì´ BNì´ í•„ìš”í•œ ì •ë„ë¥¼ ìŠ¤ìŠ¤ë¡œ í•™ìŠµ.\n",
        "\n",
        "**Q2. Transformerì—ì„œ LayerNorm ì´ìœ **\n",
        "1) ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ê°€ë³€ì  2) batch ë‚´ ìƒ˜í”Œë“¤ì´ ë…ë¦½ì ì´ì–´ì•¼ í•¨ 3) ì‘ì€ batchì—ì„œë„ ì•ˆì •ì  4) ì¶”ë¡  ì‹œ batch=1ë„ ìì—°ìŠ¤ëŸ½ê²Œ ì²˜ë¦¬\n",
        "\n",
        "**Q3. BatchNormì˜ ì •ê·œí™” íš¨ê³¼**\n",
        "ë¯¸ë‹ˆë°°ì¹˜ í†µê³„ì˜ ë…¸ì´ì¦ˆê°€ ì•”ë¬µì  ì •ê·œí™” ì—­í• . ë§¤ iterationë§ˆë‹¤ ë‹¤ë¥¸ batch ìƒ˜í”Œ â†’ ë‹¤ë¥¸ mean/var â†’ í•™ìŠµì— ë…¸ì´ì¦ˆ ì¶”ê°€ (dropoutê³¼ ìœ ì‚¬í•œ íš¨ê³¼).\n",
        "\n",
        "</details>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
