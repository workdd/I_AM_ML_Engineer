{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Activation Function ì§ì ‘ êµ¬í˜„\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- ê° í™œì„±í™” í•¨ìˆ˜ì™€ **ë¯¸ë¶„**ì„ ì§ì ‘ êµ¬í˜„\n",
    "- Non-linearityê°€ ì™œ í•„ìš”í•œì§€ XOR ë¬¸ì œë¡œ í™•ì¸\n",
    "- Vanishing Gradient ë¬¸ì œ ì‹œê°í™”\n",
    "\n",
    "## ê´€ë ¨ ë©´ì ‘ ì§ˆë¬¸\n",
    "- ì•Œê³ ìˆëŠ” Activation Functionì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”\n",
    "- ìš”ì¦˜ Sigmoid ë³´ë‹¤ ReLUë¥¼ ë§ì´ ì“°ëŠ” ì´ìœ ëŠ”?\n",
    "- Non-Linearityë¼ëŠ” ë§ì˜ ì˜ë¯¸ì™€ ê·¸ í•„ìš”ì„±ì€?\n",
    "- ReLUë¡œ ì–´ë–»ê²Œ ê³¡ì„  í•¨ìˆ˜ë¥¼ ê·¼ì‚¬í•˜ë‚˜?\n",
    "- ReLUì˜ ë¬¸ì œì ì€?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pre-Quiz\n",
    "\n",
    "### Q1. í™œì„±í™” í•¨ìˆ˜ê°€ ì—†ìœ¼ë©´ (ì„ í˜•ë§Œ ìˆìœ¼ë©´) ë¬´ìŠ¨ ë¬¸ì œê°€ ìƒê¸°ë‚˜ìš”?\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ: \n",
    "```\n",
    "\n",
    "### Q2. Sigmoidì˜ ë¬¸ì œì ì€?\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ: \n",
    "```\n",
    "\n",
    "### Q3. ReLUê°€ Sigmoidë³´ë‹¤ ë‚˜ì€ ì ì€?\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ: \n",
    "```\n",
    "\n",
    "### Q4. ReLUì˜ ë¬¸ì œì ì€?\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ: \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['font.family'] = 'AppleGothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.figsize'] = (12, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Sigmoid êµ¬í˜„\n",
    "\n",
    "### ìˆ˜ì‹\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "### ë¯¸ë¶„\n",
    "$$\\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x))$$\n",
    "\n",
    "### íŠ¹ì§•\n",
    "- ì¶œë ¥ ë²”ìœ„: (0, 1)\n",
    "- í™•ë¥ ë¡œ í•´ì„ ê°€ëŠ¥\n",
    "- **ë¬¸ì œ**: |x|ê°€ í¬ë©´ gradient â‰ˆ 0 (Vanishing Gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# TODO: Sigmoid í´ë˜ìŠ¤ êµ¬í˜„\n",
    "# =================================================\n",
    "#\n",
    "# ìš”êµ¬ì‚¬í•­:\n",
    "#   - forward(x): 1 / (1 + exp(-x)) ë°˜í™˜\n",
    "#   - backward(x): Ïƒ(x) * (1 - Ïƒ(x)) ë°˜í™˜\n",
    "#\n",
    "# íŒíŠ¸:\n",
    "#   - np.exp() ì‚¬ìš©\n",
    "#   - overflow ë°©ì§€: np.clip(x, -500, 500) ì‚¬ìš© ê¶Œì¥\n",
    "#\n",
    "# =================================================\n",
    "\n",
    "class Sigmoid:\n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        # TODO: sigmoid í•¨ìˆ˜ êµ¬í˜„\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(x):\n",
    "        # TODO: sigmoid ë¯¸ë¶„ êµ¬í˜„\n",
    "        # íŒíŠ¸: forward ê²°ê³¼ë¥¼ ì´ìš©\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ í…ŒìŠ¤íŠ¸ ============\n",
    "def test_sigmoid():\n",
    "    # forward í…ŒìŠ¤íŠ¸\n",
    "    assert np.isclose(Sigmoid.forward(0), 0.5), \"sigmoid(0) = 0.5\"\n",
    "    assert Sigmoid.forward(100) > 0.99, \"sigmoid(í°ê°’) â‰ˆ 1\"\n",
    "    assert Sigmoid.forward(-100) < 0.01, \"sigmoid(ì‘ì€ê°’) â‰ˆ 0\"\n",
    "    \n",
    "    # backward í…ŒìŠ¤íŠ¸\n",
    "    assert np.isclose(Sigmoid.backward(0), 0.25), \"sigmoid'(0) = 0.25\"\n",
    "    assert Sigmoid.backward(10) < 0.001, \"sigmoid'(í°ê°’) â‰ˆ 0 (vanishing!)\"\n",
    "    \n",
    "    print(\"âœ… Sigmoid í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
    "\n",
    "test_sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Tanh êµ¬í˜„\n",
    "\n",
    "### ìˆ˜ì‹\n",
    "$$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "\n",
    "### ë¯¸ë¶„\n",
    "$$\\tanh'(x) = 1 - \\tanh^2(x)$$\n",
    "\n",
    "### íŠ¹ì§•\n",
    "- ì¶œë ¥ ë²”ìœ„: (-1, 1)\n",
    "- **Zero-centered** (Sigmoidë³´ë‹¤ ë‚˜ìŒ)\n",
    "- ì—¬ì „íˆ Vanishing Gradient ë¬¸ì œ ìˆìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# TODO: Tanh í´ë˜ìŠ¤ êµ¬í˜„\n",
    "# =================================================\n",
    "#\n",
    "# ìš”êµ¬ì‚¬í•­:\n",
    "#   - forward(x): tanh(x) ë°˜í™˜\n",
    "#   - backward(x): 1 - tanhÂ²(x) ë°˜í™˜\n",
    "#\n",
    "# íŒíŠ¸: np.tanh() ì‚¬ìš© ê°€ëŠ¥\n",
    "#\n",
    "# =================================================\n",
    "\n",
    "class Tanh:\n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        # TODO\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(x):\n",
    "        # TODO\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ í…ŒìŠ¤íŠ¸ ============\n",
    "def test_tanh():\n",
    "    assert np.isclose(Tanh.forward(0), 0), \"tanh(0) = 0\"\n",
    "    assert np.isclose(Tanh.backward(0), 1), \"tanh'(0) = 1\"\n",
    "    assert Tanh.backward(5) < 0.01, \"tanh'(í°ê°’) â‰ˆ 0\"\n",
    "    print(\"âœ… Tanh í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
    "\n",
    "test_tanh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. ReLU êµ¬í˜„\n",
    "\n",
    "### ìˆ˜ì‹\n",
    "$$\\text{ReLU}(x) = \\max(0, x)$$\n",
    "\n",
    "### ë¯¸ë¶„\n",
    "$$\\text{ReLU}'(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\ 0 & \\text{if } x \\leq 0 \\end{cases}$$\n",
    "\n",
    "### íŠ¹ì§•\n",
    "- ê³„ì‚° íš¨ìœ¨ì  (exp ì—†ìŒ)\n",
    "- **Vanishing Gradient í•´ê²°** (ì–‘ìˆ˜ ì˜ì—­ì—ì„œ)\n",
    "- **ë¬¸ì œ**: Dying ReLU (ìŒìˆ˜ ì˜ì—­ì—ì„œ gradient = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# TODO: ReLU í´ë˜ìŠ¤ êµ¬í˜„\n",
    "# =================================================\n",
    "#\n",
    "# ìš”êµ¬ì‚¬í•­:\n",
    "#   - forward(x): max(0, x) ë°˜í™˜\n",
    "#   - backward(x): x > 0ì´ë©´ 1, ì•„ë‹ˆë©´ 0\n",
    "#\n",
    "# íŒíŠ¸:\n",
    "#   - np.maximum(0, x) ë˜ëŠ” np.where ì‚¬ìš©\n",
    "#   - backwardëŠ” (x > 0).astype(float)\n",
    "#\n",
    "# =================================================\n",
    "\n",
    "class ReLU:\n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        # TODO\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(x):\n",
    "        # TODO\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ í…ŒìŠ¤íŠ¸ ============\n",
    "def test_relu():\n",
    "    assert ReLU.forward(5) == 5, \"ReLU(ì–‘ìˆ˜) = ê·¸ëŒ€ë¡œ\"\n",
    "    assert ReLU.forward(-5) == 0, \"ReLU(ìŒìˆ˜) = 0\"\n",
    "    assert ReLU.backward(5) == 1, \"ReLU'(ì–‘ìˆ˜) = 1\"\n",
    "    assert ReLU.backward(-5) == 0, \"ReLU'(ìŒìˆ˜) = 0 (dying!)\"\n",
    "    print(\"âœ… ReLU í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
    "\n",
    "test_relu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Leaky ReLU êµ¬í˜„\n",
    "\n",
    "### ìˆ˜ì‹\n",
    "$$\\text{LeakyReLU}(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{if } x \\leq 0 \\end{cases}$$\n",
    "\n",
    "### ë¯¸ë¶„\n",
    "$$\\text{LeakyReLU}'(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\ \\alpha & \\text{if } x \\leq 0 \\end{cases}$$\n",
    "\n",
    "### íŠ¹ì§•\n",
    "- **Dying ReLU í•´ê²°**: ìŒìˆ˜ì—ì„œë„ ì‘ì€ gradient ìœ ì§€\n",
    "- Î±ëŠ” ë³´í†µ 0.01 ë˜ëŠ” 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# TODO: LeakyReLU í´ë˜ìŠ¤ êµ¬í˜„\n",
    "# =================================================\n",
    "#\n",
    "# ìš”êµ¬ì‚¬í•­:\n",
    "#   - __init__(self, alpha=0.01): alpha ì €ì¥\n",
    "#   - forward(x): x > 0ì´ë©´ x, ì•„ë‹ˆë©´ alpha * x\n",
    "#   - backward(x): x > 0ì´ë©´ 1, ì•„ë‹ˆë©´ alpha\n",
    "#\n",
    "# íŒíŠ¸: np.where(ì¡°ê±´, ì°¸ì¼ë•Œ, ê±°ì§“ì¼ë•Œ)\n",
    "#\n",
    "# =================================================\n",
    "\n",
    "class LeakyReLU:\n",
    "    def __init__(self, alpha=0.01):\n",
    "        # TODO\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        pass\n",
    "    \n",
    "    def backward(self, x):\n",
    "        # TODO\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ í…ŒìŠ¤íŠ¸ ============\n",
    "def test_leaky_relu():\n",
    "    lrelu = LeakyReLU(alpha=0.1)\n",
    "    assert lrelu.forward(5) == 5\n",
    "    assert np.isclose(lrelu.forward(-5), -0.5), \"LeakyReLU(-5) = -0.5 (alpha=0.1)\"\n",
    "    assert lrelu.backward(5) == 1\n",
    "    assert lrelu.backward(-5) == 0.1, \"LeakyReLU'(ìŒìˆ˜) = alpha\"\n",
    "    print(\"âœ… LeakyReLU í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
    "\n",
    "test_leaky_relu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. í™œì„±í™” í•¨ìˆ˜ ë¹„êµ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 1000)\n",
    "lrelu = LeakyReLU(0.1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Forward\n",
    "axes[0].plot(x, Sigmoid.forward(x), label='Sigmoid', linewidth=2)\n",
    "axes[0].plot(x, Tanh.forward(x), label='Tanh', linewidth=2)\n",
    "axes[0].plot(x, ReLU.forward(x), label='ReLU', linewidth=2)\n",
    "axes[0].plot(x, lrelu.forward(x), label='LeakyReLU', linewidth=2)\n",
    "axes[0].axhline(y=0, color='black', linewidth=0.5)\n",
    "axes[0].axvline(x=0, color='black', linewidth=0.5)\n",
    "axes[0].set_title('í™œì„±í™” í•¨ìˆ˜ ë¹„êµ')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim(-2, 5)\n",
    "\n",
    "# Backward (ë¯¸ë¶„)\n",
    "axes[1].plot(x, Sigmoid.backward(x), label=\"Sigmoid'\", linewidth=2)\n",
    "axes[1].plot(x, Tanh.backward(x), label=\"Tanh'\", linewidth=2)\n",
    "axes[1].plot(x, ReLU.backward(x), label=\"ReLU'\", linewidth=2)\n",
    "axes[1].plot(x, lrelu.backward(x), label=\"LeakyReLU'\", linewidth=2)\n",
    "axes[1].axhline(y=0, color='black', linewidth=0.5)\n",
    "axes[1].axvline(x=0, color='black', linewidth=0.5)\n",
    "axes[1].set_title('ë¯¸ë¶„ (Gradient) ë¹„êµ')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim(-0.5, 1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Vanishing Gradient ì‹œê°í™”\n",
    "\n",
    "ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì—ì„œ gradientê°€ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_gradient_flow(activation, n_layers=10, input_val=0.5):\n",
    "    \"\"\"n_layersë¥¼ í†µê³¼í•˜ë©° gradientê°€ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ ì‹œë®¬ë ˆì´ì…˜\"\"\"\n",
    "    gradients = []\n",
    "    grad = 1.0  # ì¶œë ¥ì¸µì—ì„œ ì‹œì‘\n",
    "    \n",
    "    for _ in range(n_layers):\n",
    "        # ê° ì¸µì„ ì—­ì „íŒŒë¡œ í†µê³¼\n",
    "        grad = grad * activation.backward(input_val)\n",
    "        gradients.append(grad)\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "# ì‹œë®¬ë ˆì´ì…˜\n",
    "n_layers = 15\n",
    "sigmoid_grads = simulate_gradient_flow(Sigmoid, n_layers)\n",
    "tanh_grads = simulate_gradient_flow(Tanh, n_layers)\n",
    "relu_grads = simulate_gradient_flow(ReLU, n_layers, input_val=1.0)  # ì–‘ìˆ˜ì—¬ì•¼ 1\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, n_layers+1), sigmoid_grads, 'b-o', label='Sigmoid', linewidth=2)\n",
    "plt.plot(range(1, n_layers+1), tanh_grads, 'g-s', label='Tanh', linewidth=2)\n",
    "plt.plot(range(1, n_layers+1), relu_grads, 'r-^', label='ReLU', linewidth=2)\n",
    "\n",
    "plt.xlabel('Layer (ì¶œë ¥ì¸µì—ì„œë¶€í„°)')\n",
    "plt.ylabel('Gradient í¬ê¸°')\n",
    "plt.title(f'{n_layers}ì¸µ ë„¤íŠ¸ì›Œí¬ì—ì„œ Gradient íë¦„\\n(Vanishing Gradient ì‹œê°í™”)')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.annotate('Sigmoid/Tanh: ê¸‰ê²©íˆ ê°ì†Œ!', xy=(8, sigmoid_grads[7]),\n",
    "            xytext=(10, 0.01), arrowprops=dict(arrowstyle='->'))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n15ì¸µ í›„ gradient í¬ê¸°:\")\n",
    "print(f\"  Sigmoid: {sigmoid_grads[-1]:.2e}\")\n",
    "print(f\"  Tanh: {tanh_grads[-1]:.2e}\")\n",
    "print(f\"  ReLU: {relu_grads[-1]:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. XOR ë¬¸ì œë¡œ Non-Linearity í•„ìš”ì„± í™•ì¸\n",
    "\n",
    "XORì€ ì„ í˜•ìœ¼ë¡œ ë¶„ë¦¬ ë¶ˆê°€ëŠ¥í•œ ëŒ€í‘œì ì¸ ë¬¸ì œì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR ë°ì´í„°\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_xor = np.array([0, 1, 1, 0])\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "for i in range(4):\n",
    "    color = 'blue' if y_xor[i] == 0 else 'red'\n",
    "    marker = 'o' if y_xor[i] == 0 else 's'\n",
    "    plt.scatter(X_xor[i, 0], X_xor[i, 1], c=color, s=200, marker=marker)\n",
    "\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('XOR ë¬¸ì œ\\níŒŒë€ ë™ê·¸ë¼ë¯¸: 0, ë¹¨ê°„ ë„¤ëª¨: 1\\nâ†’ í•˜ë‚˜ì˜ ì§ì„ ìœ¼ë¡œ ë¶„ë¦¬ ë¶ˆê°€!')\n",
    "plt.xlim(-0.5, 1.5)\n",
    "plt.ylim(-0.5, 1.5)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# TODO: ê°„ë‹¨í•œ 2ì¸µ ì‹ ê²½ë§ìœ¼ë¡œ XOR í•™ìŠµ\n",
    "# =================================================\n",
    "#\n",
    "# êµ¬ì¡°: Input(2) â†’ Hidden(4) â†’ Output(1)\n",
    "#\n",
    "# ìš”êµ¬ì‚¬í•­:\n",
    "#   - __init__: W1(2x4), b1(4), W2(4x1), b2(1) ì´ˆê¸°í™”\n",
    "#   - forward(X): X â†’ Linear â†’ Activation â†’ Linear â†’ Sigmoid\n",
    "#   - backward, updateëŠ” ì œê³µë¨\n",
    "#\n",
    "# ì‹¤í—˜:\n",
    "#   - activation='linear' vs 'relu' ë¹„êµ\n",
    "#\n",
    "# =================================================\n",
    "\n",
    "class XORNet:\n",
    "    def __init__(self, activation='relu'):\n",
    "        np.random.seed(42)\n",
    "        self.activation = activation\n",
    "        \n",
    "        # TODO: ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\n",
    "        # W1: (2, 4), b1: (4,)\n",
    "        # W2: (4, 1), b2: (1,)\n",
    "        # íŒíŠ¸: np.random.randn() * 0.5\n",
    "        self.W1 = None\n",
    "        self.b1 = None\n",
    "        self.W2 = None\n",
    "        self.b2 = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # TODO:\n",
    "        # 1. z1 = X @ W1 + b1\n",
    "        # 2. a1 = activation(z1)  â† linearì´ë©´ ê·¸ëŒ€ë¡œ, reluë©´ ReLU.forward\n",
    "        # 3. z2 = a1 @ W2 + b2\n",
    "        # 4. output = Sigmoid.forward(z2)\n",
    "        # 5. ì¤‘ê°„ê°’ë“¤ ì €ì¥ (backwardì—ì„œ ì‚¬ìš©)\n",
    "        pass\n",
    "    \n",
    "    # backwardì™€ updateëŠ” ì œê³µ\n",
    "    def backward(self, X, y, lr=0.5):\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer\n",
    "        dz2 = self.a2 - y.reshape(-1, 1)\n",
    "        dW2 = self.a1.T @ dz2 / m\n",
    "        db2 = np.sum(dz2, axis=0) / m\n",
    "        \n",
    "        # Hidden layer\n",
    "        da1 = dz2 @ self.W2.T\n",
    "        if self.activation == 'relu':\n",
    "            dz1 = da1 * ReLU.backward(self.z1)\n",
    "        else:\n",
    "            dz1 = da1\n",
    "        dW1 = X.T @ dz1 / m\n",
    "        db1 = np.sum(dz1, axis=0) / m\n",
    "        \n",
    "        # Update\n",
    "        self.W2 -= lr * dW2\n",
    "        self.b2 -= lr * db2\n",
    "        self.W1 -= lr * dW1\n",
    "        self.b1 -= lr * db1\n",
    "    \n",
    "    def train(self, X, y, epochs=5000, lr=1.0):\n",
    "        losses = []\n",
    "        for _ in range(epochs):\n",
    "            pred = self.forward(X)\n",
    "            loss = -np.mean(y.reshape(-1,1)*np.log(pred+1e-8) + \n",
    "                           (1-y.reshape(-1,1))*np.log(1-pred+1e-8))\n",
    "            losses.append(loss)\n",
    "            self.backward(X, y, lr)\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear vs ReLU ë¹„êµ\n",
    "net_linear = XORNet(activation='linear')\n",
    "net_relu = XORNet(activation='relu')\n",
    "\n",
    "losses_linear = net_linear.train(X_xor, y_xor, epochs=5000)\n",
    "losses_relu = net_relu.train(X_xor, y_xor, epochs=5000)\n",
    "\n",
    "# ê²°ê³¼\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss ë¹„êµ\n",
    "axes[0].plot(losses_linear, label='Linear (í™œì„±í™” ì—†ìŒ)', linewidth=2)\n",
    "axes[0].plot(losses_relu, label='ReLU', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('XOR í•™ìŠµ - Loss ë¹„êµ')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# ì˜ˆì¸¡ ê²°ê³¼\n",
    "pred_linear = (net_linear.forward(X_xor) > 0.5).flatten()\n",
    "pred_relu = (net_relu.forward(X_xor) > 0.5).flatten()\n",
    "\n",
    "result_text = f\"\"\"ì˜ˆì¸¡ ê²°ê³¼:\n",
    "        [0,0] [0,1] [1,0] [1,1]\n",
    "ì •ë‹µ:     0     1     1     0\n",
    "Linear:   {int(pred_linear[0])}     {int(pred_linear[1])}     {int(pred_linear[2])}     {int(pred_linear[3])}\n",
    "ReLU:     {int(pred_relu[0])}     {int(pred_relu[1])}     {int(pred_relu[2])}     {int(pred_relu[3])}\n",
    "\n",
    "Linear ì •í™•ë„: {np.mean(pred_linear == y_xor)*100:.0f}%\n",
    "ReLU ì •í™•ë„: {np.mean(pred_relu == y_xor)*100:.0f}%\n",
    "\"\"\"\n",
    "axes[1].text(0.1, 0.5, result_text, fontsize=12, family='monospace',\n",
    "            verticalalignment='center')\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµ')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Post-Quiz\n",
    "\n",
    "### Q1. Sigmoidì˜ gradient ìµœëŒ“ê°’ì€ ì–¼ë§ˆì¸ê°€ìš”? (ì‹œê°í™” ì°¸ê³ )\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ: \n",
    "```\n",
    "\n",
    "### Q2. 15ì¸µ ë„¤íŠ¸ì›Œí¬ì—ì„œ Sigmoid gradientê°€ ê±°ì˜ 0ì´ ë˜ëŠ” ì´ìœ ëŠ”?\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ: \n",
    "```\n",
    "\n",
    "### Q3. XORì—ì„œ Linearê°€ ì‹¤íŒ¨í•˜ê³  ReLUê°€ ì„±ê³µí•œ ì´ìœ ëŠ”?\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ: \n",
    "```\n",
    "\n",
    "### Q4. Dying ReLU ë¬¸ì œë€? í•´ê²°ì±…ì€?\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ: \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ì •ë‹µ ë° í•´ì„¤\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ‘‰ í´ë¦­í•˜ì—¬ ì •ë‹µ ë³´ê¸°</summary>\n",
    "\n",
    "### Pre-Quiz ì •ë‹µ\n",
    "\n",
    "**Q1. í™œì„±í™” í•¨ìˆ˜ ì—†ìœ¼ë©´?**\n",
    "> - ì„ í˜• í•¨ìˆ˜ì˜ í•©ì„± = ì—¬ì „íˆ ì„ í˜•\n",
    "> - ì•„ë¬´ë¦¬ ì¸µì„ ê¹Šê²Œ ìŒ“ì•„ë„ í•˜ë‚˜ì˜ ì„ í˜• ë³€í™˜ê³¼ ë™ì¼\n",
    "> - XOR ê°™ì€ ë¹„ì„ í˜• ë¬¸ì œ í•´ê²° ë¶ˆê°€\n",
    "\n",
    "**Q2. Sigmoid ë¬¸ì œì **\n",
    "> - Vanishing Gradient: |x| í´ ë•Œ gradient â‰ˆ 0\n",
    "> - Not zero-centered: ì¶œë ¥ì´ í•­ìƒ ì–‘ìˆ˜ (0~1)\n",
    "> - exp ì—°ì‚° ë¹„ìš©\n",
    "\n",
    "**Q3. ReLUê°€ ë‚˜ì€ ì **\n",
    "> - ì–‘ìˆ˜ì—ì„œ gradient = 1 (vanishing ì—†ìŒ)\n",
    "> - ê³„ì‚° íš¨ìœ¨ì  (maxë§Œ ê³„ì‚°)\n",
    "> - í¬ì†Œ í™œì„±í™” (sparse)\n",
    "\n",
    "**Q4. ReLU ë¬¸ì œì **\n",
    "> - Dying ReLU: ìŒìˆ˜ ì…ë ¥ â†’ gradient 0 â†’ ì˜ì›íˆ ì£½ìŒ\n",
    "> - Not zero-centered\n",
    "\n",
    "---\n",
    "\n",
    "### Post-Quiz ì •ë‹µ\n",
    "\n",
    "**Q1. Sigmoid gradient ìµœëŒ“ê°’**\n",
    "> 0.25 (x=0ì¼ ë•Œ)\n",
    "> Ïƒ'(0) = Ïƒ(0) Ã— (1-Ïƒ(0)) = 0.5 Ã— 0.5 = 0.25\n",
    "\n",
    "**Q2. 15ì¸µì—ì„œ Sigmoid gradient â‰ˆ 0ì¸ ì´ìœ **\n",
    "> - ê° ì¸µì—ì„œ gradient Ã— Ïƒ'(x) â‰¤ 0.25\n",
    "> - 15ì¸µ: 0.25^15 â‰ˆ 10^-9\n",
    "> - ê³±ì…ˆì´ ëˆ„ì ë˜ì–´ ê¸°í•˜ê¸‰ìˆ˜ì  ê°ì†Œ\n",
    "\n",
    "**Q3. XORì—ì„œ Linear ì‹¤íŒ¨, ReLU ì„±ê³µ ì´ìœ **\n",
    "> - Linear: y = Wx + bì˜ í•©ì„± = ì—¬ì „íˆ ì„ í˜• â†’ ì§ì„  í•˜ë‚˜ë¡œ ë¶„ë¦¬ ë¶ˆê°€\n",
    "> - ReLU: ë¹„ì„ í˜•ì„± ì¶”ê°€ â†’ ê³µê°„ì„ \"êº¾ì–´ì„œ\" ë¶„ë¦¬ ê°€ëŠ¥\n",
    "\n",
    "**Q4. Dying ReLUì™€ í•´ê²°ì±…**\n",
    "> - ë¬¸ì œ: ìŒìˆ˜ ì…ë ¥ â†’ ReLU ì¶œë ¥ 0 â†’ gradient 0 â†’ í•™ìŠµ ì•ˆ ë¨\n",
    "> - í•´ê²°: Leaky ReLU (ìŒìˆ˜ì—ì„œ ì‘ì€ ê¸°ìš¸ê¸°)\n",
    "> - í•´ê²°: PReLU (Î±ë„ í•™ìŠµ)\n",
    "> - í•´ê²°: ELU (ë¶€ë“œëŸ¬ìš´ ìŒìˆ˜ ì˜ì—­)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## êµ¬í˜„ ì •ë‹µ ì½”ë“œ\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ‘‰ í´ë¦­í•˜ì—¬ ì •ë‹µ ì½”ë“œ ë³´ê¸°</summary>\n",
    "\n",
    "```python\n",
    "class Sigmoid:\n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(x):\n",
    "        s = Sigmoid.forward(x)\n",
    "        return s * (1 - s)\n",
    "\n",
    "\n",
    "class Tanh:\n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(x):\n",
    "        return 1 - np.tanh(x)**2\n",
    "\n",
    "\n",
    "class ReLU:\n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(x):\n",
    "        return (x > 0).astype(float)\n",
    "\n",
    "\n",
    "class LeakyReLU:\n",
    "    def __init__(self, alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return np.where(x > 0, x, self.alpha * x)\n",
    "    \n",
    "    def backward(self, x):\n",
    "        return np.where(x > 0, 1, self.alpha)\n",
    "\n",
    "\n",
    "class XORNet:\n",
    "    def __init__(self, activation='relu'):\n",
    "        np.random.seed(42)\n",
    "        self.activation = activation\n",
    "        \n",
    "        self.W1 = np.random.randn(2, 4) * 0.5\n",
    "        self.b1 = np.zeros(4)\n",
    "        self.W2 = np.random.randn(4, 1) * 0.5\n",
    "        self.b2 = np.zeros(1)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        \n",
    "        if self.activation == 'relu':\n",
    "            self.a1 = ReLU.forward(self.z1)\n",
    "        else:\n",
    "            self.a1 = self.z1\n",
    "        \n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        self.a2 = Sigmoid.forward(self.z2)\n",
    "        \n",
    "        return self.a2\n",
    "```\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
