{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04. Regularization (ì •ê·œí™”)\n",
        "\n",
        "## í•™ìŠµ ëª©í‘œ\n",
        "- Overfittingì˜ ì›ì¸ ì´í•´\n",
        "- L1, L2 Regularization ë¹„êµ\n",
        "- Dropout ì›ë¦¬ ì´í•´ ë° êµ¬í˜„\n",
        "- Early Stopping ê°œë…\n",
        "\n",
        "## ê´€ë ¨ ë©´ì ‘ ì§ˆë¬¸\n",
        "- L1ê³¼ L2 regularizationì˜ ì°¨ì´ëŠ”?\n",
        "- Dropoutì´ ì™œ íš¨ê³¼ì ì¸ê°€ìš”?\n",
        "- L1ì´ sparse solutionì„ ë§Œë“œëŠ” ì´ìœ ëŠ”?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Pre-Quiz\n",
        "\n",
        "### Q1. Overfittingì´ë€?\n",
        "```\n",
        "ë‹¹ì‹ ì˜ ë‹µ: \n",
        "```\n",
        "\n",
        "### Q2. L1 vs L2 ì •ê·œí™”ì˜ ì°¨ì´ì ì„ ì„¤ëª…í•˜ì„¸ìš”\n",
        "```\n",
        "ë‹¹ì‹ ì˜ ë‹µ: \n",
        "```\n",
        "\n",
        "### Q3. Dropout rateê°€ 0.5ë¼ëŠ” ê²ƒì€ ë¬´ìŠ¨ ì˜ë¯¸ì¸ê°€ìš”?\n",
        "```\n",
        "ë‹¹ì‹ ì˜ ë‹µ: \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# matplotlib í•œê¸€ ì„¤ì • (macOS)\n",
        "plt.rcParams['font.family'] = 'AppleGothic'\n",
        "plt.rcParams['axes.unicode_minus'] = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Overfitting ì‹œê°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë°ì´í„° ìƒì„±: y = sin(x) + noise\n",
        "n_samples = 30\n",
        "X = np.sort(np.random.rand(n_samples) * 4 * np.pi)\n",
        "y = np.sin(X) + np.random.randn(n_samples) * 0.3\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# ë‹¤ì–‘í•œ ë³µì¡ë„ì˜ ëª¨ë¸ ë¹„êµ\n",
        "degrees = [1, 4, 15]\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "X_plot = np.linspace(0, 4*np.pi, 100)\n",
        "\n",
        "for ax, degree in zip(axes, degrees):\n",
        "    poly = PolynomialFeatures(degree)\n",
        "    X_train_poly = poly.fit_transform(X_train.reshape(-1, 1))\n",
        "    X_plot_poly = poly.transform(X_plot.reshape(-1, 1))\n",
        "    \n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train_poly, y_train)\n",
        "    \n",
        "    y_plot = model.predict(X_plot_poly)\n",
        "    \n",
        "    ax.scatter(X_train, y_train, c='blue', label='Train')\n",
        "    ax.scatter(X_test, y_test, c='red', label='Test')\n",
        "    ax.plot(X_plot, y_plot, 'g-', lw=2)\n",
        "    ax.plot(X_plot, np.sin(X_plot), 'k--', alpha=0.5, label='True')\n",
        "    ax.set_title(f'Degree {degree}')\n",
        "    ax.legend()\n",
        "    ax.set_ylim(-2, 2)\n",
        "\n",
        "plt.suptitle('Underfitting â†’ Good Fit â†’ Overfitting')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. L1 vs L2 Regularization\n",
        "\n",
        "### ìˆ˜ì‹\n",
        "\n",
        "| ì¢…ë¥˜ | Loss Function | íŠ¹ì§• |\n",
        "|------|--------------|------|\n",
        "| L2 (Ridge) | $L + \\lambda \\sum w_i^2$ | ëª¨ë“  ê°€ì¤‘ì¹˜ë¥¼ ì‘ê²Œ |\n",
        "| L1 (Lasso) | $L + \\lambda \\sum |w_i|$ | ì¼ë¶€ ê°€ì¤‘ì¹˜ë¥¼ 0ìœ¼ë¡œ (Sparse) |\n",
        "\n",
        "### L1ì´ Sparse Solutionì„ ë§Œë“œëŠ” ì´ìœ \n",
        "- L1 gradientëŠ” ìƒìˆ˜ (Â±1)\n",
        "- ì‘ì€ ê°€ì¤‘ì¹˜ë„ ì¼ì •í•œ ì†ë„ë¡œ 0ìœ¼ë¡œ ì´ë™\n",
        "- L2 gradientëŠ” ê°€ì¤‘ì¹˜ì— ë¹„ë¡€ (2w)\n",
        "- ì‘ì€ ê°€ì¤‘ì¹˜ëŠ” ëŠë¦¬ê²Œ 0ì— ì ‘ê·¼ (ì™„ì „íˆ 0ì´ ë˜ì§€ ì•ŠìŒ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# L1 vs L2 ì˜í–¥ ì‹œê°í™”\n",
        "def visualize_l1_l2():\n",
        "    w = np.linspace(-3, 3, 100)\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "    \n",
        "    # 1. Penalty í•¨ìˆ˜\n",
        "    axes[0].plot(w, np.abs(w), label='L1: |w|', lw=2)\n",
        "    axes[0].plot(w, w**2, label='L2: wÂ²', lw=2)\n",
        "    axes[0].set_xlabel('w')\n",
        "    axes[0].set_ylabel('Penalty')\n",
        "    axes[0].set_title('Penalty í•¨ìˆ˜')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True)\n",
        "    \n",
        "    # 2. Gradient\n",
        "    axes[1].plot(w, np.sign(w), label='L1 gradient: sign(w)', lw=2)\n",
        "    axes[1].plot(w, 2*w, label='L2 gradient: 2w', lw=2)\n",
        "    axes[1].set_xlabel('w')\n",
        "    axes[1].set_ylabel('Gradient')\n",
        "    axes[1].set_title('Gradient (wì— ëŒ€í•œ ë¯¸ë¶„)')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True)\n",
        "    \n",
        "    # 3. Contour (2D)\n",
        "    w1, w2 = np.meshgrid(np.linspace(-2, 2, 100), np.linspace(-2, 2, 100))\n",
        "    l1 = np.abs(w1) + np.abs(w2)\n",
        "    l2 = w1**2 + w2**2\n",
        "    \n",
        "    axes[2].contour(w1, w2, l1, levels=[0.5, 1, 1.5, 2], colors='blue', linestyles='--')\n",
        "    axes[2].contour(w1, w2, l2, levels=[0.5, 1, 1.5, 2], colors='red')\n",
        "    axes[2].set_xlabel('w1')\n",
        "    axes[2].set_ylabel('w2')\n",
        "    axes[2].set_title('ë“±ê³ ì„ : L1(íŒŒë‘), L2(ë¹¨ê°•)')\n",
        "    axes[2].set_aspect('equal')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_l1_l2()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =================================================\n",
        "# TODO: L1, L2 ì •ê·œí™” ì†ì‹¤ í•¨ìˆ˜ êµ¬í˜„\n",
        "# =================================================\n",
        "\n",
        "def l1_penalty(weights, lambda_):\n",
        "    \"\"\"\n",
        "    L1 ì •ê·œí™” penalty\n",
        "    \n",
        "    Args:\n",
        "        weights: ê°€ì¤‘ì¹˜ ë°°ì—´\n",
        "        lambda_: ì •ê·œí™” ê°•ë„\n",
        "    Returns:\n",
        "        penalty: lambda * sum(|w|)\n",
        "    \"\"\"\n",
        "    # ============================================\n",
        "    # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
        "    # ============================================\n",
        "    pass\n",
        "\n",
        "\n",
        "def l2_penalty(weights, lambda_):\n",
        "    \"\"\"\n",
        "    L2 ì •ê·œí™” penalty\n",
        "    \n",
        "    Args:\n",
        "        weights: ê°€ì¤‘ì¹˜ ë°°ì—´\n",
        "        lambda_: ì •ê·œí™” ê°•ë„\n",
        "    Returns:\n",
        "        penalty: lambda * sum(w^2)\n",
        "    \"\"\"\n",
        "    # ============================================\n",
        "    # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
        "    # ============================================\n",
        "    pass\n",
        "\n",
        "\n",
        "def l1_gradient(weights, lambda_):\n",
        "    \"\"\"\n",
        "    L1 ì •ê·œí™” gradient\n",
        "    \n",
        "    Returns:\n",
        "        gradient: lambda * sign(w)\n",
        "    \"\"\"\n",
        "    # ============================================\n",
        "    # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
        "    # ============================================\n",
        "    pass\n",
        "\n",
        "\n",
        "def l2_gradient(weights, lambda_):\n",
        "    \"\"\"\n",
        "    L2 ì •ê·œí™” gradient\n",
        "    \n",
        "    Returns:\n",
        "        gradient: 2 * lambda * w\n",
        "    \"\"\"\n",
        "    # ============================================\n",
        "    # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
        "    # ============================================\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============ í…ŒìŠ¤íŠ¸ ============\n",
        "def test_regularization():\n",
        "    w = np.array([1.0, -2.0, 0.5])\n",
        "    lambda_ = 0.1\n",
        "    \n",
        "    # L1\n",
        "    assert np.isclose(l1_penalty(w, lambda_), 0.35), \"L1 penalty ì˜¤ë¥˜\"\n",
        "    expected_l1_grad = np.array([0.1, -0.1, 0.1])\n",
        "    assert np.allclose(l1_gradient(w, lambda_), expected_l1_grad), \"L1 gradient ì˜¤ë¥˜\"\n",
        "    \n",
        "    # L2\n",
        "    assert np.isclose(l2_penalty(w, lambda_), 0.525), \"L2 penalty ì˜¤ë¥˜\"\n",
        "    expected_l2_grad = np.array([0.2, -0.4, 0.1])\n",
        "    assert np.allclose(l2_gradient(w, lambda_), expected_l2_grad), \"L2 gradient ì˜¤ë¥˜\"\n",
        "    \n",
        "    print(\"âœ… Regularization í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
        "\n",
        "test_regularization()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Ridge vs Lasso ë¹„êµ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ê³ ì°¨ì› ë°ì´í„°ì—ì„œ Ridge vs Lasso\n",
        "np.random.seed(42)\n",
        "\n",
        "# 100ê°œ feature ì¤‘ 10ê°œë§Œ ì‹¤ì œë¡œ ìœ ì˜ë¯¸\n",
        "n_features = 100\n",
        "n_informative = 10\n",
        "n_samples = 50\n",
        "\n",
        "# ì‹¤ì œ ê°€ì¤‘ì¹˜ (10ê°œë§Œ non-zero)\n",
        "true_weights = np.zeros(n_features)\n",
        "true_weights[:n_informative] = np.random.randn(n_informative) * 5\n",
        "\n",
        "X = np.random.randn(n_samples, n_features)\n",
        "y = X @ true_weights + np.random.randn(n_samples) * 0.5\n",
        "\n",
        "# ëª¨ë¸ í•™ìŠµ\n",
        "alpha = 1.0\n",
        "ridge = Ridge(alpha=alpha).fit(X, y)\n",
        "lasso = Lasso(alpha=alpha).fit(X, y)\n",
        "\n",
        "# ê°€ì¤‘ì¹˜ ë¹„êµ\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "axes[0].bar(range(n_features), true_weights)\n",
        "axes[0].set_title('True Weights (10ê°œë§Œ non-zero)')\n",
        "axes[0].set_xlabel('Feature Index')\n",
        "\n",
        "axes[1].bar(range(n_features), ridge.coef_)\n",
        "axes[1].set_title('Ridge Weights (ëª¨ë“  feature ì‚¬ìš©)')\n",
        "axes[1].set_xlabel('Feature Index')\n",
        "\n",
        "axes[2].bar(range(n_features), lasso.coef_)\n",
        "axes[2].set_title(f'Lasso Weights (non-zero: {np.sum(lasso.coef_ != 0)}ê°œ)')\n",
        "axes[2].set_xlabel('Feature Index')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Ridge non-zero weights: {np.sum(ridge.coef_ != 0)}\")\n",
        "print(f\"Lasso non-zero weights: {np.sum(lasso.coef_ != 0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Dropout êµ¬í˜„\n",
        "\n",
        "### ì›ë¦¬\n",
        "- **í•™ìŠµ ì‹œ**: ê° ë‰´ëŸ°ì„ í™•ë¥  pë¡œ ë¹„í™œì„±í™” (ì¶œë ¥ì„ 0ìœ¼ë¡œ)\n",
        "- **ì¶”ë¡  ì‹œ**: ëª¨ë“  ë‰´ëŸ° í™œì„±í™”, ì¶œë ¥ì— (1-p) ê³±í•¨\n",
        "- ë˜ëŠ” **Inverted Dropout**: í•™ìŠµ ì‹œ 1/(1-p) ê³±í•¨, ì¶”ë¡  ì‹œ ë³€ê²½ ì—†ìŒ\n",
        "\n",
        "### íš¨ê³¼\n",
        "- ì•™ìƒë¸” íš¨ê³¼: ë§¤ë²ˆ ë‹¤ë¥¸ ì„œë¸Œë„¤íŠ¸ì›Œí¬ í•™ìŠµ\n",
        "- Co-adaptation ë°©ì§€: íŠ¹ì • ë‰´ëŸ°ì— ì˜ì¡´í•˜ì§€ ì•Šë„ë¡"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =================================================\n",
        "# TODO: Dropout ë ˆì´ì–´ êµ¬í˜„ (Inverted Dropout)\n",
        "# =================================================\n",
        "\n",
        "class Dropout:\n",
        "    def __init__(self, p=0.5):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            p: dropout í™•ë¥  (ë¹„í™œì„±í™”í•  ë¹„ìœ¨)\n",
        "        \"\"\"\n",
        "        self.p = p\n",
        "        self.mask = None\n",
        "        self.training = True\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: input array\n",
        "        Returns:\n",
        "            out: dropout ì ìš©ëœ output\n",
        "        \"\"\"\n",
        "        # ============================================\n",
        "        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
        "        # training=True:\n",
        "        #   1. mask ìƒì„±: np.random.rand(*x.shape) > p\n",
        "        #   2. x * mask / (1 - p)  (inverted dropout)\n",
        "        # training=False:\n",
        "        #   ê·¸ëŒ€ë¡œ ë°˜í™˜\n",
        "        # ============================================\n",
        "        pass\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dout: upstream gradient\n",
        "        Returns:\n",
        "            dx: gradient (mask ì ìš©)\n",
        "        \"\"\"\n",
        "        # ============================================\n",
        "        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
        "        # maskëœ ìœ„ì¹˜ì˜ gradientëŠ” 0\n",
        "        # ============================================\n",
        "        pass\n",
        "    \n",
        "    def train(self):\n",
        "        self.training = True\n",
        "    \n",
        "    def eval(self):\n",
        "        self.training = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============ í…ŒìŠ¤íŠ¸ ============\n",
        "def test_dropout():\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    dropout = Dropout(p=0.5)\n",
        "    x = np.ones((100, 100))\n",
        "    \n",
        "    # Training mode\n",
        "    dropout.train()\n",
        "    out = dropout.forward(x)\n",
        "    \n",
        "    # ì•½ 50%ê°€ 0ì´ì–´ì•¼ í•¨\n",
        "    zero_ratio = np.mean(out == 0)\n",
        "    assert 0.4 < zero_ratio < 0.6, f\"Dropout ë¹„ìœ¨ ì˜¤ë¥˜: {zero_ratio}\"\n",
        "    \n",
        "    # Non-zero ê°’ì€ 2ë°° (1 / (1-0.5))\n",
        "    non_zero_vals = out[out != 0]\n",
        "    assert np.allclose(non_zero_vals, 2.0), f\"Inverted scaling ì˜¤ë¥˜: {non_zero_vals.mean()}\"\n",
        "    \n",
        "    # Eval mode - ë³€ê²½ ì—†ì–´ì•¼ í•¨\n",
        "    dropout.eval()\n",
        "    out_eval = dropout.forward(x)\n",
        "    assert np.allclose(out_eval, x), \"Eval modeì—ì„œëŠ” ë³€ê²½ ì—†ì–´ì•¼ í•¨\"\n",
        "    \n",
        "    print(\"âœ… Dropout í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
        "\n",
        "test_dropout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Dropout íš¨ê³¼ ì‹œê°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_dropout():\n",
        "    \"\"\"Dropoutì˜ ë§ˆìŠ¤í‚¹ íš¨ê³¼ ì‹œê°í™”\"\"\"\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    x = np.random.randn(8, 8)\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
        "    \n",
        "    dropout_rates = [0, 0.3, 0.5, 0.7]\n",
        "    \n",
        "    for ax, p in zip(axes, dropout_rates):\n",
        "        np.random.seed(42)\n",
        "        dropout = Dropout(p=p)\n",
        "        out = dropout.forward(x)\n",
        "        \n",
        "        im = ax.imshow(out, cmap='RdBu', vmin=-3, vmax=3)\n",
        "        ax.set_title(f'Dropout p={p}')\n",
        "        ax.axis('off')\n",
        "    \n",
        "    plt.colorbar(im, ax=axes, shrink=0.8)\n",
        "    plt.suptitle('Dropout: ê²€ì€ìƒ‰(0) = ë¹„í™œì„±í™”ëœ ë‰´ëŸ°')\n",
        "    plt.show()\n",
        "\n",
        "visualize_dropout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Early Stopping\n",
        "\n",
        "### ê°œë…\n",
        "- Validation lossê°€ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ í•™ìŠµ ì¡°ê¸° ì¢…ë£Œ\n",
        "- Overfitting ë°©ì§€ì˜ ê°„ë‹¨í•˜ë©´ì„œ íš¨ê³¼ì ì¸ ë°©ë²•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =================================================\n",
        "# TODO: Early Stopping êµ¬í˜„\n",
        "# =================================================\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience: ê°œì„  ì—†ì´ ê¸°ë‹¤ë¦´ epoch ìˆ˜\n",
        "            min_delta: ê°œì„ ìœ¼ë¡œ ì¸ì •í•  ìµœì†Œ ë³€í™”ëŸ‰\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.should_stop = False\n",
        "    \n",
        "    def __call__(self, val_loss):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            val_loss: í˜„ì¬ validation loss\n",
        "        Returns:\n",
        "            should_stop: Trueë©´ í•™ìŠµ ì¤‘ë‹¨\n",
        "        \"\"\"\n",
        "        # ============================================\n",
        "        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
        "        # 1. best_lossê°€ Noneì´ë©´ ì´ˆê¸°í™”\n",
        "        # 2. val_loss < best_loss - min_delta ì´ë©´ ê°œì„ \n",
        "        #    - best_loss ì—…ë°ì´íŠ¸, counter ë¦¬ì…‹\n",
        "        # 3. ì•„ë‹ˆë©´ counter += 1\n",
        "        # 4. counter >= patience ì´ë©´ should_stop = True\n",
        "        # ============================================\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Early Stopping ì‹œë®¬ë ˆì´ì…˜\n",
        "np.random.seed(42)\n",
        "\n",
        "# ê°€ìƒì˜ loss ê³¡ì„  (trainì€ ê³„ì† ê°ì†Œ, valì€ ë‚˜ì¤‘ì— ì¦ê°€)\n",
        "epochs = 100\n",
        "train_loss = 1.0 * np.exp(-np.arange(epochs) / 20)\n",
        "val_loss = train_loss + 0.1 * np.exp((np.arange(epochs) - 50) / 20)\n",
        "val_loss[:50] = train_loss[:50] + np.random.randn(50) * 0.02\n",
        "\n",
        "# Early stopping ì ìš©\n",
        "early_stop = EarlyStopping(patience=10)\n",
        "stop_epoch = epochs\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    if early_stop(val_loss[epoch]):\n",
        "        stop_epoch = epoch\n",
        "        break\n",
        "\n",
        "# ì‹œê°í™”\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_loss, label='Train Loss')\n",
        "plt.plot(val_loss, label='Val Loss')\n",
        "plt.axvline(x=stop_epoch, color='r', linestyle='--', label=f'Early Stop (epoch {stop_epoch})')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Early Stopping íš¨ê³¼')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Early stopping at epoch {stop_epoch}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Post-Quiz\n",
        "\n",
        "### Q1. L1 ì •ê·œí™”ê°€ feature selectionì— ìœ ìš©í•œ ì´ìœ ëŠ”?\n",
        "```\n",
        "ë‹¹ì‹ ì˜ ë‹µ: \n",
        "```\n",
        "\n",
        "### Q2. Dropout rateë¥¼ ë„ˆë¬´ ë†’ê²Œ ì„¤ì •í•˜ë©´ ì–´ë–¤ ë¬¸ì œê°€ ë°œìƒí•˜ë‚˜ìš”?\n",
        "```\n",
        "ë‹¹ì‹ ì˜ ë‹µ: \n",
        "```\n",
        "\n",
        "### Q3. Batch Normalizationì´ dropoutì˜ ëŒ€ì•ˆì´ ë  ìˆ˜ ìˆëŠ” ì´ìœ ëŠ”?\n",
        "```\n",
        "ë‹¹ì‹ ì˜ ë‹µ: \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## ì •ë‹µ\n",
        "\n",
        "<details>\n",
        "<summary>ğŸ‘‰ í´ë¦­í•˜ì—¬ ì •ë‹µ ì½”ë“œ ë³´ê¸°</summary>\n",
        "\n",
        "```python\n",
        "def l1_penalty(weights, lambda_):\n",
        "    return lambda_ * np.sum(np.abs(weights))\n",
        "\n",
        "def l2_penalty(weights, lambda_):\n",
        "    return lambda_ * np.sum(weights ** 2)\n",
        "\n",
        "def l1_gradient(weights, lambda_):\n",
        "    return lambda_ * np.sign(weights)\n",
        "\n",
        "def l2_gradient(weights, lambda_):\n",
        "    return 2 * lambda_ * weights\n",
        "\n",
        "\n",
        "class Dropout:\n",
        "    def __init__(self, p=0.5):\n",
        "        self.p = p\n",
        "        self.mask = None\n",
        "        self.training = True\n",
        "    \n",
        "    def forward(self, x):\n",
        "        if self.training:\n",
        "            self.mask = (np.random.rand(*x.shape) > self.p).astype(np.float64)\n",
        "            return x * self.mask / (1 - self.p)\n",
        "        else:\n",
        "            return x\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        return dout * self.mask / (1 - self.p)\n",
        "    \n",
        "    def train(self):\n",
        "        self.training = True\n",
        "    \n",
        "    def eval(self):\n",
        "        self.training = False\n",
        "\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.should_stop = False\n",
        "    \n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.should_stop = True\n",
        "        return self.should_stop\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>ğŸ‘‰ Quiz ì •ë‹µ</summary>\n",
        "\n",
        "### Pre-Quiz\n",
        "\n",
        "**Q1. Overfittingì´ë€?**\n",
        "ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ì— ê³¼ë„í•˜ê²Œ ì í•©ë˜ì–´ ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•œ ì¼ë°˜í™” ì„±ëŠ¥ì´ ë–¨ì–´ì§€ëŠ” í˜„ìƒ. Train lossëŠ” ë‚®ì§€ë§Œ Val/Test lossê°€ ë†’ìŒ.\n",
        "\n",
        "**Q2. L1 vs L2 ì°¨ì´**\n",
        "- L1: ê°€ì¤‘ì¹˜ ì ˆëŒ€ê°’ í•©ì— í˜ë„í‹° â†’ ì¼ë¶€ ê°€ì¤‘ì¹˜ë¥¼ ì •í™•íˆ 0ìœ¼ë¡œ (sparse)\n",
        "- L2: ê°€ì¤‘ì¹˜ ì œê³± í•©ì— í˜ë„í‹° â†’ ëª¨ë“  ê°€ì¤‘ì¹˜ë¥¼ ì‘ê²Œ (0ì— ê°€ê¹ê²Œ)\n",
        "\n",
        "**Q3. Dropout rate 0.5**\n",
        "í•™ìŠµ ì¤‘ ê° ë‰´ëŸ°ì´ 50% í™•ë¥ ë¡œ ë¹„í™œì„±í™”(ì¶œë ¥ 0). ë§¤ iterationë§ˆë‹¤ ë‹¤ë¥¸ ì„œë¸Œë„¤íŠ¸ì›Œí¬ë¡œ í•™ìŠµí•˜ëŠ” íš¨ê³¼.\n",
        "\n",
        "### Post-Quiz\n",
        "\n",
        "**Q1. L1ì´ feature selectionì— ìœ ìš©í•œ ì´ìœ **\n",
        "L1ì€ ì¼ë¶€ ê°€ì¤‘ì¹˜ë¥¼ ì •í™•íˆ 0ìœ¼ë¡œ ë§Œë“¤ì–´ í•´ë‹¹ featureë¥¼ ì œê±°í•˜ëŠ” íš¨ê³¼. ì–´ë–¤ featureê°€ ì¤‘ìš”í•œì§€ ìë™ ì„ íƒë¨.\n",
        "\n",
        "**Q2. Dropout rateê°€ ë„ˆë¬´ ë†’ìœ¼ë©´**\n",
        "ë„ˆë¬´ ë§ì€ ë‰´ëŸ°ì´ ë¹„í™œì„±í™”ë˜ì–´ í•™ìŠµì´ ë¶ˆì•ˆì •í•˜ê³  ëŠë ¤ì§. ì¶©ë¶„í•œ ì •ë³´ê°€ ì „ë‹¬ë˜ì§€ ì•Šì•„ underfitting ë°œìƒ ê°€ëŠ¥.\n",
        "\n",
        "**Q3. BatchNormì´ dropout ëŒ€ì•ˆì¸ ì´ìœ **\n",
        "BatchNormë„ ì •ê·œí™” íš¨ê³¼ê°€ ìˆì–´ overfitting ë°©ì§€. ë¯¸ë‹ˆë°°ì¹˜ì˜ ë…¸ì´ì¦ˆê°€ ì•”ë¬µì  ì •ê·œí™” ì—­í• . ë§ì€ í˜„ëŒ€ ëª¨ë¸ì—ì„œ dropout ì—†ì´ BatchNormë§Œ ì‚¬ìš©.\n",
        "\n",
        "</details>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
