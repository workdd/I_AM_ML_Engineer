{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03. Backpropagation from Scratch\n",
        "\n",
        "## í•™ìŠµ ëª©í‘œ\n",
        "- Chain Rule ì´í•´\n",
        "- Computational Graphì™€ ì—­ì „íŒŒ\n",
        "- ê°„ë‹¨í•œ MLPë¥¼ numpyë¡œ ì§ì ‘ êµ¬í˜„\n",
        "- Gradient ê³„ì‚° ì›ë¦¬ ì´í•´\n",
        "\n",
        "## ê´€ë ¨ ë©´ì ‘ ì§ˆë¬¸\n",
        "- Backpropagationì´ ì™œ íš¨ìœ¨ì ì¸ê°€ìš”?\n",
        "- Vanishing/Exploding Gradient ë¬¸ì œëŠ” ì™œ ë°œìƒí•˜ë‚˜ìš”?\n",
        "- Chain Ruleì„ ì„¤ëª…í•´ì£¼ì„¸ìš”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Pre-Quiz\n",
        "\n",
        "### Q1. Chain Ruleì´ë€?\n",
        "```\n",
        "ë‹¹ì‹ ì˜ ë‹µ: \n",
        "```\n",
        "\n",
        "### Q2. Forward passì™€ Backward passì˜ ì°¨ì´ëŠ”?\n",
        "```\n",
        "ë‹¹ì‹ ì˜ ë‹µ: \n",
        "```\n",
        "\n",
        "### Q3. Backpropagationì´ brute-force ë¯¸ë¶„ë³´ë‹¤ íš¨ìœ¨ì ì¸ ì´ìœ ëŠ”?\n",
        "```\n",
        "ë‹¹ì‹ ì˜ ë‹µ: \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# matplotlib í•œê¸€ ì„¤ì • (macOS)\n",
        "plt.rcParams['font.family'] = 'AppleGothic'\n",
        "plt.rcParams['axes.unicode_minus'] = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Chain Rule ë³µìŠµ\n",
        "\n",
        "### ê¸°ë³¸ ê³µì‹\n",
        "$$\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x}$$\n",
        "\n",
        "### ì˜ˆì‹œ: $y = f(g(x))$\n",
        "$$\\frac{dy}{dx} = \\frac{df}{dg} \\cdot \\frac{dg}{dx}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chain Rule ì˜ˆì‹œ: y = (2x + 1)^2\n",
        "# g(x) = 2x + 1\n",
        "# f(g) = g^2\n",
        "# dy/dx = df/dg * dg/dx = 2g * 2 = 4(2x + 1)\n",
        "\n",
        "def f(x):\n",
        "    return (2*x + 1)**2\n",
        "\n",
        "def analytical_grad(x):\n",
        "    \"\"\"í•´ì„ì  ë¯¸ë¶„: dy/dx = 4(2x+1)\"\"\"\n",
        "    return 4 * (2*x + 1)\n",
        "\n",
        "def numerical_grad(f, x, eps=1e-5):\n",
        "    \"\"\"ìˆ˜ì¹˜ ë¯¸ë¶„: (f(x+h) - f(x-h)) / 2h\"\"\"\n",
        "    return (f(x + eps) - f(x - eps)) / (2 * eps)\n",
        "\n",
        "# ê²€ì¦\n",
        "x = 2.0\n",
        "print(f\"x = {x}\")\n",
        "print(f\"í•´ì„ì  ë¯¸ë¶„: {analytical_grad(x)}\")\n",
        "print(f\"ìˆ˜ì¹˜ì  ë¯¸ë¶„: {numerical_grad(f, x)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Computational Graph ì´í•´\n",
        "\n",
        "### ê°„ë‹¨í•œ ì—°ì‚° ê·¸ë˜í”„\n",
        "```\n",
        "x â”€â”€â”\n",
        "    â”œâ”€â”€ [Ã—] â”€â”€> q â”€â”€â”\n",
        "y â”€â”€â”˜              â”œâ”€â”€ [+] â”€â”€> L\n",
        "         z â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "q = x * y\n",
        "L = q + z\n",
        "```\n",
        "\n",
        "### Forward & Backward\n",
        "- **Forward**: ì…ë ¥ â†’ ì¶œë ¥ (ê°’ ê³„ì‚°)\n",
        "- **Backward**: ì¶œë ¥ â†’ ì…ë ¥ (gradient ê³„ì‚°)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ê°„ë‹¨í•œ ì—°ì‚° ê·¸ë˜í”„ ì˜ˆì‹œ\n",
        "# L = x*y + z\n",
        "\n",
        "# Forward pass\n",
        "x, y, z = 2.0, 3.0, 4.0\n",
        "q = x * y      # q = 6\n",
        "L = q + z      # L = 10\n",
        "\n",
        "print(\"=== Forward Pass ===\")\n",
        "print(f\"x={x}, y={y}, z={z}\")\n",
        "print(f\"q = x*y = {q}\")\n",
        "print(f\"L = q+z = {L}\")\n",
        "\n",
        "# Backward pass (dL/d? ê³„ì‚°)\n",
        "dL_dL = 1.0           # í•­ìƒ 1ë¡œ ì‹œì‘\n",
        "dL_dq = dL_dL * 1     # L = q + z, dL/dq = 1\n",
        "dL_dz = dL_dL * 1     # L = q + z, dL/dz = 1\n",
        "dL_dx = dL_dq * y     # q = x*y, dq/dx = y\n",
        "dL_dy = dL_dq * x     # q = x*y, dq/dy = x\n",
        "\n",
        "print(\"\\n=== Backward Pass ===\")\n",
        "print(f\"dL/dL = {dL_dL}\")\n",
        "print(f\"dL/dq = {dL_dq}\")\n",
        "print(f\"dL/dz = {dL_dz}\")\n",
        "print(f\"dL/dx = {dL_dx}\")\n",
        "print(f\"dL/dy = {dL_dy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. ê¸°ë³¸ ì—°ì‚°ì˜ Local Gradient\n",
        "\n",
        "| ì—°ì‚° | Forward | Local Gradient |\n",
        "|------|---------|----------------|\n",
        "| ë§ì…ˆ: $c = a + b$ | $c = a + b$ | $\\frac{\\partial c}{\\partial a} = 1$, $\\frac{\\partial c}{\\partial b} = 1$ |\n",
        "| ê³±ì…ˆ: $c = a \\times b$ | $c = a \\times b$ | $\\frac{\\partial c}{\\partial a} = b$, $\\frac{\\partial c}{\\partial b} = a$ |\n",
        "| ReLU: $y = \\max(0, x)$ | $y = \\max(0, x)$ | $\\frac{\\partial y}{\\partial x} = \\mathbb{1}(x > 0)$ |\n",
        "| Sigmoid: $\\sigma(x)$ | $\\frac{1}{1+e^{-x}}$ | $\\sigma(x)(1-\\sigma(x))$ |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =================================================\n",
        "# TODO: ê¸°ë³¸ ì—°ì‚° í´ë˜ìŠ¤ë“¤ êµ¬í˜„\n",
        "# =================================================\n",
        "#\n",
        "# ê° í´ë˜ìŠ¤ëŠ” forward()ì™€ backward() ë©”ì„œë“œë¥¼ ê°€ì§‘ë‹ˆë‹¤.\n",
        "# forward: ê°’ì„ ê³„ì‚°í•˜ê³  backwardì— í•„ìš”í•œ ê°’ì„ ì €ì¥\n",
        "# backward: upstream gradientë¥¼ ë°›ì•„ local gradientì™€ ê³±í•´ ë°˜í™˜\n",
        "#\n",
        "# =================================================\n",
        "\n",
        "class Multiply:\n",
        "    \"\"\"ê³±ì…ˆ ì—°ì‚°: c = a * b\"\"\"\n",
        "    \n",
        "    def forward(self, a, b):\n",
        "        # ============================================\n",
        "        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
        "        # 1. a, bë¥¼ ì €ì¥ (backwardì—ì„œ ì‚¬ìš©)\n",
        "        # 2. a * b ë°˜í™˜\n",
        "        # ============================================\n",
        "        pass\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        # ============================================\n",
        "        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
        "        # dc/da = b, dc/db = a\n",
        "        # da = dout * b, db = dout * a\n",
        "        # ============================================\n",
        "        pass\n",
        "\n",
        "\n",
        "class Add:\n",
        "    \"\"\"ë§ì…ˆ ì—°ì‚°: c = a + b\"\"\"\n",
        "    \n",
        "    def forward(self, a, b):\n",
        "        # ============================================\n",
        "        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
        "        # ============================================\n",
        "        pass\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        # ============================================\n",
        "        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
        "        # dc/da = 1, dc/db = 1\n",
        "        # ============================================\n",
        "        pass\n",
        "\n",
        "\n",
        "class ReLU:\n",
        "    \"\"\"ReLU: y = max(0, x)\"\"\"\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # ============================================\n",
        "        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
        "        # mask = (x > 0) ì €ì¥\n",
        "        # ============================================\n",
        "        pass\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        # ============================================\n",
        "        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
        "        # dy/dx = 1 if x > 0 else 0\n",
        "        # ============================================\n",
        "        pass\n",
        "\n",
        "\n",
        "class Sigmoid:\n",
        "    \"\"\"Sigmoid: y = 1 / (1 + exp(-x))\"\"\"\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # ============================================\n",
        "        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
        "        # yë¥¼ ì €ì¥ (backwardì—ì„œ ì‚¬ìš©)\n",
        "        # ============================================\n",
        "        pass\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        # ============================================\n",
        "        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
        "        # dy/dx = y * (1 - y)\n",
        "        # ============================================\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============ í…ŒìŠ¤íŠ¸ ============\n",
        "def test_operations():\n",
        "    # Multiply\n",
        "    mul = Multiply()\n",
        "    out = mul.forward(3.0, 4.0)\n",
        "    da, db = mul.backward(1.0)\n",
        "    assert out == 12.0, f\"Multiply forward ì˜¤ë¥˜: {out}\"\n",
        "    assert da == 4.0 and db == 3.0, f\"Multiply backward ì˜¤ë¥˜: da={da}, db={db}\"\n",
        "    \n",
        "    # Add\n",
        "    add = Add()\n",
        "    out = add.forward(3.0, 4.0)\n",
        "    da, db = add.backward(1.0)\n",
        "    assert out == 7.0, f\"Add forward ì˜¤ë¥˜: {out}\"\n",
        "    assert da == 1.0 and db == 1.0, f\"Add backward ì˜¤ë¥˜: da={da}, db={db}\"\n",
        "    \n",
        "    # ReLU\n",
        "    relu = ReLU()\n",
        "    out = relu.forward(np.array([-1, 0, 2]))\n",
        "    dx = relu.backward(np.array([1, 1, 1]))\n",
        "    assert np.allclose(out, [0, 0, 2]), f\"ReLU forward ì˜¤ë¥˜: {out}\"\n",
        "    assert np.allclose(dx, [0, 0, 1]), f\"ReLU backward ì˜¤ë¥˜: {dx}\"\n",
        "    \n",
        "    # Sigmoid\n",
        "    sig = Sigmoid()\n",
        "    out = sig.forward(0.0)\n",
        "    dx = sig.backward(1.0)\n",
        "    assert np.isclose(out, 0.5), f\"Sigmoid forward ì˜¤ë¥˜: {out}\"\n",
        "    assert np.isclose(dx, 0.25), f\"Sigmoid backward ì˜¤ë¥˜: {dx}\"\n",
        "    \n",
        "    print(\"âœ… ê¸°ë³¸ ì—°ì‚° í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
        "\n",
        "test_operations()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Linear Layer êµ¬í˜„\n",
        "\n",
        "### Forward\n",
        "$$Y = XW + b$$\n",
        "- $X$: (batch, in_features)\n",
        "- $W$: (in_features, out_features)\n",
        "- $b$: (out_features,)\n",
        "- $Y$: (batch, out_features)\n",
        "\n",
        "### Backward\n",
        "$$\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y} \\cdot W^T$$\n",
        "$$\\frac{\\partial L}{\\partial W} = X^T \\cdot \\frac{\\partial L}{\\partial Y}$$\n",
        "$$\\frac{\\partial L}{\\partial b} = \\sum_{batch} \\frac{\\partial L}{\\partial Y}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =================================================\n",
        "# TODO: Linear Layer êµ¬í˜„\n",
        "# =================================================\n",
        "\n",
        "class Linear:\n",
        "    def __init__(self, in_features, out_features):\n",
        "        # Xavier initialization\n",
        "        scale = np.sqrt(2.0 / (in_features + out_features))\n",
        "        self.W = np.random.randn(in_features, out_features) * scale\n",
        "        self.b = np.zeros(out_features)\n",
        "        \n",
        "        # Gradients\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch, in_features)\n",
        "        Returns:\n",
        "            out: (batch, out_features)\n",
        "        \"\"\"\n",
        "        # ============================================\n",
        "        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
        "        # 1. self.xì— x ì €ì¥\n",
        "        # 2. Y = X @ W + b ê³„ì‚°\n",
        "        # ============================================\n",
        "        pass\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dout: (batch, out_features) - upstream gradient\n",
        "        Returns:\n",
        "            dx: (batch, in_features)\n",
        "        \"\"\"\n",
        "        # ============================================\n",
        "        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
        "        # dx = dout @ W^T\n",
        "        # dW = x^T @ dout\n",
        "        # db = sum(dout, axis=0)\n",
        "        # ============================================\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============ í…ŒìŠ¤íŠ¸ ============\n",
        "def test_linear():\n",
        "    linear = Linear(4, 3)\n",
        "    x = np.random.randn(2, 4)  # batch=2, in=4\n",
        "    \n",
        "    # Forward\n",
        "    out = linear.forward(x)\n",
        "    assert out.shape == (2, 3), f\"Forward shape ì˜¤ë¥˜: {out.shape}\"\n",
        "    \n",
        "    # Backward\n",
        "    dout = np.ones((2, 3))\n",
        "    dx = linear.backward(dout)\n",
        "    \n",
        "    assert dx.shape == x.shape, f\"dx shape ì˜¤ë¥˜: {dx.shape}\"\n",
        "    assert linear.dW.shape == linear.W.shape, f\"dW shape ì˜¤ë¥˜: {linear.dW.shape}\"\n",
        "    assert linear.db.shape == linear.b.shape, f\"db shape ì˜¤ë¥˜: {linear.db.shape}\"\n",
        "    \n",
        "    print(\"âœ… Linear í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
        "\n",
        "test_linear()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. MSE Loss êµ¬í˜„\n",
        "\n",
        "$$L = \\frac{1}{N} \\sum_{i} (y_i - \\hat{y}_i)^2$$\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial \\hat{y}} = \\frac{2}{N} (\\hat{y} - y)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =================================================\n",
        "# TODO: MSE Loss êµ¬í˜„\n",
        "# =================================================\n",
        "\n",
        "class MSELoss:\n",
        "    def forward(self, pred, target):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            pred: (batch, features) - ì˜ˆì¸¡ê°’\n",
        "            target: (batch, features) - ì •ë‹µ\n",
        "        Returns:\n",
        "            loss: scalar\n",
        "        \"\"\"\n",
        "        # ============================================\n",
        "        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
        "        # self.diff = pred - target ì €ì¥\n",
        "        # loss = mean((pred - target)^2)\n",
        "        # ============================================\n",
        "        pass\n",
        "    \n",
        "    def backward(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            dout: (batch, features)\n",
        "        \"\"\"\n",
        "        # ============================================\n",
        "        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
        "        # dL/dpred = 2/N * (pred - target)\n",
        "        # ============================================\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. ì „ì²´ MLP í•™ìŠµ\n",
        "\n",
        "### ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°\n",
        "```\n",
        "Input(2) â†’ Linear(2â†’4) â†’ ReLU â†’ Linear(4â†’1) â†’ Output\n",
        "```\n",
        "\n",
        "### XOR ë¬¸ì œ í•™ìŠµ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =================================================\n",
        "# TODO: ê°„ë‹¨í•œ MLP í´ë˜ìŠ¤ êµ¬í˜„\n",
        "# =================================================\n",
        "\n",
        "class SimpleMLP:\n",
        "    def __init__(self):\n",
        "        # ============================================\n",
        "        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
        "        # self.fc1 = Linear(2, 4)\n",
        "        # self.relu = ReLU()\n",
        "        # self.fc2 = Linear(4, 1)\n",
        "        # self.loss_fn = MSELoss()\n",
        "        # ============================================\n",
        "        pass\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"ìˆœì „íŒŒ\"\"\"\n",
        "        # ============================================\n",
        "        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
        "        # ============================================\n",
        "        pass\n",
        "    \n",
        "    def backward(self):\n",
        "        \"\"\"ì—­ì „íŒŒ\"\"\"\n",
        "        # ============================================\n",
        "        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
        "        # ì—­ìˆœìœ¼ë¡œ backward í˜¸ì¶œ\n",
        "        # ============================================\n",
        "        pass\n",
        "    \n",
        "    def update(self, lr):\n",
        "        \"\"\"íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ (SGD)\"\"\"\n",
        "        # ============================================\n",
        "        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
        "        # W = W - lr * dW\n",
        "        # b = b - lr * db\n",
        "        # ============================================\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# XOR ë°ì´í„°ì…‹\n",
        "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
        "y_xor = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
        "\n",
        "# í•™ìŠµ\n",
        "model = SimpleMLP()\n",
        "lr = 0.5\n",
        "epochs = 1000\n",
        "losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Forward\n",
        "    pred = model.forward(X_xor)\n",
        "    loss = model.loss_fn.forward(pred, y_xor)\n",
        "    losses.append(loss)\n",
        "    \n",
        "    # Backward\n",
        "    model.backward()\n",
        "    \n",
        "    # Update\n",
        "    model.update(lr)\n",
        "    \n",
        "    if (epoch + 1) % 200 == 0:\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "\n",
        "# ìµœì¢… ì˜ˆì¸¡\n",
        "print(\"\\nìµœì¢… ì˜ˆì¸¡:\")\n",
        "final_pred = model.forward(X_xor)\n",
        "for i in range(4):\n",
        "    print(f\"  {X_xor[i]} -> {final_pred[i][0]:.4f} (ì •ë‹µ: {y_xor[i][0]})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loss ê³¡ì„  ì‹œê°í™”\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('XOR í•™ìŠµ Loss ê³¡ì„ ')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. Gradient íë¦„ ì‹œê°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_gradient_flow():\n",
        "    \"\"\"ê° ë ˆì´ì–´ì˜ gradient í¬ê¸° ì‹œê°í™”\"\"\"\n",
        "    # í•™ìŠµëœ ëª¨ë¸ì—ì„œ í•œ ë²ˆ forward/backward\n",
        "    model.forward(X_xor)\n",
        "    model.loss_fn.forward(model.forward(X_xor), y_xor)\n",
        "    model.backward()\n",
        "    \n",
        "    # Gradient í¬ê¸°\n",
        "    gradients = {\n",
        "        'fc1.dW': np.abs(model.fc1.dW).mean(),\n",
        "        'fc1.db': np.abs(model.fc1.db).mean(),\n",
        "        'fc2.dW': np.abs(model.fc2.dW).mean(),\n",
        "        'fc2.db': np.abs(model.fc2.db).mean(),\n",
        "    }\n",
        "    \n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.bar(gradients.keys(), gradients.values())\n",
        "    plt.xlabel('Parameter')\n",
        "    plt.ylabel('Mean |Gradient|')\n",
        "    plt.title('ê° íŒŒë¼ë¯¸í„°ì˜ í‰ê·  Gradient í¬ê¸°')\n",
        "    plt.yscale('log')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "visualize_gradient_flow()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Post-Quiz\n",
        "\n",
        "### Q1. ì™œ backward ì‹œ ì—°ì‚° ì—­ìˆœìœ¼ë¡œ ì§„í–‰í•˜ë‚˜ìš”?\n",
        "```\n",
        "ë‹¹ì‹ ì˜ ë‹µ: \n",
        "```\n",
        "\n",
        "### Q2. ReLUì˜ backwardì—ì„œ inputì´ 0 ì´í•˜ì¼ ë•Œ gradientê°€ 0ì¸ ì´ìœ ëŠ”?\n",
        "```\n",
        "ë‹¹ì‹ ì˜ ë‹µ: \n",
        "```\n",
        "\n",
        "### Q3. ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì—ì„œ gradientê°€ ì‘ì•„ì§€ëŠ” ë¬¸ì œ(Vanishing)ë¥¼ í•´ê²°í•˜ëŠ” ë°©ë²•ì€?\n",
        "```\n",
        "ë‹¹ì‹ ì˜ ë‹µ: \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## ì •ë‹µ\n",
        "\n",
        "<details>\n",
        "<summary>ğŸ‘‰ í´ë¦­í•˜ì—¬ ì •ë‹µ ì½”ë“œ ë³´ê¸°</summary>\n",
        "\n",
        "```python\n",
        "class Multiply:\n",
        "    def forward(self, a, b):\n",
        "        self.a, self.b = a, b\n",
        "        return a * b\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        return dout * self.b, dout * self.a\n",
        "\n",
        "\n",
        "class Add:\n",
        "    def forward(self, a, b):\n",
        "        return a + b\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        return dout, dout\n",
        "\n",
        "\n",
        "class ReLU:\n",
        "    def forward(self, x):\n",
        "        self.mask = (x > 0)\n",
        "        return np.maximum(0, x)\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        return dout * self.mask\n",
        "\n",
        "\n",
        "class Sigmoid:\n",
        "    def forward(self, x):\n",
        "        self.y = 1 / (1 + np.exp(-x))\n",
        "        return self.y\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        return dout * self.y * (1 - self.y)\n",
        "\n",
        "\n",
        "class Linear:\n",
        "    def __init__(self, in_features, out_features):\n",
        "        scale = np.sqrt(2.0 / (in_features + out_features))\n",
        "        self.W = np.random.randn(in_features, out_features) * scale\n",
        "        self.b = np.zeros(out_features)\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "    \n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return x @ self.W + self.b\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        self.dW = self.x.T @ dout\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        return dout @ self.W.T\n",
        "\n",
        "\n",
        "class MSELoss:\n",
        "    def forward(self, pred, target):\n",
        "        self.diff = pred - target\n",
        "        self.n = pred.size\n",
        "        return np.mean(self.diff ** 2)\n",
        "    \n",
        "    def backward(self):\n",
        "        return 2 * self.diff / self.n\n",
        "\n",
        "\n",
        "class SimpleMLP:\n",
        "    def __init__(self):\n",
        "        self.fc1 = Linear(2, 4)\n",
        "        self.relu = ReLU()\n",
        "        self.fc2 = Linear(4, 1)\n",
        "        self.loss_fn = MSELoss()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.fc1.forward(x)\n",
        "        x = self.relu.forward(x)\n",
        "        x = self.fc2.forward(x)\n",
        "        return x\n",
        "    \n",
        "    def backward(self):\n",
        "        dout = self.loss_fn.backward()\n",
        "        dout = self.fc2.backward(dout)\n",
        "        dout = self.relu.backward(dout)\n",
        "        dout = self.fc1.backward(dout)\n",
        "    \n",
        "    def update(self, lr):\n",
        "        self.fc1.W -= lr * self.fc1.dW\n",
        "        self.fc1.b -= lr * self.fc1.db\n",
        "        self.fc2.W -= lr * self.fc2.dW\n",
        "        self.fc2.b -= lr * self.fc2.db\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>ğŸ‘‰ Quiz ì •ë‹µ</summary>\n",
        "\n",
        "### Pre-Quiz\n",
        "\n",
        "**Q1. Chain Ruleì´ë€?**\n",
        "í•©ì„± í•¨ìˆ˜ì˜ ë¯¸ë¶„ë²•. y=f(g(x))ì¼ ë•Œ dy/dx = df/dg Ã— dg/dx. ì¤‘ê°„ ë³€ìˆ˜ë¥¼ ê±°ì³ ìµœì¢… ì¶œë ¥ì— ëŒ€í•œ ê° ë³€ìˆ˜ì˜ ì˜í–¥ì„ ê³„ì‚°.\n",
        "\n",
        "**Q2. Forward vs Backward**\n",
        "- Forward: ì…ë ¥â†’ì¶œë ¥ ë°©í–¥ìœ¼ë¡œ ê°’ ê³„ì‚°\n",
        "- Backward: ì¶œë ¥â†’ì…ë ¥ ë°©í–¥ìœ¼ë¡œ gradient ê³„ì‚° (chain rule ì ìš©)\n",
        "\n",
        "**Q3. Backpropì´ íš¨ìœ¨ì ì¸ ì´ìœ **\n",
        "ì¤‘ê°„ ê³„ì‚° ê²°ê³¼ë¥¼ ì¬ì‚¬ìš©í•˜ê¸° ë•Œë¬¸. Brute-forceëŠ” ê° íŒŒë¼ë¯¸í„°ë§ˆë‹¤ ì „ì²´ ë„¤íŠ¸ì›Œí¬ë¥¼ ë‹¤ì‹œ ê³„ì‚°í•˜ì§€ë§Œ, backpropì€ í•œ ë²ˆì˜ forward-backwardë¡œ ëª¨ë“  gradientë¥¼ ê³„ì‚°.\n",
        "\n",
        "### Post-Quiz\n",
        "\n",
        "**Q1. ì—­ìˆœ ì§„í–‰ ì´ìœ **\n",
        "Chain rule ë•Œë¬¸. dL/dxë¥¼ êµ¬í•˜ë ¤ë©´ dL/dyê°€ ë¨¼ì € í•„ìš”. ì¶œë ¥ì—ì„œ ì‹œì‘í•´ì•¼ ëª¨ë“  upstream gradientê°€ ì¤€ë¹„ë¨.\n",
        "\n",
        "**Q2. ReLU gradientê°€ 0ì¸ ì´ìœ **\n",
        "ReLU(x) = max(0, x). xâ‰¤0ì´ë©´ ì¶œë ¥ì´ 0ìœ¼ë¡œ ìƒìˆ˜ â†’ xì˜ ë³€í™”ê°€ ì¶œë ¥ì— ì˜í–¥ ì—†ìŒ â†’ gradient = 0.\n",
        "\n",
        "**Q3. Vanishing Gradient í•´ê²°ë²•**\n",
        "- ReLU (sigmoid/tanh ëŒ€ì‹ )\n",
        "- Residual Connection (skip connection)\n",
        "- Batch/Layer Normalization\n",
        "- ì ì ˆí•œ ì´ˆê¸°í™” (Xavier, He)\n",
        "- Gradient Clipping\n",
        "\n",
        "</details>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
