{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Gradient Descent & Optimizer ì§ì ‘ êµ¬í˜„\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- Gradient Descentì˜ ë™ì‘ ì›ë¦¬ ì´í•´\n",
    "- Momentum, RMSprop, Adamì„ **ì§ì ‘ êµ¬í˜„**\n",
    "- ê° optimizerì˜ ì°¨ì´ë¥¼ ì‹œê°í™”ë¡œ í™•ì¸\n",
    "\n",
    "## ê´€ë ¨ ë©´ì ‘ ì§ˆë¬¸\n",
    "- Gradient Descentì— ëŒ€í•´ì„œ ì‰½ê²Œ ì„¤ëª…í•œë‹¤ë©´?\n",
    "- SGD, RMSprop, Adamì— ëŒ€í•´ì„œ ì•„ëŠ”ëŒ€ë¡œ ì„¤ëª…í•œë‹¤ë©´?\n",
    "- ëª¨ë©˜í…€ì˜ ìˆ˜ì‹ì„ ì ì–´ ë³¸ë‹¤ë©´?\n",
    "- Local Minima ë¬¸ì œì—ë„ ë¶ˆêµ¬í•˜ê³  ë”¥ëŸ¬ë‹ì´ ì˜ ë˜ëŠ” ì´ìœ ëŠ”?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pre-Quiz (êµ¬í˜„ ì „ì— ë¨¼ì € ë‹µí•´ë³´ì„¸ìš”)\n",
    "\n",
    "### Q1. Gradient Descentë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•œë‹¤ë©´?\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ: \n",
    "```\n",
    "\n",
    "### Q2. Momentumì´ í•„ìš”í•œ ì´ìœ ëŠ”?\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ: \n",
    "```\n",
    "\n",
    "### Q3. Adam = Momentum + RMSpropì¸ë°, ê°ê° ë¬´ì—‡ì„ ë‹´ë‹¹í•˜ë‚˜ìš”?\n",
    "```\n",
    "Momentumì´ ë‹´ë‹¹í•˜ëŠ” ê²ƒ: \n",
    "RMSpropì´ ë‹´ë‹¹í•˜ëŠ” ê²ƒ: \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì • (macOS)\n",
    "plt.rcParams['font.family'] = 'AppleGothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. ì†ì‹¤ í•¨ìˆ˜ ì •ì˜ (ì œê³µë¨)\n",
    "\n",
    "ìµœì í™” í…ŒìŠ¤íŠ¸ìš© í•¨ìˆ˜ë“¤ì…ë‹ˆë‹¤. ì´ ë¶€ë¶„ì€ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic(x, y):\n",
    "    \"\"\"ê°„ë‹¨í•œ ì´ì°¨ í•¨ìˆ˜: f(x,y) = xÂ² + 5yÂ²\n",
    "    \n",
    "    íŠ¹ì§•: y ë°©í–¥ì´ x ë°©í–¥ë³´ë‹¤ 5ë°° ê°€íŒŒë¦„ â†’ ì§€ê·¸ì¬ê·¸ í˜„ìƒ ë°œìƒ\n",
    "    Global minimum: (0, 0)\n",
    "    \"\"\"\n",
    "    return x**2 + 5*y**2\n",
    "\n",
    "def quadratic_gradient(x, y):\n",
    "    \"\"\"ì´ì°¨ í•¨ìˆ˜ì˜ gradient\"\"\"\n",
    "    return np.array([2*x, 10*y])\n",
    "\n",
    "\n",
    "def rosenbrock(x, y):\n",
    "    \"\"\"Rosenbrock í•¨ìˆ˜: ì¢ì€ ê³¨ì§œê¸° í˜•íƒœ\n",
    "    \n",
    "    Global minimum: (1, 1) = 0\n",
    "    ë‚œì´ë„ ë†’ìŒ: ê³¨ì§œê¸°ë¥¼ ë”°ë¼ê°€ì•¼ í•¨\n",
    "    \"\"\"\n",
    "    return (1 - x)**2 + 100*(y - x**2)**2\n",
    "\n",
    "def rosenbrock_gradient(x, y):\n",
    "    \"\"\"Rosenbrock í•¨ìˆ˜ì˜ gradient\"\"\"\n",
    "    dx = -2*(1 - x) - 400*x*(y - x**2)\n",
    "    dy = 200*(y - x**2)\n",
    "    return np.array([dx, dy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Vanilla Gradient Descent êµ¬í˜„\n",
    "\n",
    "ê°€ì¥ ê¸°ë³¸ì ì¸ GDì…ë‹ˆë‹¤.\n",
    "\n",
    "### ìˆ˜ì‹\n",
    "$$\\theta_{t+1} = \\theta_t - \\eta \\cdot \\nabla L$$\n",
    "\n",
    "- $\\eta$: learning rate\n",
    "- $\\nabla L$: gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# TODO: Vanilla Gradient Descent êµ¬í˜„\n",
    "# =================================================\n",
    "#\n",
    "# ìˆ˜ì‹: Î¸_new = Î¸ - lr * gradient\n",
    "#\n",
    "# ìš”êµ¬ì‚¬í•­:\n",
    "#   - __init__(self, lr): learning rate ì €ì¥\n",
    "#   - update(self, params, grads): ì—…ë°ì´íŠ¸ëœ params ë°˜í™˜\n",
    "#\n",
    "# =================================================\n",
    "\n",
    "class VanillaGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        # TODO: learning rate ì €ì¥\n",
    "        pass\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        # TODO: params ì—…ë°ì´íŠ¸ í›„ ë°˜í™˜\n",
    "        # íŒíŠ¸: params - lr * grads\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ í…ŒìŠ¤íŠ¸ ============\n",
    "def test_vanilla_gd():\n",
    "    opt = VanillaGD(lr=0.1)\n",
    "    params = np.array([1.0, 2.0])\n",
    "    grads = np.array([0.5, 0.5])\n",
    "    \n",
    "    new_params = opt.update(params, grads)\n",
    "    expected = np.array([0.95, 1.95])  # 1 - 0.1*0.5, 2 - 0.1*0.5\n",
    "    \n",
    "    assert new_params is not None, \"update ë©”ì„œë“œ êµ¬í˜„ í•„ìš”\"\n",
    "    assert np.allclose(new_params, expected), f\"Expected {expected}, got {new_params}\"\n",
    "    print(\"âœ… VanillaGD í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
    "\n",
    "test_vanilla_gd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Momentum êµ¬í˜„\n",
    "\n",
    "### í•µì‹¬ ì•„ì´ë””ì–´\n",
    "- ê³¼ê±° gradient ë°©í–¥ì„ **ê¸°ì–µ**í•˜ì—¬ ê´€ì„±ì²˜ëŸ¼ ì‘ìš©\n",
    "- ì§€ê·¸ì¬ê·¸ í˜„ìƒ ê°ì†Œ, ìˆ˜ë ´ ì†ë„ í–¥ìƒ\n",
    "\n",
    "### ìˆ˜ì‹\n",
    "$$v_t = \\beta \\cdot v_{t-1} + \\eta \\cdot \\nabla L$$\n",
    "$$\\theta_{t+1} = \\theta_t - v_t$$\n",
    "\n",
    "- $\\beta$: momentum ê³„ìˆ˜ (ë³´í†µ 0.9)\n",
    "- $v_t$: velocity (ì†ë„)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# TODO: Momentum SGD êµ¬í˜„\n",
    "# =================================================\n",
    "#\n",
    "# ìˆ˜ì‹:\n",
    "#   v = Î² * v + lr * gradient\n",
    "#   Î¸_new = Î¸ - v\n",
    "#\n",
    "# ìš”êµ¬ì‚¬í•­:\n",
    "#   - __init__(self, lr, momentum): lr, momentum(Î²) ì €ì¥, velocity ì´ˆê¸°í™”\n",
    "#   - update(self, params, grads): velocity ì—…ë°ì´íŠ¸ í›„ params ë°˜í™˜\n",
    "#\n",
    "# ì£¼ì˜:\n",
    "#   - velocityëŠ” ì²˜ìŒì— Noneìœ¼ë¡œ ë‘ê³ , ì²« updateì—ì„œ ì´ˆê¸°í™”\n",
    "#   - velocity shapeì€ paramsì™€ ë™ì¼í•´ì•¼ í•¨\n",
    "#\n",
    "# =================================================\n",
    "\n",
    "class MomentumGD:\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        # TODO: lr, momentum ì €ì¥, velocityëŠ” Noneìœ¼ë¡œ ì´ˆê¸°í™”\n",
    "        pass\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        # TODO:\n",
    "        # 1. velocityê°€ Noneì´ë©´ zeros_like(params)ë¡œ ì´ˆê¸°í™”\n",
    "        # 2. velocity ì—…ë°ì´íŠ¸: v = Î² * v + lr * grads\n",
    "        # 3. params ì—…ë°ì´íŠ¸: params - velocity\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ í…ŒìŠ¤íŠ¸ ============\n",
    "def test_momentum():\n",
    "    opt = MomentumGD(lr=0.1, momentum=0.9)\n",
    "    params = np.array([1.0, 2.0])\n",
    "    grads = np.array([0.5, 0.5])\n",
    "    \n",
    "    # ì²« ë²ˆì§¸ ì—…ë°ì´íŠ¸\n",
    "    new_params = opt.update(params, grads)\n",
    "    assert new_params is not None, \"update ë©”ì„œë“œ êµ¬í˜„ í•„ìš”\"\n",
    "    \n",
    "    # ë‘ ë²ˆì§¸ ì—…ë°ì´íŠ¸ (momentum íš¨ê³¼ í™•ì¸)\n",
    "    new_params2 = opt.update(new_params, grads)\n",
    "    \n",
    "    # velocityê°€ ëˆ„ì ë˜ì–´ì•¼ í•¨\n",
    "    print(f\"1st update: {new_params}\")\n",
    "    print(f\"2nd update: {new_params2}\")\n",
    "    print(\"âœ… MomentumGD í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
    "\n",
    "test_momentum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. RMSprop êµ¬í˜„\n",
    "\n",
    "### í•µì‹¬ ì•„ì´ë””ì–´\n",
    "- gradientì˜ **í¬ê¸°(ì œê³±)**ë¥¼ ëˆ„ì \n",
    "- ë§ì´ ì—…ë°ì´íŠ¸ëœ ë°©í–¥ì€ lr ê°ì†Œ, ì ê²Œ ì—…ë°ì´íŠ¸ëœ ë°©í–¥ì€ lr ìœ ì§€\n",
    "- **Adaptive Learning Rate**\n",
    "\n",
    "### ìˆ˜ì‹\n",
    "$$E[g^2]_t = \\gamma \\cdot E[g^2]_{t-1} + (1-\\gamma) \\cdot g_t^2$$\n",
    "$$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} \\cdot g_t$$\n",
    "\n",
    "- $\\gamma$: decay rate (ë³´í†µ 0.99)\n",
    "- $\\epsilon$: 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€ (ë³´í†µ 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# TODO: RMSprop êµ¬í˜„\n",
    "# =================================================\n",
    "#\n",
    "# ìˆ˜ì‹:\n",
    "#   cache = Î³ * cache + (1-Î³) * gradsÂ²\n",
    "#   Î¸_new = Î¸ - lr * grads / (âˆšcache + Îµ)\n",
    "#\n",
    "# ìš”êµ¬ì‚¬í•­:\n",
    "#   - __init__(self, lr, decay, epsilon): íŒŒë¼ë¯¸í„° ì €ì¥, cacheëŠ” None\n",
    "#   - update(self, params, grads): cache ì—…ë°ì´íŠ¸ í›„ params ë°˜í™˜\n",
    "#\n",
    "# =================================================\n",
    "\n",
    "class RMSprop:\n",
    "    def __init__(self, lr=0.01, decay=0.99, epsilon=1e-8):\n",
    "        # TODO: lr, decay, epsilon ì €ì¥, cacheëŠ” None\n",
    "        pass\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        # TODO:\n",
    "        # 1. cacheê°€ Noneì´ë©´ zeros_like(params)ë¡œ ì´ˆê¸°í™”\n",
    "        # 2. cache ì—…ë°ì´íŠ¸: decay * cache + (1-decay) * grads**2\n",
    "        # 3. params ì—…ë°ì´íŠ¸: params - lr * grads / (sqrt(cache) + epsilon)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ í…ŒìŠ¤íŠ¸ ============\n",
    "def test_rmsprop():\n",
    "    opt = RMSprop(lr=0.1, decay=0.9, epsilon=1e-8)\n",
    "    params = np.array([1.0, 2.0])\n",
    "    grads = np.array([0.5, 1.0])  # y ë°©í–¥ gradientê°€ ë” í¼\n",
    "    \n",
    "    new_params = opt.update(params, grads)\n",
    "    assert new_params is not None, \"update ë©”ì„œë“œ êµ¬í˜„ í•„ìš”\"\n",
    "    \n",
    "    # y ë°©í–¥ì€ gradientê°€ ì»¤ì„œ lrì´ ì¤„ì–´ë“¤ì–´ì•¼ í•¨\n",
    "    print(f\"ì—…ë°ì´íŠ¸ í›„: {new_params}\")\n",
    "    print(\"âœ… RMSprop í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
    "\n",
    "test_rmsprop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Adam êµ¬í˜„ (í•µì‹¬!)\n",
    "\n",
    "### í•µì‹¬ ì•„ì´ë””ì–´\n",
    "**Adam = Momentum + RMSprop**\n",
    "- 1ì°¨ ëª¨ë©˜íŠ¸ (m): gradientì˜ ì´ë™ í‰ê·  â†’ **ë°©í–¥**\n",
    "- 2ì°¨ ëª¨ë©˜íŠ¸ (v): gradient ì œê³±ì˜ ì´ë™ í‰ê·  â†’ **í¬ê¸° ì¡°ì ˆ**\n",
    "- Bias Correction: ì´ˆê¸° ë‹¨ê³„ì—ì„œ 0ì— í¸í–¥ë˜ëŠ” ê²ƒ ë³´ì •\n",
    "\n",
    "### ìˆ˜ì‹\n",
    "$$m_t = \\beta_1 \\cdot m_{t-1} + (1-\\beta_1) \\cdot g_t$$\n",
    "$$v_t = \\beta_2 \\cdot v_{t-1} + (1-\\beta_2) \\cdot g_t^2$$\n",
    "$$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$$\n",
    "$$\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$$\n",
    "$$\\theta_{t+1} = \\theta_t - \\frac{\\eta \\cdot \\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$$\n",
    "\n",
    "- $\\beta_1$: 1ì°¨ ëª¨ë©˜íŠ¸ decay (ë³´í†µ 0.9)\n",
    "- $\\beta_2$: 2ì°¨ ëª¨ë©˜íŠ¸ decay (ë³´í†µ 0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# TODO: Adam êµ¬í˜„\n",
    "# =================================================\n",
    "#\n",
    "# ìˆ˜ì‹:\n",
    "#   m = Î²â‚ * m + (1-Î²â‚) * grads        (1ì°¨ ëª¨ë©˜íŠ¸)\n",
    "#   v = Î²â‚‚ * v + (1-Î²â‚‚) * gradsÂ²       (2ì°¨ ëª¨ë©˜íŠ¸)\n",
    "#   m_hat = m / (1 - Î²â‚^t)             (bias correction)\n",
    "#   v_hat = v / (1 - Î²â‚‚^t)\n",
    "#   Î¸_new = Î¸ - lr * m_hat / (âˆšv_hat + Îµ)\n",
    "#\n",
    "# ìš”êµ¬ì‚¬í•­:\n",
    "#   - __init__: lr, beta1, beta2, epsilon ì €ì¥\n",
    "#               m, vëŠ” None, t=0ìœ¼ë¡œ ì´ˆê¸°í™”\n",
    "#   - update: t ì¦ê°€, m/v ì—…ë°ì´íŠ¸, bias correction ì ìš©\n",
    "#\n",
    "# =================================================\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, lr=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        # TODO: íŒŒë¼ë¯¸í„°ë“¤ ì €ì¥\n",
    "        # m, v = None, t = 0\n",
    "        pass\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        # TODO:\n",
    "        # 1. m, vê°€ Noneì´ë©´ ì´ˆê¸°í™”\n",
    "        # 2. t += 1\n",
    "        # 3. m, v ì—…ë°ì´íŠ¸\n",
    "        # 4. bias correction (m_hat, v_hat)\n",
    "        # 5. params ì—…ë°ì´íŠ¸\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ í…ŒìŠ¤íŠ¸ ============\n",
    "def test_adam():\n",
    "    opt = Adam(lr=0.1, beta1=0.9, beta2=0.999)\n",
    "    params = np.array([1.0, 2.0])\n",
    "    grads = np.array([0.5, 0.5])\n",
    "    \n",
    "    # ì—¬ëŸ¬ ë²ˆ ì—…ë°ì´íŠ¸\n",
    "    for i in range(3):\n",
    "        params = opt.update(params, grads)\n",
    "        print(f\"Step {i+1}: {params}\")\n",
    "    \n",
    "    assert params is not None, \"update ë©”ì„œë“œ êµ¬í˜„ í•„ìš”\"\n",
    "    print(\"âœ… Adam í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
    "\n",
    "test_adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. ì‹œê°í™” ë° ë¹„êµ (ì œê³µë¨)\n",
    "\n",
    "êµ¬í˜„í•œ optimizerë“¤ì„ ë¹„êµí•´ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(func, grad_func, optimizer, start_point, n_iterations=100):\n",
    "    \"\"\"ìµœì í™” ì‹¤í–‰ ë° ê¶¤ì  ë°˜í™˜\"\"\"\n",
    "    point = np.array(start_point, dtype=float)\n",
    "    trajectory = [point.copy()]\n",
    "    losses = [func(point[0], point[1])]\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        grad = grad_func(point[0], point[1])\n",
    "        point = optimizer.update(point, grad)\n",
    "        trajectory.append(point.copy())\n",
    "        losses.append(func(point[0], point[1]))\n",
    "    \n",
    "    return np.array(trajectory), np.array(losses)\n",
    "\n",
    "\n",
    "def plot_comparison(func, trajectories_dict, xlim=(-4, 4), ylim=(-4, 4), title=\"Optimizer ë¹„êµ\"):\n",
    "    \"\"\"ì—¬ëŸ¬ optimizerì˜ ê¶¤ì  ë¹„êµ\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # ë“±ê³ ì„ \n",
    "    x = np.linspace(xlim[0], xlim[1], 100)\n",
    "    y = np.linspace(ylim[0], ylim[1], 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = np.log10(func(X, Y) + 1)\n",
    "    \n",
    "    axes[0].contour(X, Y, Z, levels=30, cmap='viridis', alpha=0.5)\n",
    "    axes[0].contourf(X, Y, Z, levels=30, cmap='viridis', alpha=0.3)\n",
    "    \n",
    "    colors = ['red', 'blue', 'green', 'orange']\n",
    "    for (name, (traj, losses)), color in zip(trajectories_dict.items(), colors):\n",
    "        axes[0].plot(traj[:, 0], traj[:, 1], 'o-', color=color,\n",
    "                    label=name, markersize=3, linewidth=1.5)\n",
    "        axes[0].scatter(traj[0, 0], traj[0, 1], color=color, s=100, marker='*')\n",
    "        \n",
    "        axes[1].plot(losses, color=color, label=name, linewidth=2)\n",
    "    \n",
    "    axes[0].set_xlabel('x')\n",
    "    axes[0].set_ylabel('y')\n",
    "    axes[0].set_title(f'{title} - ê¶¤ì ')\n",
    "    axes[0].legend()\n",
    "    axes[0].set_xlim(xlim)\n",
    "    axes[0].set_ylim(ylim)\n",
    "    \n",
    "    axes[1].set_xlabel('Iteration')\n",
    "    axes[1].set_ylabel('Loss (log scale)')\n",
    "    axes[1].set_yscale('log')\n",
    "    axes[1].set_title(f'{title} - Loss ë³€í™”')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ì°¨ í•¨ìˆ˜ì—ì„œ ë¹„êµ\n",
    "start = [-3.5, 3.5]\n",
    "n_iter = 50\n",
    "\n",
    "results = {\n",
    "    'Vanilla GD': optimize(quadratic, quadratic_gradient, VanillaGD(lr=0.1), start, n_iter),\n",
    "    'Momentum': optimize(quadratic, quadratic_gradient, MomentumGD(lr=0.1, momentum=0.9), start, n_iter),\n",
    "    'RMSprop': optimize(quadratic, quadratic_gradient, RMSprop(lr=0.1), start, n_iter),\n",
    "    'Adam': optimize(quadratic, quadratic_gradient, Adam(lr=0.1), start, n_iter),\n",
    "}\n",
    "\n",
    "plot_comparison(quadratic, results, title=\"ì´ì°¨ í•¨ìˆ˜ (xÂ² + 5yÂ²)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rosenbrock í•¨ìˆ˜ì—ì„œ ë¹„êµ (ë” ì–´ë ¤ìš´ ì¼€ì´ìŠ¤)\n",
    "start = [-1.5, 1.5]\n",
    "n_iter = 300\n",
    "\n",
    "results = {\n",
    "    'Vanilla GD': optimize(rosenbrock, rosenbrock_gradient, VanillaGD(lr=0.001), start, n_iter),\n",
    "    'Momentum': optimize(rosenbrock, rosenbrock_gradient, MomentumGD(lr=0.001, momentum=0.9), start, n_iter),\n",
    "    'RMSprop': optimize(rosenbrock, rosenbrock_gradient, RMSprop(lr=0.01), start, n_iter),\n",
    "    'Adam': optimize(rosenbrock, rosenbrock_gradient, Adam(lr=0.01), start, n_iter),\n",
    "}\n",
    "\n",
    "plot_comparison(rosenbrock, results, xlim=(-2, 2), ylim=(-1, 3), title=\"Rosenbrock í•¨ìˆ˜\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Post-Quiz (ì‹œê°í™” ê²°ê³¼ë¥¼ ë³´ê³  ë‹µí•´ë³´ì„¸ìš”)\n",
    "\n",
    "### Q1. ì´ì°¨ í•¨ìˆ˜ì—ì„œ Vanilla GDê°€ ì§€ê·¸ì¬ê·¸ë¡œ ì›€ì§ì´ëŠ” ì´ìœ ëŠ”?\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ: \n",
    "```\n",
    "\n",
    "### Q2. Momentumì´ ì§€ê·¸ì¬ê·¸ë¥¼ ì¤„ì´ëŠ” ì›ë¦¬ëŠ”?\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ: \n",
    "```\n",
    "\n",
    "### Q3. Rosenbrock í•¨ìˆ˜ì—ì„œ Adamì´ ê°€ì¥ ì˜ ëœ ì´ìœ ëŠ”?\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ: \n",
    "```\n",
    "\n",
    "### Q4. ì‹¤ë¬´ì—ì„œ Adamì„ ê¸°ë³¸ìœ¼ë¡œ ì“°ëŠ” ì´ìœ ëŠ”?\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ: \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ì •ë‹µ ë° í•´ì„¤\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ‘‰ í´ë¦­í•˜ì—¬ ì •ë‹µ ë³´ê¸°</summary>\n",
    "\n",
    "### Pre-Quiz ì •ë‹µ\n",
    "\n",
    "**Q1. Gradient Descent í•œ ë¬¸ì¥ ì •ì˜**\n",
    "> ì†ì‹¤ í•¨ìˆ˜ì˜ gradient(ê¸°ìš¸ê¸°) ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ê¸ˆì”© ì´ë™ì‹œì¼œ ìµœì†Ÿê°’ì„ ì°¾ëŠ” ìµœì í™” ì•Œê³ ë¦¬ì¦˜\n",
    "\n",
    "**Q2. Momentumì´ í•„ìš”í•œ ì´ìœ **\n",
    "> - ì§€ê·¸ì¬ê·¸ í˜„ìƒ ê°ì†Œ (ìˆ˜ì§ ë°©í–¥ ì§„ë™ ìƒì‡„)\n",
    "> - Local minima íƒˆì¶œ ê°€ëŠ¥ì„± ì¦ê°€\n",
    "> - ìˆ˜ë ´ ì†ë„ í–¥ìƒ\n",
    "\n",
    "**Q3. Adamì˜ êµ¬ì„±**\n",
    "> - Momentum (1ì°¨ ëª¨ë©˜íŠ¸): gradientì˜ **ë°©í–¥** ì •ë³´ (ì–´ë””ë¡œ ê°ˆì§€)\n",
    "> - RMSprop (2ì°¨ ëª¨ë©˜íŠ¸): gradientì˜ **í¬ê¸°** ì •ë³´ (ì–¼ë§ˆë‚˜ ê°ˆì§€)\n",
    "\n",
    "---\n",
    "\n",
    "### Post-Quiz ì •ë‹µ\n",
    "\n",
    "**Q1. ì§€ê·¸ì¬ê·¸ ì´ìœ **\n",
    "> - y ë°©í–¥ gradientê°€ x ë°©í–¥ë³´ë‹¤ 5ë°° í¼ (5yÂ² vs xÂ²)\n",
    "> - ê°™ì€ lr ì ìš© ì‹œ y ë°©í–¥ìœ¼ë¡œ ê³¼í•˜ê²Œ ì´ë™\n",
    "> - ë‹¤ìŒ ìŠ¤í…ì—ì„œ ë°˜ëŒ€ë¡œ ë³´ì • â†’ ì§€ê·¸ì¬ê·¸\n",
    "\n",
    "**Q2. Momentum ì›ë¦¬**\n",
    "> - ê³¼ê±° velocityë¥¼ ê¸°ì–µ\n",
    "> - ìˆ˜ì§(ì§€ê·¸ì¬ê·¸) ë°©í–¥: ë°˜ëŒ€ ë°©í–¥ì´ ìƒì‡„ë¨\n",
    "> - ìˆ˜í‰(ëª©í‘œ) ë°©í–¥: ê°™ì€ ë°©í–¥ì´ ëˆ„ì ë¨\n",
    "\n",
    "**Q3. Adamì´ ì˜ ë˜ëŠ” ì´ìœ **\n",
    "> - Momentum: ì¢ì€ ê³¨ì§œê¸°ì—ì„œ ë°©í–¥ ìœ ì§€\n",
    "> - RMSprop: ë°©í–¥ë³„ë¡œ ì ì‘ì  lr ì¡°ì ˆ\n",
    "> - ë‘ ì¥ì  ê²°í•© â†’ ë³µì¡í•œ ì§€í˜•ì—ì„œë„ íš¨ê³¼ì \n",
    "\n",
    "**Q4. Adamì„ ê¸°ë³¸ìœ¼ë¡œ ì“°ëŠ” ì´ìœ **\n",
    "> - lrì— ëœ ë¯¼ê° (ëŒ€ë¶€ë¶„ 0.001ë¡œ ì‹œì‘í•´ë„ OK)\n",
    "> - ëŒ€ë¶€ë¶„ì˜ ë¬¸ì œì—ì„œ ì•ˆì •ì \n",
    "> - ë³„ë„ íŠœë‹ ì—†ì´ ì¢‹ì€ ì„±ëŠ¥\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## êµ¬í˜„ ì •ë‹µ ì½”ë“œ\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ‘‰ í´ë¦­í•˜ì—¬ ì •ë‹µ ì½”ë“œ ë³´ê¸°</summary>\n",
    "\n",
    "```python\n",
    "class VanillaGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        return params - self.lr * grads\n",
    "\n",
    "\n",
    "class MomentumGD:\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.velocity = None\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        if self.velocity is None:\n",
    "            self.velocity = np.zeros_like(params)\n",
    "        \n",
    "        self.velocity = self.momentum * self.velocity + self.lr * grads\n",
    "        return params - self.velocity\n",
    "\n",
    "\n",
    "class RMSprop:\n",
    "    def __init__(self, lr=0.01, decay=0.99, epsilon=1e-8):\n",
    "        self.lr = lr\n",
    "        self.decay = decay\n",
    "        self.epsilon = epsilon\n",
    "        self.cache = None\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        if self.cache is None:\n",
    "            self.cache = np.zeros_like(params)\n",
    "        \n",
    "        self.cache = self.decay * self.cache + (1 - self.decay) * grads**2\n",
    "        return params - self.lr * grads / (np.sqrt(self.cache) + self.epsilon)\n",
    "\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, lr=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.t = 0\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m = np.zeros_like(params)\n",
    "            self.v = np.zeros_like(params)\n",
    "        \n",
    "        self.t += 1\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * grads\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * grads**2\n",
    "        \n",
    "        m_hat = self.m / (1 - self.beta1**self.t)\n",
    "        v_hat = self.v / (1 - self.beta2**self.t)\n",
    "        \n",
    "        return params - self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
