{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Layer Normalization êµ¬í˜„\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- Layer Normalizationì„ **ì§ì ‘ êµ¬í˜„**\n",
    "- Batch Norm vs Layer Norm ì°¨ì´ ì´í•´\n",
    "- ì™œ Transformerì—ì„œ Layer Normì„ ì“°ëŠ”ì§€\n",
    "\n",
    "## ê´€ë ¨ ë©´ì ‘ ì§ˆë¬¸\n",
    "- Batch Normalizationê³¼ Layer Normalizationì˜ ì°¨ì´ëŠ”?\n",
    "- Transformerì—ì„œ Layer Normì„ ì“°ëŠ” ì´ìœ ëŠ”?\n",
    "- Pre-LN vs Post-LNì˜ ì°¨ì´ëŠ”?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pre-Quiz\n",
    "\n",
    "### Q1. Batch Normì€ ì–´ëŠ ì¶•ìœ¼ë¡œ ì •ê·œí™”í•˜ë‚˜ìš”? Layer Normì€?\n",
    "```\n",
    "Batch Norm: \n",
    "Layer Norm: \n",
    "```\n",
    "\n",
    "### Q2. NLPì—ì„œ Batch Norm ëŒ€ì‹  Layer Normì„ ì“°ëŠ” ì´ìœ ëŠ”?\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ: \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['font.family'] = 'AppleGothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Layer Normalization êµ¬í˜„\n",
    "\n",
    "### ìˆ˜ì‹\n",
    "$$y = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta$$\n",
    "\n",
    "- $\\mu$: ë§ˆì§€ë§‰ ì°¨ì›ì˜ í‰ê· \n",
    "- $\\sigma^2$: ë§ˆì§€ë§‰ ì°¨ì›ì˜ ë¶„ì‚°\n",
    "- $\\gamma$, $\\beta$: í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =================================================\n# TODO: Layer Normalization êµ¬í˜„\n# =================================================\n#\n# $$\\hat{x} = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$$\n# $$y = \\gamma \\hat{x} + \\beta$$\n#\n# - Î¼, ÏƒÂ²: ë§ˆì§€ë§‰ ì¶•ì—ì„œ ê³„ì‚° (ê° sampleë³„ ë…ë¦½)\n# - Î³, Î²: learnable parameters (shape = normalized_shape)\n#\n# Batch Normê³¼ ì°¨ì´: ì •ê·œí™” ì¶•ì´ ë‹¤ë¦„!\n#\n# =================================================\n\nclass LayerNorm(nn.Module):\n    def __init__(self, normalized_shape, eps=1e-6):\n        \"\"\"\n        Args:\n            normalized_shape: ì •ê·œí™”í•  ì°¨ì› í¬ê¸° (ë³´í†µ d_model)\n            eps: ë¶„ëª¨ ì•ˆì •í™”\n        \"\"\"\n        super().__init__()\n        \n        if isinstance(normalized_shape, int):\n            normalized_shape = (normalized_shape,)\n        \n        # ============================================\n        # Î³, Î² íŒŒë¼ë¯¸í„° ì •ì˜\n        # ============================================\n        \n        pass\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"(..., normalized_shape) â†’ ë™ì¼ shape\"\"\"\n        # ============================================\n        # ìœ„ ìˆ˜ì‹ êµ¬í˜„ (mean, varëŠ” dim=-1)\n        # ============================================\n        \n        pass"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ í…ŒìŠ¤íŠ¸ ============\n",
    "def test_layer_norm():\n",
    "    batch, seq, d_model = 2, 10, 64\n",
    "    \n",
    "    # ì§ì ‘ êµ¬í˜„\n",
    "    our_ln = LayerNorm(d_model)\n",
    "    # PyTorch êµ¬í˜„\n",
    "    pytorch_ln = nn.LayerNorm(d_model)\n",
    "    \n",
    "    # ë™ì¼í•œ ê°€ì¤‘ì¹˜ ì„¤ì •\n",
    "    pytorch_ln.weight.data = our_ln.gamma.data.clone()\n",
    "    pytorch_ln.bias.data = our_ln.beta.data.clone()\n",
    "    \n",
    "    x = torch.randn(batch, seq, d_model)\n",
    "    \n",
    "    our_output = our_ln(x)\n",
    "    pytorch_output = pytorch_ln(x)\n",
    "    \n",
    "    assert our_output.shape == x.shape, f\"Shape ì˜¤ë¥˜: {our_output.shape}\"\n",
    "    assert torch.allclose(our_output, pytorch_output, atol=1e-5), \"PyTorchì™€ ë¶ˆì¼ì¹˜!\"\n",
    "    \n",
    "    # ì •ê·œí™” í™•ì¸: ë§ˆì§€ë§‰ ì¶•ì˜ mean â‰ˆ 0, std â‰ˆ 1\n",
    "    mean = our_output.mean(dim=-1)\n",
    "    std = our_output.std(dim=-1)\n",
    "    \n",
    "    print(\"âœ… Layer Norm í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
    "    print(f\"   Output mean (should â‰ˆ 0): {mean.mean():.6f}\")\n",
    "    print(f\"   Output std (should â‰ˆ 1): {std.mean():.6f}\")\n",
    "\n",
    "test_layer_norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Batch Norm vs Layer Norm ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# ë°ì´í„° (batch=4, features=3)\n",
    "data = [\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9],\n",
    "    [10, 11, 12]\n",
    "]\n",
    "\n",
    "# Batch Norm: ì„¸ë¡œ ë°©í–¥ (ê°™ì€ feature across batch)\n",
    "axes[0].imshow([[1], [1], [1], [1]], cmap='Blues', alpha=0.5, aspect='auto', extent=[-0.5, 0.5, 3.5, -0.5])\n",
    "axes[0].imshow([[1], [1], [1], [1]], cmap='Greens', alpha=0.5, aspect='auto', extent=[0.5, 1.5, 3.5, -0.5])\n",
    "axes[0].imshow([[1], [1], [1], [1]], cmap='Reds', alpha=0.5, aspect='auto', extent=[1.5, 2.5, 3.5, -0.5])\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(3):\n",
    "        axes[0].text(j, i, str(data[i][j]), ha='center', va='center', fontsize=14)\n",
    "\n",
    "axes[0].set_xticks([0, 1, 2])\n",
    "axes[0].set_yticks([0, 1, 2, 3])\n",
    "axes[0].set_xticklabels(['f1', 'f2', 'f3'])\n",
    "axes[0].set_yticklabels(['batch1', 'batch2', 'batch3', 'batch4'])\n",
    "axes[0].set_title('Batch Normalization\\nê° featureë¥¼ batch ë°©í–¥ìœ¼ë¡œ ì •ê·œí™”')\n",
    "\n",
    "# Layer Norm: ê°€ë¡œ ë°©í–¥ (ê°™ì€ sampleì˜ ëª¨ë“  features)\n",
    "axes[1].imshow([[1, 1, 1]], cmap='Blues', alpha=0.5, aspect='auto', extent=[-0.5, 2.5, 0.5, -0.5])\n",
    "axes[1].imshow([[1, 1, 1]], cmap='Greens', alpha=0.5, aspect='auto', extent=[-0.5, 2.5, 1.5, 0.5])\n",
    "axes[1].imshow([[1, 1, 1]], cmap='Oranges', alpha=0.5, aspect='auto', extent=[-0.5, 2.5, 2.5, 1.5])\n",
    "axes[1].imshow([[1, 1, 1]], cmap='Reds', alpha=0.5, aspect='auto', extent=[-0.5, 2.5, 3.5, 2.5])\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(3):\n",
    "        axes[1].text(j, i, str(data[i][j]), ha='center', va='center', fontsize=14)\n",
    "\n",
    "axes[1].set_xticks([0, 1, 2])\n",
    "axes[1].set_yticks([0, 1, 2, 3])\n",
    "axes[1].set_xticklabels(['f1', 'f2', 'f3'])\n",
    "axes[1].set_yticklabels(['batch1', 'batch2', 'batch3', 'batch4'])\n",
    "axes[1].set_title('Layer Normalization\\nê° sampleì„ feature ë°©í–¥ìœ¼ë¡œ ì •ê·œí™”')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Post-Quiz\n",
    "\n",
    "### Q1. ê°€ë³€ ê¸¸ì´ ì‹œí€€ìŠ¤ì—ì„œ Batch Normì´ ë¬¸ì œì¸ ì´ìœ ëŠ”?\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ: \n",
    "```\n",
    "\n",
    "### Q2. Pre-LN (Attention ì „ì— LN) vs Post-LNì˜ ì°¨ì´ëŠ”?\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ: \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ì •ë‹µ\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ‘‰ í´ë¦­í•˜ì—¬ ì •ë‹µ ë³´ê¸°</summary>\n",
    "\n",
    "### Pre-Quiz\n",
    "\n",
    "**Q1. ì •ê·œí™” ì¶•**\n",
    "> - Batch Norm: **batch ì¶•** (ê°™ì€ feature across samples)\n",
    "> - Layer Norm: **feature ì¶•** (ê°™ì€ sampleì˜ ëª¨ë“  features)\n",
    "\n",
    "**Q2. NLPì—ì„œ Layer Norm ì‚¬ìš© ì´ìœ **\n",
    "> - **ê°€ë³€ ê¸¸ì´**: ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ë‹¤ë¥´ë©´ batch í†µê³„ê°€ ë¶ˆì•ˆì •\n",
    "> - **batch size ë…ë¦½**: Layer Normì€ batch size 1ì—ì„œë„ ë™ì‘\n",
    "> - **inference ì¼ê´€ì„±**: í•™ìŠµ/ì¶”ë¡  ì‹œ ë™ì¼í•œ ë™ì‘\n",
    "\n",
    "---\n",
    "\n",
    "### Post-Quiz\n",
    "\n",
    "**Q1. ê°€ë³€ ê¸¸ì´ì—ì„œ Batch Norm ë¬¸ì œ**\n",
    "> - paddingëœ ìœ„ì¹˜ë„ í†µê³„ì— í¬í•¨ â†’ ì™œê³¡\n",
    "> - ì§§ì€ ë¬¸ì¥ê³¼ ê¸´ ë¬¸ì¥ì˜ í†µê³„ê°€ ì„ì„\n",
    "> - running statisticsê°€ ë¶ˆì•ˆì •\n",
    "\n",
    "**Q2. Pre-LN vs Post-LN**\n",
    "> - **Post-LN (ì›ë˜)**: LN(x + Attention(x)) - gradient íë¦„ ì–´ë ¤ì›€\n",
    "> - **Pre-LN**: x + Attention(LN(x)) - í•™ìŠµ ì•ˆì •ì , warm-up ë¶ˆí•„ìš”\n",
    "> - ìµœê·¼ ëª¨ë¸ë“¤ì€ Pre-LN ì„ í˜¸\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## êµ¬í˜„ ì •ë‹µ\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ‘‰ í´ë¦­í•˜ì—¬ ì •ë‹µ ì½”ë“œ ë³´ê¸°</summary>\n",
    "\n",
    "```python\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-6):\n",
    "        super().__init__()\n",
    "        \n",
    "        if isinstance(normalized_shape, int):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        \n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.eps = eps\n",
    "        \n",
    "        self.gamma = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(normalized_shape))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        \n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        \n",
    "        return self.gamma * x_norm + self.beta\n",
    "```\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}