{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 08. Mini GPT êµ¬í˜„ - ì „ì²´ ì¡°ë¦½\n",
        "\n",
        "## í•™ìŠµ ëª©í‘œ\n",
        "- ì§€ê¸ˆê¹Œì§€ êµ¬í˜„í•œ ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì¡°ë¦½\n",
        "- Weight Tying ê¸°ë²• ì´í•´\n",
        "- Autoregressive Text Generation êµ¬í˜„\n",
        "- Decoding ì „ëµ (Greedy, Top-k, Top-p) ì´í•´\n",
        "\n",
        "## ê´€ë ¨ ë©´ì ‘ ì§ˆë¬¸\n",
        "- GPT-2ì˜ íŒŒë¼ë¯¸í„° ìˆ˜ëŠ” ì–´ë–»ê²Œ ê²°ì •ë˜ë‚˜ìš”?\n",
        "- Weight Tyingì´ë€ ë¬´ì—‡ì´ê³  ì™œ ì‚¬ìš©í•˜ë‚˜ìš”?\n",
        "- Temperature, Top-k, Top-pì˜ ì—­í• ì€?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Pre-Quiz\n",
        "\n",
        "### Q1. GPT ëª¨ë¸ì˜ í•µì‹¬ êµ¬ì„±ìš”ì†Œë¥¼ ìˆœì„œëŒ€ë¡œ ë‚˜ì—´í•˜ì„¸ìš”\n",
        "```\n",
        "ì…ë ¥ â†’ ___ â†’ ___ â†’ ___ â†’ ___ â†’ ì¶œë ¥\n",
        "```\n",
        "\n",
        "### Q2. Weight Tyingì´ë€?\n",
        "```\n",
        "ë‹¹ì‹ ì˜ ë‹µ: \n",
        "```\n",
        "\n",
        "### Q3. Temperatureê°€ ë†’ìœ¼ë©´ ìƒì„± ê²°ê³¼ëŠ” ì–´ë–»ê²Œ ë³€í•˜ë‚˜ìš”?\n",
        "```\n",
        "ë‹¹ì‹ ì˜ ë‹µ: \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from typing import Optional\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# matplotlib í•œê¸€ ì„¤ì • (macOS)\n",
        "plt.rcParams['font.family'] = 'AppleGothic'\n",
        "plt.rcParams['axes.unicode_minus'] = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì´ì „ì— êµ¬í˜„í•œ ì»´í¬ë„ŒíŠ¸ë“¤ (ì œê³µ)\n",
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.bmm(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask, float('-inf'))\n",
        "    weights = F.softmax(scores, dim=-1)\n",
        "    return torch.bmm(weights, value), weights\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        \n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "        seq_q, seq_k = query.size(1), key.size(1)\n",
        "        \n",
        "        Q = self.W_q(query).view(batch_size, seq_q, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.W_k(key).view(batch_size, seq_k, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.W_v(value).view(batch_size, seq_k, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        \n",
        "        Q = Q.contiguous().view(batch_size * self.num_heads, seq_q, self.d_k)\n",
        "        K = K.contiguous().view(batch_size * self.num_heads, seq_k, self.d_k)\n",
        "        V = V.contiguous().view(batch_size * self.num_heads, seq_k, self.d_k)\n",
        "        \n",
        "        if mask is not None and mask.dim() == 2:\n",
        "            mask = mask.unsqueeze(0).expand(batch_size * self.num_heads, -1, -1)\n",
        "        \n",
        "        output, weights = scaled_dot_product_attention(Q, K, V, mask)\n",
        "        \n",
        "        output = output.view(batch_size, self.num_heads, seq_q, self.d_k)\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_q, self.d_model)\n",
        "        \n",
        "        return self.dropout(self.W_o(output)), weights.view(batch_size, self.num_heads, seq_q, seq_k)\n",
        "\n",
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff=None, dropout=0.1):\n",
        "        super().__init__()\n",
        "        if d_ff is None:\n",
        "            d_ff = d_model * 4\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.dropout(self.fc2(self.dropout(F.gelu(self.fc1(x)))))\n",
        "\n",
        "def create_causal_mask(seq_len, device=None):\n",
        "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
        "    if device is not None:\n",
        "        mask = mask.to(device)\n",
        "    return mask\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff=None, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.ffn = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        seq_len = x.size(1)\n",
        "        causal_mask = create_causal_mask(seq_len, x.device)\n",
        "        if mask is not None:\n",
        "            combined_mask = causal_mask | mask\n",
        "        else:\n",
        "            combined_mask = causal_mask\n",
        "        \n",
        "        normed = self.norm1(x)\n",
        "        attn_out, _ = self.attention(normed, normed, normed, combined_mask)\n",
        "        x = x + self.dropout(attn_out)\n",
        "        \n",
        "        normed = self.norm2(x)\n",
        "        ffn_out = self.ffn(normed)\n",
        "        x = x + ffn_out\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. GPT ì„¤ì • í´ë˜ìŠ¤"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MiniGPTConfig:\n",
        "    \"\"\"GPT ì„¤ì •\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int = 50257,  # GPT-2 vocab size\n",
        "        d_model: int = 768,\n",
        "        num_heads: int = 12,\n",
        "        num_layers: int = 12,\n",
        "        d_ff: int = None,  # default: d_model * 4\n",
        "        max_len: int = 1024,\n",
        "        dropout: float = 0.1,\n",
        "    ):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.num_layers = num_layers\n",
        "        self.d_ff = d_ff if d_ff is not None else d_model * 4\n",
        "        self.max_len = max_len\n",
        "        self.dropout = dropout\n",
        "\n",
        "# GPT-2 í¬ê¸°ë³„ ì„¤ì •\n",
        "GPT2_CONFIGS = {\n",
        "    'small': MiniGPTConfig(d_model=768, num_heads=12, num_layers=12),    # 117M\n",
        "    'medium': MiniGPTConfig(d_model=1024, num_heads=16, num_layers=24),  # 345M\n",
        "    'large': MiniGPTConfig(d_model=1280, num_heads=20, num_layers=36),   # 762M\n",
        "    'xl': MiniGPTConfig(d_model=1600, num_heads=25, num_layers=48),      # 1.5B\n",
        "}\n",
        "\n",
        "print(\"GPT-2 í¬ê¸°ë³„ ì„¤ì •:\")\n",
        "for name, config in GPT2_CONFIGS.items():\n",
        "    print(f\"  {name}: d_model={config.d_model}, heads={config.num_heads}, layers={config.num_layers}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Mini GPT êµ¬í˜„\n",
        "\n",
        "### ì „ì²´ êµ¬ì¡°\n",
        "```\n",
        "token_ids â†’ Token Embedding â†’ + â†’ Dropout â†’ DecoderBlock Ã— N â†’ LayerNorm â†’ LM Head â†’ logits\n",
        "                              â†‘\n",
        "            Position Embedding\n",
        "```\n",
        "\n",
        "### Weight Tying\n",
        "Token Embeddingê³¼ LM Headì˜ weightë¥¼ ê³µìœ í•˜ëŠ” ê¸°ë²•:\n",
        "- íŒŒë¼ë¯¸í„° ìˆ˜ ê°ì†Œ (vocab_size Ã— d_model ì ˆì•½)\n",
        "- ì„±ëŠ¥ í–¥ìƒ íš¨ê³¼ë„ ìˆìŒ\n",
        "- GPT-2, GPT-3 ë“±ì—ì„œ ì‚¬ìš©"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =================================================\n",
        "# TODO: Mini GPT êµ¬í˜„\n",
        "# =================================================\n",
        "#\n",
        "# êµ¬í˜„ ìˆœì„œ:\n",
        "# 1. __init__: ëª¨ë“  ë ˆì´ì–´ ì´ˆê¸°í™”\n",
        "# 2. forward: ìˆœì „íŒŒ (loss ê³„ì‚° í¬í•¨)\n",
        "# 3. generate: autoregressive í…ìŠ¤íŠ¸ ìƒì„±\n",
        "#\n",
        "# =================================================\n",
        "\n",
        "class MiniGPT(nn.Module):\n",
        "    def __init__(self, config: MiniGPTConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        \n",
        "        # ============================================\n",
        "        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
        "        # - token_embedding: nn.Embedding(vocab_size, d_model)\n",
        "        # - position_embedding: nn.Embedding(max_len, d_model)\n",
        "        # - dropout: nn.Dropout\n",
        "        # - blocks: nn.ModuleList of DecoderBlock\n",
        "        # - ln_f: nn.LayerNorm (final)\n",
        "        # - lm_head: nn.Linear(d_model, vocab_size, bias=False)\n",
        "        # \n",
        "        # (ì„ íƒ) Weight Tying:\n",
        "        # self.lm_head.weight = self.token_embedding.weight\n",
        "        # ============================================\n",
        "        \n",
        "        pass  # ì´ ì¤„ì„ ì§€ìš°ê³  êµ¬í˜„\n",
        "    \n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        targets: Optional[torch.Tensor] = None\n",
        "    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_ids: (batch, seq) - ì…ë ¥ í† í° ID\n",
        "            targets: (batch, seq) - íƒ€ê²Ÿ í† í° ID (í•™ìŠµ ì‹œ)\n",
        "            \n",
        "        Returns:\n",
        "            logits: (batch, seq, vocab_size)\n",
        "            loss: cross entropy loss (targetsê°€ ìˆëŠ” ê²½ìš°)\n",
        "        \"\"\"\n",
        "        # ============================================\n",
        "        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
        "        # 1. token embedding\n",
        "        # 2. position ids ìƒì„± ë° embedding\n",
        "        # 3. dropout(tok_emb + pos_emb)\n",
        "        # 4. decoder blocks ìˆœì°¨ í†µê³¼\n",
        "        # 5. final layer norm\n",
        "        # 6. lm_headë¡œ logits ìƒì„±\n",
        "        # 7. loss ê³„ì‚° (targets ìˆëŠ” ê²½ìš°)\n",
        "        #    - shift: logits[..., :-1, :], targets[..., 1:]\n",
        "        #    - cross_entropy\n",
        "        # ============================================\n",
        "        \n",
        "        pass  # ì´ ì¤„ì„ ì§€ìš°ê³  êµ¬í˜„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============ í…ŒìŠ¤íŠ¸ ============\n",
        "def test_mini_gpt_forward():\n",
        "    config = MiniGPTConfig(\n",
        "        vocab_size=1000,\n",
        "        d_model=64,\n",
        "        num_heads=4,\n",
        "        num_layers=2,\n",
        "        max_len=128\n",
        "    )\n",
        "    \n",
        "    model = MiniGPT(config)\n",
        "    \n",
        "    batch, seq = 2, 10\n",
        "    input_ids = torch.randint(0, config.vocab_size, (batch, seq))\n",
        "    targets = torch.randint(0, config.vocab_size, (batch, seq))\n",
        "    \n",
        "    # Forward without targets\n",
        "    logits, loss = model(input_ids)\n",
        "    assert logits.shape == (batch, seq, config.vocab_size)\n",
        "    assert loss is None\n",
        "    \n",
        "    # Forward with targets\n",
        "    logits, loss = model(input_ids, targets)\n",
        "    assert logits.shape == (batch, seq, config.vocab_size)\n",
        "    assert loss is not None\n",
        "    \n",
        "    print(\"âœ… MiniGPT Forward í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
        "    print(f\"   Logits shape: {logits.shape}\")\n",
        "    print(f\"   Loss: {loss.item():.4f}\")\n",
        "    \n",
        "    # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°\n",
        "    num_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"   Parameters: {num_params:,}\")\n",
        "\n",
        "test_mini_gpt_forward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Text Generation êµ¬í˜„\n",
        "\n",
        "### Decoding ì „ëµë“¤\n",
        "\n",
        "| ì „ëµ | ì„¤ëª… | íŠ¹ì§• |\n",
        "|------|------|------|\n",
        "| **Greedy** | argmax ì„ íƒ | ê²°ì •ì , ë°˜ë³µ ë¬¸ì œ |\n",
        "| **Temperature** | logits/T í›„ sampling | Tâ†‘ ë‹¤ì–‘ì„±â†‘ |\n",
        "| **Top-k** | ìƒìœ„ kê°œë§Œ sampling | í™•ë¥  ë‚®ì€ í† í° ì œê±° |\n",
        "| **Top-p (Nucleus)** | ëˆ„ì í™•ë¥  pê¹Œì§€ sampling | ë™ì  í›„ë³´ ìˆ˜ |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =================================================\n",
        "# TODO: generate ë©”ì„œë“œë¥¼ MiniGPTì— ì¶”ê°€\n",
        "# =================================================\n",
        "#\n",
        "# ì•„ë˜ ì½”ë“œë¥¼ MiniGPT í´ë˜ìŠ¤ì— ë©”ì„œë“œë¡œ ì¶”ê°€í•˜ì„¸ìš”\n",
        "# (ë˜ëŠ” í•¨ìˆ˜ë¡œ êµ¬í˜„í•´ë„ ë©ë‹ˆë‹¤)\n",
        "#\n",
        "# =================================================\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(\n",
        "    model: MiniGPT,\n",
        "    input_ids: torch.Tensor,\n",
        "    max_new_tokens: int = 50,\n",
        "    temperature: float = 1.0,\n",
        "    top_k: int = None,\n",
        "    top_p: float = None,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Autoregressive í…ìŠ¤íŠ¸ ìƒì„±\n",
        "    \n",
        "    Args:\n",
        "        model: MiniGPT ëª¨ë¸\n",
        "        input_ids: (batch, seq) - ì‹œì‘ í† í°ë“¤\n",
        "        max_new_tokens: ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜\n",
        "        temperature: sampling temperature (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ greedy)\n",
        "        top_k: top-k sampling (Noneì´ë©´ ì‚¬ìš© ì•ˆ í•¨)\n",
        "        top_p: nucleus sampling (Noneì´ë©´ ì‚¬ìš© ì•ˆ í•¨)\n",
        "        \n",
        "    Returns:\n",
        "        generated: (batch, seq + max_new_tokens)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # ============================================\n",
        "    # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
        "    # \n",
        "    # for _ in range(max_new_tokens):\n",
        "    #     1. í˜„ì¬ input_idsë¡œ forward (ë§ˆì§€ë§‰ ìœ„ì¹˜ logitsë§Œ í•„ìš”)\n",
        "    #     2. temperature ì ìš©: logits = logits / temperature\n",
        "    #     3. top_k filtering (ìˆëŠ” ê²½ìš°)\n",
        "    #        - ìƒìœ„ kê°œ ì™¸ -infë¡œ ì„¤ì •\n",
        "    #     4. top_p filtering (ìˆëŠ” ê²½ìš°)\n",
        "    #        - ëˆ„ì  í™•ë¥  p ì´í›„ -infë¡œ ì„¤ì •\n",
        "    #     5. softmax â†’ í™•ë¥  ë¶„í¬\n",
        "    #     6. multinomial sampling (ë˜ëŠ” greedy)\n",
        "    #     7. ìƒˆ í† í°ì„ input_idsì— concat\n",
        "    # ============================================\n",
        "    \n",
        "    pass  # ì´ ì¤„ì„ ì§€ìš°ê³  êµ¬í˜„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Top-k, Top-p í•„í„°ë§ í—¬í¼ í•¨ìˆ˜ (íŒíŠ¸)\n",
        "def top_k_filtering(logits, top_k):\n",
        "    \"\"\"Top-k í•„í„°ë§: ìƒìœ„ kê°œ ì™¸ -infë¡œ ì„¤ì •\"\"\"\n",
        "    if top_k is None or top_k <= 0:\n",
        "        return logits\n",
        "    \n",
        "    values, _ = torch.topk(logits, top_k)\n",
        "    min_value = values[:, -1].unsqueeze(-1)\n",
        "    return torch.where(logits < min_value, torch.tensor(float('-inf')), logits)\n",
        "\n",
        "\n",
        "def top_p_filtering(logits, top_p):\n",
        "    \"\"\"Top-p (Nucleus) í•„í„°ë§: ëˆ„ì  í™•ë¥  p ì´í›„ -infë¡œ ì„¤ì •\"\"\"\n",
        "    if top_p is None or top_p >= 1.0:\n",
        "        return logits\n",
        "    \n",
        "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "    \n",
        "    # ëˆ„ì  í™•ë¥ ì´ pë¥¼ ë„˜ëŠ” í† í°ë“¤ ë§ˆìŠ¤í‚¹\n",
        "    sorted_indices_to_remove = cumulative_probs > top_p\n",
        "    # ì²« ë²ˆì§¸ í† í°ì€ í•­ìƒ ìœ ì§€\n",
        "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "    sorted_indices_to_remove[..., 0] = False\n",
        "    \n",
        "    indices_to_remove = sorted_indices_to_remove.scatter(\n",
        "        dim=-1, index=sorted_indices, src=sorted_indices_to_remove\n",
        "    )\n",
        "    logits[indices_to_remove] = float('-inf')\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============ í…ŒìŠ¤íŠ¸ ============\n",
        "def test_generate():\n",
        "    config = MiniGPTConfig(\n",
        "        vocab_size=100,\n",
        "        d_model=64,\n",
        "        num_heads=4,\n",
        "        num_layers=2,\n",
        "        max_len=128\n",
        "    )\n",
        "    \n",
        "    model = MiniGPT(config)\n",
        "    \n",
        "    # ì‹œì‘ í† í°\n",
        "    prompt = torch.randint(0, config.vocab_size, (1, 5))\n",
        "    \n",
        "    # ë‹¤ì–‘í•œ decoding ì „ëµ í…ŒìŠ¤íŠ¸\n",
        "    print(\"Generation í…ŒìŠ¤íŠ¸:\")\n",
        "    print(f\"  Prompt shape: {prompt.shape}\")\n",
        "    \n",
        "    # Greedy\n",
        "    greedy_output = generate(model, prompt, max_new_tokens=10, temperature=0.001)\n",
        "    print(f\"  Greedy output shape: {greedy_output.shape}\")\n",
        "    \n",
        "    # Temperature sampling\n",
        "    temp_output = generate(model, prompt, max_new_tokens=10, temperature=0.8)\n",
        "    print(f\"  Temperature(0.8) output shape: {temp_output.shape}\")\n",
        "    \n",
        "    # Top-k\n",
        "    topk_output = generate(model, prompt, max_new_tokens=10, temperature=1.0, top_k=10)\n",
        "    print(f\"  Top-k(10) output shape: {topk_output.shape}\")\n",
        "    \n",
        "    # Top-p\n",
        "    topp_output = generate(model, prompt, max_new_tokens=10, temperature=1.0, top_p=0.9)\n",
        "    print(f\"  Top-p(0.9) output shape: {topp_output.shape}\")\n",
        "    \n",
        "    print(\"\\nâœ… Generate í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
        "\n",
        "test_generate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. íŒŒë¼ë¯¸í„° ìˆ˜ ë¶„ì„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_parameters(model, verbose=False):\n",
        "    \"\"\"ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ ë¶„ì„\"\"\"\n",
        "    total = 0\n",
        "    breakdown = {}\n",
        "    \n",
        "    for name, param in model.named_parameters():\n",
        "        count = param.numel()\n",
        "        total += count\n",
        "        \n",
        "        # ì»´í¬ë„ŒíŠ¸ë³„ ê·¸ë£¹í•‘\n",
        "        component = name.split('.')[0]\n",
        "        breakdown[component] = breakdown.get(component, 0) + count\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"\\níŒŒë¼ë¯¸í„° ë¶„ì„:\")\n",
        "        for comp, count in sorted(breakdown.items(), key=lambda x: -x[1]):\n",
        "            pct = count / total * 100\n",
        "            print(f\"  {comp}: {count:,} ({pct:.1f}%)\")\n",
        "        print(f\"  ---------------\")\n",
        "        print(f\"  Total: {total:,}\")\n",
        "    \n",
        "    return total, breakdown\n",
        "\n",
        "# GPT-2 í¬ê¸°ë³„ íŒŒë¼ë¯¸í„° ìˆ˜ ë¹„êµ\n",
        "print(\"GPT-2 í¬ê¸°ë³„ íŒŒë¼ë¯¸í„° ìˆ˜:\")\n",
        "for name, config in GPT2_CONFIGS.items():\n",
        "    model = MiniGPT(config)\n",
        "    total, _ = count_parameters(model)\n",
        "    print(f\"  {name}: {total/1e6:.1f}M parameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ìƒì„¸ ë¶„ì„ (small ëª¨ë¸)\n",
        "config = MiniGPTConfig(\n",
        "    vocab_size=50257,\n",
        "    d_model=768,\n",
        "    num_heads=12,\n",
        "    num_layers=12\n",
        ")\n",
        "model = MiniGPT(config)\n",
        "count_parameters(model, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Decoding ì „ëµ ë¹„êµ ì‹œê°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_decoding_strategies():\n",
        "    \"\"\"Decoding ì „ëµë³„ í™•ë¥  ë¶„í¬ ì‹œê°í™”\"\"\"\n",
        "    torch.manual_seed(42)\n",
        "    \n",
        "    # ê°€ìƒì˜ logits (vocab_size=20)\n",
        "    logits = torch.randn(1, 20) * 2\n",
        "    vocab = [f'tok_{i}' for i in range(20)]\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "    \n",
        "    # 1. Original (temperature=1)\n",
        "    probs_orig = F.softmax(logits, dim=-1).squeeze()\n",
        "    axes[0, 0].bar(range(20), probs_orig.numpy())\n",
        "    axes[0, 0].set_title('Original (T=1.0)')\n",
        "    axes[0, 0].set_xlabel('Token')\n",
        "    axes[0, 0].set_ylabel('Probability')\n",
        "    \n",
        "    # 2. Low Temperature (greedy-like)\n",
        "    probs_low = F.softmax(logits / 0.3, dim=-1).squeeze()\n",
        "    axes[0, 1].bar(range(20), probs_low.numpy())\n",
        "    axes[0, 1].set_title('Low Temperature (T=0.3)')\n",
        "    axes[0, 1].set_xlabel('Token')\n",
        "    axes[0, 1].set_ylabel('Probability')\n",
        "    \n",
        "    # 3. High Temperature (diverse)\n",
        "    probs_high = F.softmax(logits / 2.0, dim=-1).squeeze()\n",
        "    axes[1, 0].bar(range(20), probs_high.numpy())\n",
        "    axes[1, 0].set_title('High Temperature (T=2.0)')\n",
        "    axes[1, 0].set_xlabel('Token')\n",
        "    axes[1, 0].set_ylabel('Probability')\n",
        "    \n",
        "    # 4. Top-k (k=5)\n",
        "    logits_topk = top_k_filtering(logits.clone(), top_k=5)\n",
        "    probs_topk = F.softmax(logits_topk, dim=-1).squeeze()\n",
        "    axes[1, 1].bar(range(20), probs_topk.numpy())\n",
        "    axes[1, 1].set_title('Top-k (k=5)')\n",
        "    axes[1, 1].set_xlabel('Token')\n",
        "    axes[1, 1].set_ylabel('Probability')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_decoding_strategies()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Post-Quiz\n",
        "\n",
        "### Q1. Weight Tyingì„ ì‚¬ìš©í•˜ë©´ íŒŒë¼ë¯¸í„°ê°€ ì–¼ë§ˆë‚˜ ì¤„ì–´ë“œë‚˜ìš”? (vocab_size=50257, d_model=768)\n",
        "```\n",
        "ë‹¹ì‹ ì˜ ë‹µ: \n",
        "```\n",
        "\n",
        "### Q2. Temperature=0.5 vs Temperature=1.5ì˜ ìƒì„± ê²°ê³¼ ì°¨ì´ëŠ”?\n",
        "```\n",
        "ë‹¹ì‹ ì˜ ë‹µ: \n",
        "```\n",
        "\n",
        "### Q3. Top-k=50, Top-p=0.9ë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ë©´ ì–´ë–»ê²Œ ë™ì‘í•˜ë‚˜ìš”?\n",
        "```\n",
        "ë‹¹ì‹ ì˜ ë‹µ: \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## ì •ë‹µ\n",
        "\n",
        "<details>\n",
        "<summary>ğŸ‘‰ í´ë¦­í•˜ì—¬ ì •ë‹µ ì½”ë“œ ë³´ê¸°</summary>\n",
        "\n",
        "```python\n",
        "class MiniGPT(nn.Module):\n",
        "    def __init__(self, config: MiniGPTConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        \n",
        "        self.token_embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.position_embedding = nn.Embedding(config.max_len, config.d_model)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        \n",
        "        self.blocks = nn.ModuleList([\n",
        "            DecoderBlock(config.d_model, config.num_heads, config.d_ff, config.dropout)\n",
        "            for _ in range(config.num_layers)\n",
        "        ])\n",
        "        \n",
        "        self.ln_f = nn.LayerNorm(config.d_model)\n",
        "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "        \n",
        "        # Weight Tying (ì„ íƒ)\n",
        "        self.lm_head.weight = self.token_embedding.weight\n",
        "    \n",
        "    def forward(self, input_ids, targets=None):\n",
        "        batch, seq_len = input_ids.shape\n",
        "        device = input_ids.device\n",
        "        \n",
        "        # Embeddings\n",
        "        tok_emb = self.token_embedding(input_ids)\n",
        "        pos_ids = torch.arange(seq_len, device=device).unsqueeze(0)\n",
        "        pos_emb = self.position_embedding(pos_ids)\n",
        "        \n",
        "        x = self.dropout(tok_emb + pos_emb)\n",
        "        \n",
        "        # Decoder blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        \n",
        "        # Output\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        \n",
        "        # Loss\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            # Shift: logits[:-1] ì˜ˆì¸¡ â†’ targets[1:]\n",
        "            shift_logits = logits[..., :-1, :].contiguous()\n",
        "            shift_targets = targets[..., 1:].contiguous()\n",
        "            loss = F.cross_entropy(\n",
        "                shift_logits.view(-1, self.config.vocab_size),\n",
        "                shift_targets.view(-1)\n",
        "            )\n",
        "        \n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(model, input_ids, max_new_tokens=50, temperature=1.0, top_k=None, top_p=None):\n",
        "    model.eval()\n",
        "    \n",
        "    for _ in range(max_new_tokens):\n",
        "        # max_len ì œí•œ\n",
        "        idx_cond = input_ids[:, -model.config.max_len:]\n",
        "        \n",
        "        # Forward\n",
        "        logits, _ = model(idx_cond)\n",
        "        logits = logits[:, -1, :]  # ë§ˆì§€ë§‰ ìœ„ì¹˜ë§Œ\n",
        "        \n",
        "        # Temperature\n",
        "        logits = logits / max(temperature, 1e-8)\n",
        "        \n",
        "        # Top-k filtering\n",
        "        if top_k is not None:\n",
        "            logits = top_k_filtering(logits, top_k)\n",
        "        \n",
        "        # Top-p filtering\n",
        "        if top_p is not None:\n",
        "            logits = top_p_filtering(logits, top_p)\n",
        "        \n",
        "        # Sampling\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "        \n",
        "        # Concat\n",
        "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
        "    \n",
        "    return input_ids\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>ğŸ‘‰ Pre-Quiz ì •ë‹µ</summary>\n",
        "\n",
        "### Q1. GPT êµ¬ì„±ìš”ì†Œ\n",
        "ì…ë ¥ â†’ **Token Embedding + Position Embedding** â†’ **Dropout** â†’ **DecoderBlock Ã— N** â†’ **LayerNorm** â†’ **LM Head** â†’ ì¶œë ¥\n",
        "\n",
        "### Q2. Weight Tyingì´ë€?\n",
        "Token Embeddingê³¼ LM Head(ì¶œë ¥ projection)ì˜ ê°€ì¤‘ì¹˜ë¥¼ **ê³µìœ **í•˜ëŠ” ê¸°ë²•. íŒŒë¼ë¯¸í„° ìˆ˜ ì ˆì•½ + ì…ë ¥/ì¶œë ¥ ì„ë² ë”© ê³µê°„ ì¼ê´€ì„± ìœ ì§€.\n",
        "\n",
        "### Q3. Temperature ë†’ìœ¼ë©´?\n",
        "logitsë¥¼ Të¡œ ë‚˜ëˆ„ë©´ softmax ë¶„í¬ê°€ **ë” ê· ë“±**í•´ì§ â†’ ë‹¤ì–‘í•œ í† í°ì´ ì„ íƒë  í™•ë¥ â†‘ â†’ ì°½ì˜ì ì´ì§€ë§Œ ì¼ê´€ì„±â†“\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>ğŸ‘‰ Post-Quiz ì •ë‹µ</summary>\n",
        "\n",
        "### Q1. Weight Tying íŒŒë¼ë¯¸í„° ì ˆì•½\n",
        "vocab_size Ã— d_model = 50257 Ã— 768 = **38,597,376** â‰ˆ **38.6M** íŒŒë¼ë¯¸í„° ì ˆì•½\n",
        "\n",
        "### Q2. Temperature ì°¨ì´\n",
        "- **T=0.5**: í™•ë¥  ë¶„í¬ê°€ ë” ë¾°ì¡±í•´ì§ â†’ ë†’ì€ í™•ë¥  í† í° ìœ„ì£¼ â†’ **ê²°ì •ì , ë°˜ë³µ ê²½í–¥**\n",
        "- **T=1.5**: í™•ë¥  ë¶„í¬ê°€ ë” í‰í‰í•´ì§ â†’ ë‹¤ì–‘í•œ í† í° ì„ íƒ â†’ **ë‹¤ì–‘í•˜ì§€ë§Œ ì¼ê´€ì„±â†“**\n",
        "\n",
        "### Q3. Top-k + Top-p í•¨ê»˜ ì‚¬ìš©\n",
        "ë‘ í•„í„°ë§ì´ **ìˆœì°¨ì **ìœ¼ë¡œ ì ìš©ë¨. ë¨¼ì € Top-kë¡œ í›„ë³´ë¥¼ kê°œë¡œ ì œí•œ â†’ ê·¸ ì¤‘ Top-pë¡œ ëˆ„ì í™•ë¥  pê¹Œì§€ë§Œ ìœ ì§€. ë” **ì—„ê²©í•œ í•„í„°ë§** íš¨ê³¼.\n",
        "\n",
        "</details>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
