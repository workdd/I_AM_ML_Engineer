{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Position-wise Feed-Forward Network êµ¬í˜„\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- FFNì˜ êµ¬ì¡°ì™€ ì—­í•  ì´í•´\n",
    "- \"Position-wise\"ì˜ ì˜ë¯¸ ì´í•´\n",
    "- ReLU vs GELU ì°¨ì´\n",
    "\n",
    "## ê´€ë ¨ ë©´ì ‘ ì§ˆë¬¸\n",
    "- Transformerì—ì„œ FFNì˜ ì—­í• ì€?\n",
    "- Position-wiseê°€ ë¬´ìŠ¨ ëœ»ì¸ê°€ìš”?\n",
    "- d_ffëŠ” ë³´í†µ d_modelì˜ ëª‡ ë°°ì¸ê°€ìš”?\n",
    "\n",
    "## í•µì‹¬ ìˆ˜ì‹\n",
    "$$\\text{FFN}(x) = \\text{Activation}(xW_1 + b_1)W_2 + b_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pre-Quiz\n",
    "\n",
    "### Q1. FFNì—ì„œ d_ffê°€ d_modelë³´ë‹¤ í° ì´ìœ ëŠ”?\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ: \n",
    "```\n",
    "\n",
    "### Q2. \"Position-wise\"ê°€ ì˜ë¯¸í•˜ëŠ” ê²ƒì€?\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ: \n",
    "```\n",
    "\n",
    "### Q3. BERT/GPTì—ì„œ ReLU ëŒ€ì‹  GELUë¥¼ ì“°ëŠ” ì´ìœ ëŠ”?\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ: \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['font.family'] = 'AppleGothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. FFN êµ¬í˜„\n",
    "\n",
    "### êµ¬ì¡°\n",
    "```\n",
    "Input: (batch, seq, d_model)\n",
    "    â†“ Linear(d_model â†’ d_ff)\n",
    "(batch, seq, d_ff)\n",
    "    â†“ Activation (GELU/ReLU)\n",
    "    â†“ Dropout\n",
    "    â†“ Linear(d_ff â†’ d_model)\n",
    "    â†“ Dropout\n",
    "Output: (batch, seq, d_model)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# TODO: Position-wise FFN êµ¬í˜„\n",
    "# =================================================\n",
    "#\n",
    "# Args:\n",
    "#   d_model: ì…ë ¥/ì¶œë ¥ ì°¨ì›\n",
    "#   d_ff: hidden ì°¨ì› (default: d_model * 4)\n",
    "#   dropout: dropout í™•ë¥ \n",
    "#   activation: 'relu' ë˜ëŠ” 'gelu'\n",
    "#\n",
    "# êµ¬ì¡°:\n",
    "#   fc1: Linear(d_model, d_ff)\n",
    "#   fc2: Linear(d_ff, d_model)\n",
    "#   activation: GELU ë˜ëŠ” ReLU\n",
    "#\n",
    "# =================================================\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        d_ff: int = None,\n",
    "        dropout: float = 0.1,\n",
    "        activation: str = 'gelu'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if d_ff is None:\n",
    "            d_ff = d_model * 4\n",
    "        \n",
    "        # ============================================\n",
    "        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
    "        # fc1, fc2, dropout, activation í•¨ìˆ˜\n",
    "        # ============================================\n",
    "        \n",
    "        pass  # ì´ ì¤„ì„ ì§€ìš°ê³  êµ¬í˜„\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq, d_model)\n",
    "        Returns:\n",
    "            output: (batch, seq, d_model)\n",
    "        \"\"\"\n",
    "        # ============================================\n",
    "        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
    "        # x â†’ fc1 â†’ activation â†’ dropout â†’ fc2 â†’ dropout\n",
    "        # ============================================\n",
    "        \n",
    "        pass  # ì´ ì¤„ì„ ì§€ìš°ê³  êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ í…ŒìŠ¤íŠ¸ ============\n",
    "def test_ffn():\n",
    "    batch, seq, d_model = 2, 10, 64\n",
    "    \n",
    "    ffn = PositionWiseFeedForward(d_model, d_ff=256, activation='gelu')\n",
    "    x = torch.randn(batch, seq, d_model)\n",
    "    \n",
    "    output = ffn(x)\n",
    "    \n",
    "    assert output.shape == x.shape, f\"Shape ì˜¤ë¥˜: {output.shape}\"\n",
    "    print(\"âœ… FFN í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
    "    print(f\"   Input: {x.shape}\")\n",
    "    print(f\"   Output: {output.shape}\")\n",
    "    \n",
    "    # íŒŒë¼ë¯¸í„° ìˆ˜ í™•ì¸\n",
    "    total_params = sum(p.numel() for p in ffn.parameters())\n",
    "    print(f\"   Total parameters: {total_params:,}\")\n",
    "\n",
    "test_ffn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. ReLU vs GELU ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-3, 3, 100)\n",
    "\n",
    "relu = F.relu(x)\n",
    "gelu = F.gelu(x)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(x.numpy(), relu.numpy(), label='ReLU', linewidth=2)\n",
    "plt.plot(x.numpy(), gelu.numpy(), label='GELU', linewidth=2)\n",
    "plt.axhline(y=0, color='black', linewidth=0.5)\n",
    "plt.axvline(x=0, color='black', linewidth=0.5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('ReLU vs GELU\\nGELUëŠ” ìŒìˆ˜ ì˜ì—­ì—ì„œë„ ì‘ì€ ê°’ í†µê³¼ (smoother)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Post-Quiz\n",
    "\n",
    "### Q1. d_model=512, d_ff=2048ì¼ ë•Œ FFNì˜ íŒŒë¼ë¯¸í„° ìˆ˜ëŠ”?\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ: (ê³„ì‚°ì‹ í¬í•¨)\n",
    "```\n",
    "\n",
    "### Q2. FFNì´ ì—†ìœ¼ë©´ Transformerê°€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ: \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ì •ë‹µ\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ‘‰ í´ë¦­í•˜ì—¬ ì •ë‹µ ë³´ê¸°</summary>\n",
    "\n",
    "### Pre-Quiz\n",
    "\n",
    "**Q1. d_ffê°€ í° ì´ìœ **\n",
    "> - í™•ì¥ í›„ ì¶•ì†Œë¡œ **ë¹„ì„ í˜• ë³€í™˜ ëŠ¥ë ¥** ì¦ê°€\n",
    "> - ë” ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ ê°€ëŠ¥\n",
    "> - \"bottleneck\" ì•„í‚¤í…ì²˜ì˜ ì—­ë°©í–¥\n",
    "\n",
    "**Q2. Position-wise ì˜ë¯¸**\n",
    "> - ê° positionì— **ë…ë¦½ì ìœ¼ë¡œ** ì ìš©\n",
    "> - ê°™ì€ weightë¥¼ ê³µìœ í•˜ì§€ë§Œ ìœ„ì¹˜ë³„ë¡œ ë”°ë¡œ ê³„ì‚°\n",
    "> - CNNì˜ 1x1 convolutionê³¼ ìœ ì‚¬\n",
    "\n",
    "**Q3. GELU ì‚¬ìš© ì´ìœ **\n",
    "> - ReLUë³´ë‹¤ smooth â†’ gradient íë¦„ ì¢‹ìŒ\n",
    "> - ìŒìˆ˜ì—ì„œë„ ì‘ì€ ê°’ í†µê³¼ (Dying ReLU ë°©ì§€)\n",
    "> - ì‹¤í—˜ì ìœ¼ë¡œ NLPì—ì„œ ë” ë‚˜ì€ ì„±ëŠ¥\n",
    "\n",
    "---\n",
    "\n",
    "### Post-Quiz\n",
    "\n",
    "**Q1. íŒŒë¼ë¯¸í„° ìˆ˜**\n",
    "> - fc1: 512 Ã— 2048 + 2048 = 1,050,624\n",
    "> - fc2: 2048 Ã— 512 + 512 = 1,049,088\n",
    "> - **Total: ì•½ 210ë§Œ ê°œ**\n",
    "\n",
    "**Q2. FFN ì—†ìœ¼ë©´?**\n",
    "> - Attentionë§Œìœ¼ë¡œëŠ” ì„ í˜• ë³€í™˜ì— ê°€ê¹Œì›€\n",
    "> - **ë¹„ì„ í˜•ì„± ë¶€ì¡±** â†’ í‘œí˜„ë ¥ ê°ì†Œ\n",
    "> - FFNì´ ê° í† í°ì— ë¹„ì„ í˜• ë³€í™˜ ì ìš©\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## êµ¬í˜„ ì •ë‹µ\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ‘‰ í´ë¦­í•˜ì—¬ ì •ë‹µ ì½”ë“œ ë³´ê¸°</summary>\n",
    "\n",
    "```python\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=None, dropout=0.1, activation='gelu'):\n",
    "        super().__init__()\n",
    "        if d_ff is None:\n",
    "            d_ff = d_model * 4\n",
    "        \n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        if activation == 'gelu':\n",
    "            self.activation = F.gelu\n",
    "        else:\n",
    "            self.activation = F.relu\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
