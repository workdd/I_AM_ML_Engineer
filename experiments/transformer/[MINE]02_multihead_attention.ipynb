{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Multi-Head Attention ì§ì ‘ êµ¬í˜„\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "\n",
    "-   Multi-Head Attentionì˜ ë™ì‘ ì›ë¦¬ ì´í•´\n",
    "-   Head ë¶„ë¦¬ì™€ ë³‘í•© ê³¼ì • êµ¬í˜„\n",
    "-   ì™œ ì—¬ëŸ¬ headê°€ í•„ìš”í•œì§€ ì´í•´\n",
    "\n",
    "## ê´€ë ¨ ë©´ì ‘ ì§ˆë¬¸\n",
    "\n",
    "-   Multi-Head Attentionì´ ì™œ í•„ìš”í•œê°€ìš”?\n",
    "-   Head ìˆ˜ë¥¼ ëŠ˜ë¦¬ë©´ ë¬´ì—‡ì´ ì¢‹ì•„ì§€ë‚˜ìš”?\n",
    "-   d_model, num_heads, d_kì˜ ê´€ê³„ëŠ”?\n",
    "\n",
    "## í•µì‹¬ ìˆ˜ì‹\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$ $$\\text{where } \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pre-Quiz\n",
    "\n",
    "### Q1. Single Head ëŒ€ì‹  Multi-Headë¥¼ ì“°ëŠ” ì´ìœ ëŠ”?\n",
    "\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ:\n",
    "```\n",
    "\n",
    "### Q2. d_model=512, num_heads=8ì´ë©´ d_këŠ”?\n",
    "\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ:\n",
    "```\n",
    "\n",
    "### Q3. ê° headê°€ í•™ìŠµí•˜ëŠ” ê²ƒì€?\n",
    "\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ:\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"AppleGothic\"\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01ì—ì„œ êµ¬í˜„í•œ scaled_dot_product_attention (ì œê³µ)\n",
    "def scaled_dot_product_attention(\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    mask: Optional[torch.Tensor] = None,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.bmm(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask, float(\"-inf\"))\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.bmm(weights, value)\n",
    "    return output, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Multi-Head Attention êµ¬í˜„\n",
    "\n",
    "### í•µì‹¬ ì•„ì´ë””ì–´\n",
    "\n",
    "-   í•˜ë‚˜ì˜ attention ëŒ€ì‹  **hê°œì˜ head**ë¡œ ë³‘ë ¬ ì²˜ë¦¬\n",
    "-   ê° headëŠ” **ë‹¤ë¥¸ ê´€ê³„ íŒ¨í„´**ì„ í•™ìŠµ\n",
    "    -   head 1: ì£¼ì–´-ë™ì‚¬ ê´€ê³„\n",
    "    -   head 2: í˜•ìš©ì‚¬-ëª…ì‚¬ ê´€ê³„\n",
    "    -   head 3: ì‹œê°„ ê´€ê³„ ë“±...\n",
    "\n",
    "### Shape ë³€í™˜ ê³¼ì •\n",
    "\n",
    "```\n",
    "Input: (batch, seq, d_model)\n",
    "    â†“ Q, K, V projection\n",
    "(batch, seq, d_model)\n",
    "    â†“ reshape to heads\n",
    "(batch, seq, num_heads, d_k)\n",
    "    â†“ transpose\n",
    "(batch, num_heads, seq, d_k)\n",
    "    â†“ attention (ê° head ë…ë¦½ ê³„ì‚°)\n",
    "(batch, num_heads, seq, d_k)\n",
    "    â†“ transpose + reshape (concat)\n",
    "(batch, seq, d_model)\n",
    "    â†“ output projection\n",
    "Output: (batch, seq, d_model)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# TODO: Multi-Head Attention êµ¬í˜„\n",
    "# =================================================\n",
    "#\n",
    "# $$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
    "# $$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
    "#\n",
    "# í•µì‹¬: d_modelì„ num_headsë¡œ ë‚˜ëˆ ì„œ ë³‘ë ¬ ì²˜ë¦¬\n",
    "#       d_k = d_model // num_heads\n",
    "#\n",
    "# Shape ë³€í™˜:\n",
    "#   (B, T, d_model) â†’ (B, h, T, d_k) â†’ attention â†’ (B, T, d_model)\n",
    "#\n",
    "# =================================================\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # 4ê°œì˜ Linear layers ì •ì˜\n",
    "        self.WQ = nn.Linear(self.d_model, self.d_model)\n",
    "        self.WK = nn.Linear(self.d_model, self.d_model)\n",
    "        self.WV = nn.Linear(self.d_model, self.d_model)\n",
    "\n",
    "        self.WO = nn.Linear(self.d_model, self.d_model)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: (B, T_q, d_model)\n",
    "            key: (B, T_k, d_model)\n",
    "            value: (B, T_k, d_model)\n",
    "\n",
    "        Returns:\n",
    "            output: (B, T_q, d_model)\n",
    "            weights: (B, h, T_q, T_k)\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "        seq_q = query.size(1)  # queryëŠ” ì§ˆë¬¸ì— í•´ë‹¹í•˜ë¯€ë¡œ sizeê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ\n",
    "        seq_k = seq_v = key.size(1)  # keyì™€ valueëŠ” í•­ìƒ ê°™ì€ sizeì„\n",
    "\n",
    "        # ìœ„ ìˆ˜ì‹ê³¼ shape ë³€í™˜ì„ êµ¬í˜„í•˜ì„¸ìš”\n",
    "        Q = self.WQ(query)\n",
    "        K = self.WQ(key)\n",
    "        V = self.WQ(value)\n",
    "\n",
    "        Q = Q.view(batch_size, seq_q, self.num_heads, self.d_k)\n",
    "        K = K.view(batch_size, seq_k, self.num_heads, self.d_k)\n",
    "        V = V.view(batch_size, seq_v, self.num_heads, self.d_k)\n",
    "\n",
    "        Q = Q.transpose(batch_size, self.num_heads, seq_q, self.d_k)\n",
    "        K = K.transpose(batch_size, self.num_heads, seq_k, self.d_k)\n",
    "        V = V.transpose(batch_size, self.num_heads, seq_v, self.d_k)\n",
    "\n",
    "        Q = Q.contiguous().view(batch_size, seq_q, self.num_heads, d_k)\n",
    "        K = K.contiguous().view(batch_size, seq_k, self.num_heads, d_k)\n",
    "        V = V.contiguous().view(batch_size, seq_v, self.num_heads, d_k)\n",
    "\n",
    "        if mask:\n",
    "            pass\n",
    "            \n",
    "\n",
    "        output, weight = scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        output = output.\n",
    "\n",
    "\n",
    "        return output, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-Attention wrapper (í¸ì˜ìš©)\n",
    "class MultiHeadSelfAttention(MultiHeadAttention):\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
    "        return super().forward(x, x, x, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ í…ŒìŠ¤íŠ¸ 1: Shape í™•ì¸ ============\n",
    "def test_shape():\n",
    "    batch, seq, d_model, num_heads = 2, 10, 64, 8\n",
    "\n",
    "    mha = MultiHeadSelfAttention(d_model, num_heads)\n",
    "    x = torch.randn(batch, seq, d_model)\n",
    "\n",
    "    output, weights = mha(x)\n",
    "\n",
    "    assert output.shape == (batch, seq, d_model), f\"Output shape ì˜¤ë¥˜: {output.shape}\"\n",
    "    assert weights.shape == (batch, num_heads, seq, seq), f\"Weights shape ì˜¤ë¥˜: {weights.shape}\"\n",
    "    print(\"âœ… Shape í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
    "    print(f\"   Input: {x.shape}\")\n",
    "    print(f\"   Output: {output.shape}\")\n",
    "    print(f\"   Attention weights: {weights.shape}\")\n",
    "\n",
    "\n",
    "test_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ í…ŒìŠ¤íŠ¸ 2: Cross Attention ============\n",
    "def test_cross_attention():\n",
    "    batch, seq_q, seq_k, d_model, num_heads = 2, 8, 12, 64, 8\n",
    "\n",
    "    mha = MultiHeadAttention(d_model, num_heads)\n",
    "    q = torch.randn(batch, seq_q, d_model)\n",
    "    k = torch.randn(batch, seq_k, d_model)\n",
    "    v = torch.randn(batch, seq_k, d_model)\n",
    "\n",
    "    output, weights = mha(q, k, v)\n",
    "\n",
    "    assert output.shape == (batch, seq_q, d_model)\n",
    "    assert weights.shape == (batch, num_heads, seq_q, seq_k)\n",
    "    print(\"âœ… Cross Attention í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
    "    print(f\"   Q: {q.shape}, K: {k.shape}, V: {v.shape}\")\n",
    "    print(f\"   Output: {output.shape}\")\n",
    "\n",
    "\n",
    "test_cross_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ í…ŒìŠ¤íŠ¸ 3: Attention weights í•© ê²€ì¦ ============\n",
    "def test_attention_sum():\n",
    "    batch, seq, d_model, num_heads = 2, 6, 64, 8\n",
    "\n",
    "    mha = MultiHeadSelfAttention(d_model, num_heads)\n",
    "    x = torch.randn(batch, seq, d_model)\n",
    "\n",
    "    _, weights = mha(x)\n",
    "\n",
    "    # ê° headì˜ ê° queryì— ëŒ€í•´ weights í•©ì´ 1\n",
    "    row_sums = weights.sum(dim=-1)  # (batch, num_heads, seq)\n",
    "    expected = torch.ones_like(row_sums)\n",
    "\n",
    "    assert torch.allclose(row_sums, expected, atol=1e-5), \"Weights í•©ì´ 1ì´ ì•„ë‹˜!\"\n",
    "    print(\"âœ… Attention weights í•© ê²€ì¦ í†µê³¼!\")\n",
    "\n",
    "\n",
    "test_attention_sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Headë³„ Attention ì‹œê°í™”\n",
    "\n",
    "ê° headê°€ ë‹¤ë¥¸ íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_multihead_attention(weights, tokens=None, figsize=(16, 4)):\n",
    "    \"\"\"ê° headì˜ attention weights ì‹œê°í™”\"\"\"\n",
    "    if weights.dim() == 4:\n",
    "        weights = weights[0]  # ì²« ë²ˆì§¸ ë°°ì¹˜\n",
    "\n",
    "    num_heads = weights.shape[0]\n",
    "    seq_len = weights.shape[1]\n",
    "\n",
    "    if tokens is None:\n",
    "        tokens = [f\"pos{i}\" for i in range(seq_len)]\n",
    "\n",
    "    fig, axes = plt.subplots(1, num_heads, figsize=figsize)\n",
    "\n",
    "    for h in range(num_heads):\n",
    "        ax = axes[h] if num_heads > 1 else axes\n",
    "        im = ax.imshow(weights[h].detach().numpy(), cmap=\"Blues\", vmin=0, vmax=0.5)\n",
    "        ax.set_title(f\"Head {h+1}\")\n",
    "        ax.set_xticks(range(seq_len))\n",
    "        ax.set_yticks(range(seq_len))\n",
    "        if h == 0:\n",
    "            ax.set_xticklabels(tokens, rotation=45, ha=\"right\", fontsize=8)\n",
    "            ax.set_yticklabels(tokens, fontsize=8)\n",
    "        else:\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "\n",
    "    plt.suptitle(\"Multi-Head Attention Weights (ê° headê°€ ë‹¤ë¥¸ íŒ¨í„´ í•™ìŠµ)\", y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ì‹œê°í™” í…ŒìŠ¤íŠ¸\n",
    "batch, seq, d_model, num_heads = 1, 6, 64, 4\n",
    "tokens = [\"I\", \"love\", \"natural\", \"language\", \"processing\", \".\"]\n",
    "\n",
    "mha = MultiHeadSelfAttention(d_model, num_heads)\n",
    "x = torch.randn(batch, seq, d_model)\n",
    "_, weights = mha(x)\n",
    "\n",
    "visualize_multihead_attention(weights, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. PyTorch ê³µì‹ êµ¬í˜„ê³¼ ë¹„êµ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vs_pytorch():\n",
    "    batch, seq, d_model, num_heads = 2, 8, 64, 8\n",
    "\n",
    "    # ë™ì¼í•œ ê°€ì¤‘ì¹˜ë¡œ ì´ˆê¸°í™”í•˜ì—¬ ë¹„êµ\n",
    "    torch.manual_seed(42)\n",
    "    our_mha = MultiHeadSelfAttention(d_model, num_heads)\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    pytorch_mha = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
    "\n",
    "    x = torch.randn(batch, seq, d_model)\n",
    "\n",
    "    our_output, _ = our_mha(x)\n",
    "    pytorch_output, _ = pytorch_mha(x, x, x)\n",
    "\n",
    "    # ê°€ì¤‘ì¹˜ê°€ ë‹¤ë¥´ë¯€ë¡œ ì¶œë ¥ì€ ë‹¤ë¥¼ ìˆ˜ ìˆì§€ë§Œ, shapeì€ ë™ì¼í•´ì•¼ í•¨\n",
    "    assert our_output.shape == pytorch_output.shape, \"Shape ë¶ˆì¼ì¹˜!\"\n",
    "    print(\"âœ… PyTorch êµ¬í˜„ê³¼ shape ë™ì¼!\")\n",
    "    print(f\"   Our output: {our_output.shape}\")\n",
    "    print(f\"   PyTorch output: {pytorch_output.shape}\")\n",
    "\n",
    "\n",
    "test_vs_pytorch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Post-Quiz\n",
    "\n",
    "### Q1. num_heads=8, d_model=512ì¼ ë•Œ ê° headì˜ d_këŠ”?\n",
    "\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ:\n",
    "```\n",
    "\n",
    "### Q2. headë¥¼ ëŠ˜ë¦¬ë©´ ê³„ì‚°ëŸ‰ì´ ì¦ê°€í•˜ë‚˜ìš”?\n",
    "\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ:\n",
    "```\n",
    "\n",
    "### Q3. Multi-Headì˜ ì¥ì ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ?\n",
    "\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ:\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ì •ë‹µ\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ‘‰ í´ë¦­í•˜ì—¬ ì •ë‹µ ë³´ê¸°</summary>\n",
    "\n",
    "### Pre-Quiz\n",
    "\n",
    "**Q1. Multi-Head ì‚¬ìš© ì´ìœ **\n",
    "> - ì—¬ëŸ¬ ê´€ê³„ íŒ¨í„´ì„ **ë³‘ë ¬ë¡œ** í•™ìŠµ\n",
    "> - head 1: ì£¼ì–´-ë™ì‚¬, head 2: ìˆ˜ì‹ì–´ ê´€ê³„ ë“±\n",
    "> - Single headë¡œëŠ” í•œ ê°€ì§€ íŒ¨í„´ë§Œ í•™ìŠµ\n",
    "\n",
    "**Q2. d_k ê³„ì‚°**\n",
    "> d_k = d_model / num_heads = 512 / 8 = **64**\n",
    "\n",
    "**Q3. ê° headê°€ í•™ìŠµí•˜ëŠ” ê²ƒ**\n",
    "> - ì„œë¡œ ë‹¤ë¥¸ subspaceì—ì„œì˜ attention íŒ¨í„´\n",
    "> - ë¬¸ë²•ì  ê´€ê³„, ì˜ë¯¸ì  ê´€ê³„, ìœ„ì¹˜ ê´€ê³„ ë“±\n",
    "> - í•™ìŠµ í›„ ê° headê°€ specializationë¨\n",
    "\n",
    "---\n",
    "\n",
    "### Post-Quiz\n",
    "\n",
    "**Q1. d_k**\n",
    "\n",
    "> 512 / 8 = **64**\n",
    "\n",
    "**Q2. ê³„ì‚°ëŸ‰**\n",
    "\n",
    "> **ì¦ê°€í•˜ì§€ ì•ŠìŒ!** ì´ d_modelì€ ê·¸ëŒ€ë¡œì´ê³  ë‚˜ëˆ ì„œ ê³„ì‚°í•˜ë¯€ë¡œê³„ì‚°ëŸ‰ì€ ë™ì¼. ì˜¤íˆë ¤ ë³‘ë ¬í™” ê°€ëŠ¥í•˜ì—¬ íš¨ìœ¨ì .\n",
    "\n",
    "**Q3. ì¥ì  í•œ ë¬¸ì¥**\n",
    "\n",
    "> \"ì—¬ëŸ¬ ê´€ê³„ íŒ¨í„´ì„ ë³‘ë ¬ë¡œ í•™ìŠµí•˜ì—¬ ë‹¤ì–‘í•œ ì–¸ì–´ êµ¬ì¡°ë¥¼ ë™ì‹œì— í¬ì°©\"\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## êµ¬í˜„ ì •ë‹µ ì½”ë“œ\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ‘‰ í´ë¦­í•˜ì—¬ ì •ë‹µ ì½”ë“œ ë³´ê¸°</summary>\n",
    "\n",
    "```python\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        seq_q = query.size(1)\n",
    "        seq_k = key.size(1)\n",
    "\n",
    "        # 1. Linear projection\n",
    "        Q = self.W_q(query)  # (batch, seq_q, d_model)\n",
    "        K = self.W_k(key)    # (batch, seq_k, d_model)\n",
    "        V = self.W_v(value)  # (batch, seq_k, d_model)\n",
    "\n",
    "        # 2. Reshape to (batch, seq, num_heads, d_k)\n",
    "        Q = Q.view(batch_size, seq_q, self.num_heads, self.d_k)\n",
    "        K = K.view(batch_size, seq_k, self.num_heads, self.d_k)\n",
    "        V = V.view(batch_size, seq_k, self.num_heads, self.d_k)\n",
    "\n",
    "        # 3. Transpose to (batch, num_heads, seq, d_k)\n",
    "        Q = Q.transpose(1, 2)  # (batch, num_heads, seq_q, d_k)\n",
    "        K = K.transpose(1, 2)  # (batch, num_heads, seq_k, d_k)\n",
    "        V = V.transpose(1, 2)  # (batch, num_heads, seq_k, d_k)\n",
    "\n",
    "        # 4. Reshape for attention: (batch*num_heads, seq, d_k)\n",
    "        Q = Q.contiguous().view(batch_size * self.num_heads, seq_q, self.d_k)\n",
    "        K = K.contiguous().view(batch_size * self.num_heads, seq_k, self.d_k)\n",
    "        V = V.contiguous().view(batch_size * self.num_heads, seq_k, self.d_k)\n",
    "\n",
    "        # 5. Attention (maskë„ reshape í•„ìš”)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).expand(-1, self.num_heads, -1, -1)\n",
    "            mask = mask.contiguous().view(batch_size * self.num_heads, seq_q, seq_k)\n",
    "\n",
    "        output, weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # 6. Reshape: (batch*num_heads, seq, d_k) -> (batch, num_heads, seq, d_k)\n",
    "        output = output.view(batch_size, self.num_heads, seq_q, self.d_k)\n",
    "        weights = weights.view(batch_size, self.num_heads, seq_q, seq_k)\n",
    "\n",
    "        # 7. Transpose + concat: (batch, seq, d_model)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_q, self.d_model)\n",
    "\n",
    "        # 8. Output projection\n",
    "        output = self.W_o(output)\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        return output, weights\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
