{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. Transformer Encoder Block êµ¬í˜„\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- ì§€ê¸ˆê¹Œì§€ ë§Œë“  ì»´í¬ë„ŒíŠ¸ ì¡°ë¦½\n",
    "- Residual Connection ì´í•´\n",
    "- Pre-LN vs Post-LN êµ¬ì¡°\n",
    "\n",
    "## ê´€ë ¨ ë©´ì ‘ ì§ˆë¬¸\n",
    "- Encoder Blockì˜ êµ¬ì¡°ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”\n",
    "- Residual Connectionì´ ì™œ í•„ìš”í•œê°€ìš”?\n",
    "- BERTëŠ” ëª‡ ê°œì˜ Encoder Blockì„ ì‚¬ìš©í•˜ë‚˜ìš”?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pre-Quiz\n",
    "\n",
    "### Q1. Encoder Blockì˜ ë‘ ê°€ì§€ sub-layerëŠ”?\n",
    "```\n",
    "1. \n",
    "2. \n",
    "```\n",
    "\n",
    "### Q2. Residual Connectionì´ í•„ìš”í•œ ì´ìœ ëŠ”?\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ: \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ì „ì— êµ¬í˜„í•œ ì»´í¬ë„ŒíŠ¸ë“¤ (ì œê³µ)\n",
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.bmm(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return torch.bmm(weights, value), weights\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        seq_q, seq_k = query.size(1), key.size(1)\n",
    "        \n",
    "        Q = self.W_q(query).view(batch_size, seq_q, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(key).view(batch_size, seq_k, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(value).view(batch_size, seq_k, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        Q = Q.contiguous().view(batch_size * self.num_heads, seq_q, self.d_k)\n",
    "        K = K.contiguous().view(batch_size * self.num_heads, seq_k, self.d_k)\n",
    "        V = V.contiguous().view(batch_size * self.num_heads, seq_k, self.d_k)\n",
    "        \n",
    "        output, weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        output = output.view(batch_size, self.num_heads, seq_q, self.d_k)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_q, self.d_model)\n",
    "        \n",
    "        return self.dropout(self.W_o(output)), weights.view(batch_size, self.num_heads, seq_q, seq_k)\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        if d_ff is None:\n",
    "            d_ff = d_model * 4\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.fc2(self.dropout(F.gelu(self.fc1(x)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Encoder Block êµ¬í˜„\n",
    "\n",
    "### êµ¬ì¡° (Post-LN, ì›ë˜ Transformer)\n",
    "```\n",
    "x â†’ MultiHeadAttention â†’ + â†’ LayerNorm â†’ FFN â†’ + â†’ LayerNorm â†’ output\n",
    "    â†‘___________________|                       â†‘_________|\n",
    "         (residual)                              (residual)\n",
    "```\n",
    "\n",
    "### êµ¬ì¡° (Pre-LN, í˜„ëŒ€ì )\n",
    "```\n",
    "x â†’ LayerNorm â†’ MultiHeadAttention â†’ + â†’ LayerNorm â†’ FFN â†’ + â†’ output\n",
    "                                     â†‘                     â†‘\n",
    "    â†‘________________________________|   â†‘________________|\n",
    "              (residual)                    (residual)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# TODO: Encoder Block êµ¬í˜„ (Pre-LN ë°©ì‹)\n",
    "# =================================================\n",
    "#\n",
    "# êµ¬ì¡° (Pre-LN):\n",
    "#   1. norm1 â†’ attention â†’ dropout â†’ residual\n",
    "#   2. norm2 â†’ ffn â†’ residual\n",
    "#\n",
    "# =================================================\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        num_heads: int,\n",
    "        d_ff: int = None,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ============================================\n",
    "        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
    "        # - attention: MultiHeadAttention\n",
    "        # - ffn: PositionWiseFeedForward\n",
    "        # - norm1, norm2: LayerNorm\n",
    "        # - dropout\n",
    "        # ============================================\n",
    "        \n",
    "        pass  # ì´ ì¤„ì„ ì§€ìš°ê³  êµ¬í˜„\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq, d_model)\n",
    "            mask: optional attention mask\n",
    "        Returns:\n",
    "            output: (batch, seq, d_model)\n",
    "        \"\"\"\n",
    "        # ============================================\n",
    "        # Pre-LN êµ¬í˜„:\n",
    "        # 1. attn_out = attention(norm1(x), norm1(x), norm1(x))\n",
    "        # 2. x = x + dropout(attn_out)\n",
    "        # 3. ffn_out = ffn(norm2(x))\n",
    "        # 4. x = x + ffn_out\n",
    "        # ============================================\n",
    "        \n",
    "        pass  # ì´ ì¤„ì„ ì§€ìš°ê³  êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ í…ŒìŠ¤íŠ¸ ============\n",
    "def test_encoder_block():\n",
    "    batch, seq, d_model, num_heads = 2, 10, 64, 8\n",
    "    \n",
    "    encoder = EncoderBlock(d_model, num_heads)\n",
    "    x = torch.randn(batch, seq, d_model)\n",
    "    \n",
    "    output = encoder(x)\n",
    "    \n",
    "    assert output.shape == x.shape, f\"Shape ì˜¤ë¥˜: {output.shape}\"\n",
    "    print(\"âœ… Encoder Block í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
    "    print(f\"   Input: {x.shape} â†’ Output: {output.shape}\")\n",
    "\n",
    "test_encoder_block()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Transformer Encoder (ì—¬ëŸ¬ ë¸”ë¡ ìŒ“ê¸°)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, num_layers, d_ff=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(d_model)  # ë§ˆì§€ë§‰ ì •ê·œí™”\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "encoder = TransformerEncoder(d_model=64, num_heads=8, num_layers=6)\n",
    "x = torch.randn(2, 10, 64)\n",
    "output = encoder(x)\n",
    "print(f\"6-layer Encoder: {x.shape} â†’ {output.shape}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in encoder.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ì •ë‹µ\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ‘‰ í´ë¦­í•˜ì—¬ ì •ë‹µ ì½”ë“œ ë³´ê¸°</summary>\n",
    "\n",
    "```python\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.ffn = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Pre-LN: LayerNorm first\n",
    "        normed = self.norm1(x)\n",
    "        attn_out, _ = self.attention(normed, normed, normed, mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        \n",
    "        normed = self.norm2(x)\n",
    "        ffn_out = self.ffn(normed)\n",
    "        x = x + ffn_out\n",
    "        \n",
    "        return x\n",
    "```\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
