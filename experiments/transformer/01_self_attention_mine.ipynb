{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Self-Attention ì§ì ‘ êµ¬í˜„ (PyTorch)\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- Scaled Dot-Product Attentionì„ **ì²˜ìŒë¶€í„° êµ¬í˜„**\n",
    "- Q, K, Vì˜ ì—­í•  ì´í•´\n",
    "- âˆšd_k ìŠ¤ì¼€ì¼ë§ì´ í•„ìš”í•œ ì´ìœ  ì²´í—˜\n",
    "\n",
    "## ê´€ë ¨ ë©´ì ‘ ì§ˆë¬¸\n",
    "- Self-Attentionì˜ ë™ì‘ ì›ë¦¬ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”\n",
    "- Q, K, Vê°€ ê°ê° ë¬´ì—‡ì„ ì˜ë¯¸í•˜ë‚˜ìš”?\n",
    "- âˆšd_kë¡œ ë‚˜ëˆ„ëŠ” ì´ìœ ëŠ”?\n",
    "- Attention maskëŠ” ì–¸ì œ ì“°ë‚˜ìš”?\n",
    "\n",
    "## í•µì‹¬ ìˆ˜ì‹\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pre-Quiz\n",
    "\n",
    "### Q1. Self-Attentionì—ì„œ Q, K, Vì˜ ì—­í• ì€?\n",
    "```\n",
    "Q (Query): \n",
    "K (Key): \n",
    "V (Value): \n",
    "```\n",
    "\n",
    "### Q2. âˆšd_kë¡œ ë‚˜ëˆ„ëŠ” ì´ìœ ëŠ”?\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ: \n",
    "```\n",
    "\n",
    "### Q3. Causal MaskëŠ” ì–¸ì œ í•„ìš”í•œê°€ìš”?\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ: \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "\n",
    "plt.rcParams['font.family'] = 'AppleGothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# ì¬í˜„ì„±\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Scaled Dot-Product Attention êµ¬í˜„\n",
    "\n",
    "### êµ¬í˜„ ë‹¨ê³„\n",
    "1. Qì™€ K^T ë‚´ì  â†’ attention scores\n",
    "2. âˆšd_kë¡œ ìŠ¤ì¼€ì¼ë§\n",
    "3. (optional) mask ì ìš©\n",
    "4. softmax â†’ attention weights\n",
    "5. weightsì™€ V ë‚´ì  â†’ output\n",
    "\n",
    "### Shape íë¦„\n",
    "```\n",
    "Q: (batch, seq_q, d_k)\n",
    "K: (batch, seq_k, d_k)\n",
    "V: (batch, seq_k, d_v)\n",
    "\n",
    "scores = Q @ K^T: (batch, seq_q, seq_k)\n",
    "weights = softmax(scores): (batch, seq_q, seq_k)\n",
    "output = weights @ V: (batch, seq_q, d_v)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# TODO: Scaled Dot-Product Attention êµ¬í˜„\n",
    "# =================================================\n",
    "#\n",
    "# ìˆ˜ì‹: Attention(Q, K, V) = softmax(QK^T / âˆšd_k) V\n",
    "#\n",
    "# Args:\n",
    "#   query: (batch, seq_len, d_k)\n",
    "#   key: (batch, seq_len, d_k)\n",
    "#   value: (batch, seq_len, d_v)\n",
    "#   mask: (batch, seq_len, seq_len) - Trueì¸ ìœ„ì¹˜ ë§ˆìŠ¤í‚¹\n",
    "#\n",
    "# Returns:\n",
    "#   output: (batch, seq_len, d_v)\n",
    "#   attention_weights: (batch, seq_len, seq_len)\n",
    "#\n",
    "# í—ˆìš© í•¨ìˆ˜:\n",
    "#   - torch.bmm() ë˜ëŠ” torch.matmul()\n",
    "#   - F.softmax()\n",
    "#   - tensor.transpose()\n",
    "#   - tensor.masked_fill()\n",
    "#   - math.sqrt()\n",
    "#\n",
    "# ê¸ˆì§€:\n",
    "#   - F.scaled_dot_product_attention()\n",
    "#   - nn.MultiheadAttention()\n",
    "#\n",
    "# =================================================\n",
    "\n",
    "def scaled_dot_product_attention(\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    mask: Optional[torch.Tensor] = None,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention\n",
    "    \n",
    "    êµ¬í˜„ ìˆœì„œ:\n",
    "    1. d_k = query.size(-1)\n",
    "    2. scores = Q @ K^T  (hint: torch.bmm ë˜ëŠ” matmul)\n",
    "    3. scores = scores / sqrt(d_k)\n",
    "    4. if mask: scoresë¥¼ -infë¡œ ë§ˆìŠ¤í‚¹\n",
    "    5. weights = softmax(scores, dim=-1)\n",
    "    6. output = weights @ V\n",
    "    \"\"\"\n",
    "    # ============================================\n",
    "    d_k = query.size(-1)\n",
    "    \n",
    "    \n",
    "    # ============================================\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ í…ŒìŠ¤íŠ¸ 1: ê¸°ë³¸ ë™ì‘ ============\n",
    "def test_basic():\n",
    "    batch, seq, d_k = 2, 4, 8\n",
    "    q = torch.randn(batch, seq, d_k)\n",
    "    k = torch.randn(batch, seq, d_k)\n",
    "    v = torch.randn(batch, seq, d_k)\n",
    "    \n",
    "    output, weights = scaled_dot_product_attention(q, k, v)\n",
    "    \n",
    "    assert output.shape == (batch, seq, d_k), f\"Output shape ì˜¤ë¥˜: {output.shape}\"\n",
    "    assert weights.shape == (batch, seq, seq), f\"Weights shape ì˜¤ë¥˜: {weights.shape}\"\n",
    "    print(\"âœ… ê¸°ë³¸ ë™ì‘ í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
    "    print(f\"   Output shape: {output.shape}\")\n",
    "    print(f\"   Weights shape: {weights.shape}\")\n",
    "\n",
    "test_basic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ í…ŒìŠ¤íŠ¸ 2: Softmax ê²€ì¦ ============\n",
    "def test_softmax():\n",
    "    batch, seq, d_k = 2, 4, 8\n",
    "    q = torch.randn(batch, seq, d_k)\n",
    "    k = torch.randn(batch, seq, d_k)\n",
    "    v = torch.randn(batch, seq, d_k)\n",
    "    \n",
    "    _, weights = scaled_dot_product_attention(q, k, v)\n",
    "    \n",
    "    # ê° rowì˜ í•©ì´ 1ì¸ì§€ í™•ì¸\n",
    "    row_sums = weights.sum(dim=-1)\n",
    "    assert torch.allclose(row_sums, torch.ones_like(row_sums), atol=1e-5), \\\n",
    "        f\"Weights í•©ì´ 1ì´ ì•„ë‹˜: {row_sums}\"\n",
    "    print(\"âœ… Softmax ê²€ì¦ í†µê³¼! (ê° row í•© = 1)\")\n",
    "\n",
    "test_softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ í…ŒìŠ¤íŠ¸ 3: Mask ì ìš© ============\n",
    "def test_mask():\n",
    "    batch, seq, d_k = 2, 4, 8\n",
    "    q = torch.randn(batch, seq, d_k)\n",
    "    k = torch.randn(batch, seq, d_k)\n",
    "    v = torch.randn(batch, seq, d_k)\n",
    "    \n",
    "    # Causal mask (ìƒì‚¼ê° = True)\n",
    "    mask = torch.triu(torch.ones(seq, seq), diagonal=1).bool()\n",
    "    mask = mask.unsqueeze(0).expand(batch, -1, -1)\n",
    "    \n",
    "    _, weights = scaled_dot_product_attention(q, k, v, mask=mask)\n",
    "    \n",
    "    # ë§ˆìŠ¤í¬ëœ ìœ„ì¹˜ì˜ weightëŠ” 0ì´ì–´ì•¼ í•¨\n",
    "    masked_weights = weights[mask]\n",
    "    assert torch.allclose(masked_weights, torch.zeros_like(masked_weights), atol=1e-6), \\\n",
    "        f\"ë§ˆìŠ¤í¬ëœ ìœ„ì¹˜ê°€ 0ì´ ì•„ë‹˜: max={masked_weights.max()}\"\n",
    "    print(\"âœ… Mask ì ìš© í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
    "    print(f\"   ë§ˆìŠ¤í¬ëœ ìœ„ì¹˜ max weight: {masked_weights.max():.2e}\")\n",
    "\n",
    "test_mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ í…ŒìŠ¤íŠ¸ 4: PyTorch êµ¬í˜„ê³¼ ë¹„êµ ============\n",
    "def test_vs_pytorch():\n",
    "    batch, seq, d_k = 2, 4, 8\n",
    "    q = torch.randn(batch, seq, d_k)\n",
    "    k = torch.randn(batch, seq, d_k)\n",
    "    v = torch.randn(batch, seq, d_k)\n",
    "    \n",
    "    our_output, _ = scaled_dot_product_attention(q, k, v)\n",
    "    pytorch_output = F.scaled_dot_product_attention(q, k, v)\n",
    "    \n",
    "    assert torch.allclose(our_output, pytorch_output, atol=1e-5), \\\n",
    "        f\"PyTorch êµ¬í˜„ê³¼ ë‹¤ë¦„!\"\n",
    "    print(\"âœ… PyTorch ê³µì‹ êµ¬í˜„ê³¼ ë™ì¼! ğŸ‰\")\n",
    "\n",
    "test_vs_pytorch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. SelfAttention í´ë˜ìŠ¤ êµ¬í˜„\n",
    "\n",
    "ì…ë ¥ Xë¥¼ Q, K, Vë¡œ projectioní•œ í›„ attention ê³„ì‚°\n",
    "\n",
    "```\n",
    "X: (batch, seq, d_model)\n",
    "    â†“ W_q, W_k, W_v (Linear layers)\n",
    "Q, K, V: (batch, seq, d_k), (batch, seq, d_k), (batch, seq, d_v)\n",
    "    â†“ scaled_dot_product_attention\n",
    "Output: (batch, seq, d_v)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# TODO: SelfAttention í´ë˜ìŠ¤ êµ¬í˜„\n",
    "# =================================================\n",
    "#\n",
    "# êµ¬ì¡°:\n",
    "#   - W_q: Linear(d_model, d_k)\n",
    "#   - W_k: Linear(d_model, d_k)\n",
    "#   - W_v: Linear(d_model, d_v)\n",
    "#\n",
    "# Forward:\n",
    "#   1. Q = X @ W_q\n",
    "#   2. K = X @ W_k\n",
    "#   3. V = X @ W_v\n",
    "#   4. output, weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "#\n",
    "# =================================================\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, d_k: int = None, d_v: int = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: ì…ë ¥ ì°¨ì›\n",
    "            d_k: Query, Key ì°¨ì› (default: d_model)\n",
    "            d_v: Value ì°¨ì› (default: d_model)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # ============================================\n",
    "        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
    "        # d_k, d_vê°€ Noneì´ë©´ d_model ì‚¬ìš©\n",
    "        # W_q, W_k, W_v Linear layers ìƒì„±\n",
    "        # ============================================\n",
    "        \n",
    "        pass  # ì´ ì¤„ì„ ì§€ìš°ê³  êµ¬í˜„\n",
    "    \n",
    "    def forward(\n",
    "        self, x: torch.Tensor, mask: Optional[torch.Tensor] = None\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq, d_model)\n",
    "            mask: optional attention mask\n",
    "        Returns:\n",
    "            output: (batch, seq, d_v)\n",
    "            attention_weights: (batch, seq, seq)\n",
    "        \"\"\"\n",
    "        # ============================================\n",
    "        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
    "        # 1. Q, K, V projection\n",
    "        # 2. scaled_dot_product_attention í˜¸ì¶œ\n",
    "        # ============================================\n",
    "        \n",
    "        pass  # ì´ ì¤„ì„ ì§€ìš°ê³  êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ í…ŒìŠ¤íŠ¸: SelfAttention ============\n",
    "def test_self_attention():\n",
    "    batch, seq, d_model = 2, 4, 64\n",
    "    x = torch.randn(batch, seq, d_model)\n",
    "    \n",
    "    attention = SelfAttention(d_model)\n",
    "    output, weights = attention(x)\n",
    "    \n",
    "    assert output.shape == x.shape, f\"Output shape ì˜¤ë¥˜: {output.shape}\"\n",
    "    assert weights.shape == (batch, seq, seq), f\"Weights shape ì˜¤ë¥˜: {weights.shape}\"\n",
    "    \n",
    "    # d_k, d_v ë‹¤ë¥´ê²Œ ì„¤ì •\n",
    "    attention2 = SelfAttention(d_model, d_k=32, d_v=48)\n",
    "    output2, _ = attention2(x)\n",
    "    assert output2.shape == (batch, seq, 48), f\"d_v ì ìš© ì˜¤ë¥˜: {output2.shape}\"\n",
    "    \n",
    "    print(\"âœ… SelfAttention í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
    "    print(f\"   Input: {x.shape} â†’ Output: {output.shape}\")\n",
    "\n",
    "test_self_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Attention ì‹œê°í™”\n",
    "\n",
    "í•™ìŠµëœ attention weightsë¥¼ heatmapìœ¼ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(weights, tokens=None, title=\"Attention Weights\"):\n",
    "    \"\"\"Attention weights heatmap ì‹œê°í™”\"\"\"\n",
    "    if weights.dim() == 3:\n",
    "        weights = weights[0]  # ì²« ë²ˆì§¸ ë°°ì¹˜ë§Œ\n",
    "    \n",
    "    weights = weights.detach().numpy()\n",
    "    seq_len = weights.shape[0]\n",
    "    \n",
    "    if tokens is None:\n",
    "        tokens = [f\"pos_{i}\" for i in range(seq_len)]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    im = ax.imshow(weights, cmap='Blues')\n",
    "    \n",
    "    ax.set_xticks(range(seq_len))\n",
    "    ax.set_yticks(range(seq_len))\n",
    "    ax.set_xticklabels(tokens, rotation=45, ha='right')\n",
    "    ax.set_yticklabels(tokens)\n",
    "    \n",
    "    ax.set_xlabel('Key')\n",
    "    ax.set_ylabel('Query')\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    plt.colorbar(im)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ì˜ˆì‹œ\n",
    "batch, seq, d_model = 1, 6, 64\n",
    "x = torch.randn(batch, seq, d_model)\n",
    "attention = SelfAttention(d_model)\n",
    "_, weights = attention(x)\n",
    "\n",
    "tokens = ['ë‚˜ëŠ”', 'ì–´ì œ', 'í•™êµì—', 'ê°”ë‹¤', '.', '[PAD]']\n",
    "visualize_attention(weights, tokens, \"Self-Attention Weights (ëœë¤ ì´ˆê¸°í™”)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Causal Mask ì ìš© ì‹œê°í™”\n",
    "seq = 6\n",
    "causal_mask = torch.triu(torch.ones(seq, seq), diagonal=1).bool()\n",
    "\n",
    "x = torch.randn(1, seq, d_model)\n",
    "_, weights_causal = attention(x, mask=causal_mask)\n",
    "\n",
    "visualize_attention(weights_causal, tokens, \"Causal Masked Attention\\n(ë¯¸ë˜ í† í° ë³¼ ìˆ˜ ì—†ìŒ)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. âˆšd_k ìŠ¤ì¼€ì¼ë§ íš¨ê³¼ ì‹¤í—˜\n",
    "\n",
    "ìŠ¤ì¼€ì¼ë§ ì—†ì´ ì–´ë–¤ ë¬¸ì œê°€ ìƒê¸°ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_without_scaling(query, key, value):\n",
    "    \"\"\"ìŠ¤ì¼€ì¼ë§ ì—†ëŠ” attention (ë¬¸ì œ í™•ì¸ìš©)\"\"\"\n",
    "    scores = torch.bmm(query, key.transpose(-2, -1))\n",
    "    # âˆšd_kë¡œ ë‚˜ëˆ„ì§€ ì•ŠìŒ!\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.bmm(weights, value)\n",
    "    return output, weights\n",
    "\n",
    "# d_kê°€ ì»¤ì§ˆìˆ˜ë¡ ë¬¸ì œ ì‹¬í™”\n",
    "d_k_values = [8, 64, 256, 1024]\n",
    "\n",
    "fig, axes = plt.subplots(2, len(d_k_values), figsize=(16, 8))\n",
    "\n",
    "for i, d_k in enumerate(d_k_values):\n",
    "    q = torch.randn(1, 10, d_k)\n",
    "    k = torch.randn(1, 10, d_k)\n",
    "    v = torch.randn(1, 10, d_k)\n",
    "    \n",
    "    # ìŠ¤ì¼€ì¼ë§ ì—†ìŒ\n",
    "    _, weights_no_scale = attention_without_scaling(q, k, v)\n",
    "    \n",
    "    # ìŠ¤ì¼€ì¼ë§ ìˆìŒ\n",
    "    _, weights_scaled = scaled_dot_product_attention(q, k, v)\n",
    "    \n",
    "    # ì‹œê°í™”\n",
    "    axes[0, i].imshow(weights_no_scale[0].detach(), cmap='Blues', vmin=0, vmax=1)\n",
    "    axes[0, i].set_title(f'd_k={d_k}\\nìŠ¤ì¼€ì¼ë§ ì—†ìŒ')\n",
    "    axes[0, i].set_xlabel(f'max={weights_no_scale.max():.3f}')\n",
    "    \n",
    "    axes[1, i].imshow(weights_scaled[0].detach(), cmap='Blues', vmin=0, vmax=1)\n",
    "    axes[1, i].set_title(f'd_k={d_k}\\nâˆšd_k ìŠ¤ì¼€ì¼ë§')\n",
    "    axes[1, i].set_xlabel(f'max={weights_scaled.max():.3f}')\n",
    "\n",
    "axes[0, 0].set_ylabel('No Scaling')\n",
    "axes[1, 0].set_ylabel('With Scaling')\n",
    "\n",
    "plt.suptitle('âˆšd_k ìŠ¤ì¼€ì¼ë§ íš¨ê³¼\\n(ìŠ¤ì¼€ì¼ë§ ì—†ìœ¼ë©´ d_k ì»¤ì§ˆìˆ˜ë¡ one-hotì— ê°€ê¹Œì›Œì§ â†’ gradient ì†Œì‹¤)', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Post-Quiz\n",
    "\n",
    "### Q1. ìŠ¤ì¼€ì¼ë§ ì—†ì´ d_kê°€ ì»¤ì§€ë©´ attention weightsê°€ ì–´ë–»ê²Œ ë³€í•˜ë‚˜ìš”?\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ: \n",
    "```\n",
    "\n",
    "### Q2. ì´ê²ƒì´ í•™ìŠµì— ì™œ ë¬¸ì œê°€ ë˜ë‚˜ìš”?\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ: \n",
    "```\n",
    "\n",
    "### Q3. Causal maskë¥¼ ì ìš©í•˜ë©´ attention íŒ¨í„´ì´ ì–´ë–»ê²Œ ë°”ë€Œë‚˜ìš”?\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ: \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ì •ë‹µ ë° í•´ì„¤\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ‘‰ í´ë¦­í•˜ì—¬ ì •ë‹µ ë³´ê¸°</summary>\n",
    "\n",
    "### Pre-Quiz ì •ë‹µ\n",
    "\n",
    "**Q1. Q, K, Vì˜ ì—­í• **\n",
    "> - Q (Query): \"ë¬´ì—‡ì„ ì°¾ê³  ìˆëŠ”ì§€\" - í˜„ì¬ ìœ„ì¹˜ì—ì„œ ì–´ë–¤ ì •ë³´ê°€ í•„ìš”í•œì§€\n",
    "> - K (Key): \"ë¬´ì—‡ì„ ê°€ì§€ê³  ìˆëŠ”ì§€\" - ê° ìœ„ì¹˜ê°€ ì–´ë–¤ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ”ì§€\n",
    "> - V (Value): \"ì‹¤ì œ ê°’\" - ì‹¤ì œë¡œ ì „ë‹¬í•  ì •ë³´\n",
    "> - QÂ·K ë‚´ì  = ìœ ì‚¬ë„, ë†’ìœ¼ë©´ í•´ë‹¹ Vë¥¼ ë§ì´ ê°€ì ¸ì˜´\n",
    "\n",
    "**Q2. âˆšd_kë¡œ ë‚˜ëˆ„ëŠ” ì´ìœ **\n",
    "> - d_kê°€ í¬ë©´ QÂ·K ë‚´ì ê°’ì˜ ë¶„ì‚°ì´ ì»¤ì§ (ê° ì›ì†Œê°€ ë…ë¦½ì´ë©´ ë¶„ì‚° âˆ d_k)\n",
    "> - ë‚´ì ê°’ì´ í¬ë©´ softmaxê°€ ê·¹ë‹¨ì  (one-hotì— ê°€ê¹Œì›€)\n",
    "> - gradientê°€ ê±°ì˜ 0 â†’ í•™ìŠµ ë¶ˆê°€\n",
    "> - âˆšd_kë¡œ ë‚˜ëˆ„ë©´ ë¶„ì‚°ì„ 1ë¡œ ì •ê·œí™”\n",
    "\n",
    "**Q3. Causal Mask**\n",
    "> - Decoderì—ì„œ ì‚¬ìš© (GPT ë“±)\n",
    "> - ë¯¸ë˜ í† í°ì„ ë³¼ ìˆ˜ ì—†ê²Œ í•¨ (Auto-regressive ìƒì„±)\n",
    "> - ìƒì‚¼ê° ë¶€ë¶„ì„ -infë¡œ ë§ˆìŠ¤í‚¹\n",
    "\n",
    "---\n",
    "\n",
    "### Post-Quiz ì •ë‹µ\n",
    "\n",
    "**Q1. ìŠ¤ì¼€ì¼ë§ ì—†ì´ d_k ì»¤ì§€ë©´?**\n",
    "> - ë‚´ì ê°’ ë¶„ì‚° ì¦ê°€ â†’ softmax ì…ë ¥ì´ ê·¹ë‹¨ì \n",
    "> - attention weightsê°€ one-hotì— ê°€ê¹Œì›Œì§\n",
    "> - í•˜ë‚˜ì˜ í† í°ì—ë§Œ ê±°ì˜ 100% ì§‘ì¤‘\n",
    "\n",
    "**Q2. í•™ìŠµì— ì™œ ë¬¸ì œ?**\n",
    "> - softmax(ê·¹ë‹¨ê°’)ì˜ gradient â‰ˆ 0\n",
    "> - Vanishing Gradient â†’ í•™ìŠµ ì•ˆ ë¨\n",
    "> - ì—¬ëŸ¬ í† í°ì˜ ì •ë³´ë¥¼ í˜¼í•©í•˜ì§€ ëª»í•¨\n",
    "\n",
    "**Q3. Causal mask íŒ¨í„´**\n",
    "> - í•˜ì‚¼ê° íŒ¨í„´ (ëŒ€ê°ì„  í¬í•¨)\n",
    "> - ê° ìœ„ì¹˜ëŠ” ìì‹ ê³¼ ì´ì „ ìœ„ì¹˜ë§Œ ë³¼ ìˆ˜ ìˆìŒ\n",
    "> - ì²« í† í°ì€ ìê¸° ìì‹ ë§Œ, ë§ˆì§€ë§‰ í† í°ì€ ëª¨ë“  ì´ì „ í† í° ì°¸ì¡°\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## êµ¬í˜„ ì •ë‹µ ì½”ë“œ\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ‘‰ í´ë¦­í•˜ì—¬ ì •ë‹µ ì½”ë“œ ë³´ê¸°</summary>\n",
    "\n",
    "```python\n",
    "def scaled_dot_product_attention(\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    mask: Optional[torch.Tensor] = None,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \n",
    "    d_k = query.size(-1)\n",
    "    \n",
    "    # 1. Q @ K^T\n",
    "    scores = torch.bmm(query, key.transpose(-2, -1))\n",
    "    \n",
    "    # 2. Scale by âˆšd_k\n",
    "    scores = scores / math.sqrt(d_k)\n",
    "    \n",
    "    # 3. Apply mask\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "    \n",
    "    # 4. Softmax\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # 5. Weighted sum with V\n",
    "    output = torch.bmm(weights, value)\n",
    "    \n",
    "    return output, weights\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, d_k: int = None, d_v: int = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_k = d_k if d_k is not None else d_model\n",
    "        self.d_v = d_v if d_v is not None else d_model\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, self.d_k)\n",
    "        self.W_k = nn.Linear(d_model, self.d_k)\n",
    "        self.W_v = nn.Linear(d_model, self.d_v)\n",
    "    \n",
    "    def forward(\n",
    "        self, x: torch.Tensor, mask: Optional[torch.Tensor] = None\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        output, weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        return output, weights\n",
    "```\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
