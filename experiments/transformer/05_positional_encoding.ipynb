{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. Positional Encoding êµ¬í˜„\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- Sin/Cos Positional Encoding êµ¬í˜„\n",
    "- ì™œ ìœ„ì¹˜ ì •ë³´ê°€ í•„ìš”í•œì§€ ì´í•´\n",
    "- Sinusoidal vs Learnable PE ë¹„êµ\n",
    "\n",
    "## ê´€ë ¨ ë©´ì ‘ ì§ˆë¬¸\n",
    "- Transformerì— ìœ„ì¹˜ ì •ë³´ê°€ í•„ìš”í•œ ì´ìœ ëŠ”?\n",
    "- Sin/Cosë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ”?\n",
    "- RoPEì™€ Sinusoidal PEì˜ ì°¨ì´ëŠ”?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pre-Quiz\n",
    "\n",
    "### Q1. Attentionì€ ì™œ ìœ„ì¹˜ ì •ë³´ë¥¼ ìƒë‚˜ìš”?\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ: \n",
    "```\n",
    "\n",
    "### Q2. Sin/Cos PEì˜ ì¥ì ì€?\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ: \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams['font.family'] = 'AppleGothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Sinusoidal Positional Encoding êµ¬í˜„\n",
    "\n",
    "### ìˆ˜ì‹\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "- pos: ìœ„ì¹˜ (0, 1, 2, ...)\n",
    "- i: ì°¨ì› ì¸ë±ìŠ¤\n",
    "- ì§ìˆ˜ ì°¨ì›: sin, í™€ìˆ˜ ì°¨ì›: cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# TODO: Sinusoidal Positional Encoding êµ¬í˜„\n",
    "# =================================================\n",
    "#\n",
    "# ìˆ˜ì‹:\n",
    "#   PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "#   PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "#\n",
    "# ì¶œë ¥: (1, max_len, d_model) - ë¯¸ë¦¬ ê³„ì‚°í•´ì„œ ì €ì¥\n",
    "#\n",
    "# =================================================\n",
    "\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # ============================================\n",
    "        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
    "        # 1. position: (max_len, 1) - 0, 1, 2, ..., max_len-1\n",
    "        # 2. div_term: 10000^(2i/d_model)\n",
    "        # 3. PE ê³„ì‚°: ì§ìˆ˜=sin, í™€ìˆ˜=cos\n",
    "        # 4. register_bufferë¡œ ì €ì¥ (í•™ìŠµ ì•ˆ í•¨)\n",
    "        #\n",
    "        # íŒíŠ¸:\n",
    "        #   position = torch.arange(max_len).unsqueeze(1)\n",
    "        #   div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        # ============================================\n",
    "        \n",
    "        pass  # ì´ ì¤„ì„ ì§€ìš°ê³  êµ¬í˜„\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model)\n",
    "        Returns:\n",
    "            output: (batch, seq_len, d_model) with PE added\n",
    "        \"\"\"\n",
    "        # ============================================\n",
    "        # x + PE[:, :seq_len, :]\n",
    "        # ============================================\n",
    "        \n",
    "        pass  # ì´ ì¤„ì„ ì§€ìš°ê³  êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ í…ŒìŠ¤íŠ¸ ============\n",
    "def test_pe():\n",
    "    batch, seq, d_model = 2, 50, 64\n",
    "    \n",
    "    pe = SinusoidalPositionalEncoding(d_model, max_len=100)\n",
    "    x = torch.zeros(batch, seq, d_model)  # ì…ë ¥ì´ 0ì´ë©´ PEë§Œ ì¶œë ¥\n",
    "    \n",
    "    output = pe(x)\n",
    "    \n",
    "    assert output.shape == x.shape, f\"Shape ì˜¤ë¥˜: {output.shape}\"\n",
    "    \n",
    "    # ê°™ì€ ìœ„ì¹˜ëŠ” í•­ìƒ ê°™ì€ PE\n",
    "    assert torch.allclose(output[0], output[1]), \"batchê°„ PEê°€ ë‹¤ë¦„!\"\n",
    "    \n",
    "    print(\"âœ… Positional Encoding í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
    "    print(f\"   Output shape: {output.shape}\")\n",
    "\n",
    "test_pe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. PE ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 64\n",
    "max_len = 100\n",
    "\n",
    "pe = SinusoidalPositionalEncoding(d_model, max_len, dropout=0.0)\n",
    "x = torch.zeros(1, max_len, d_model)\n",
    "pe_values = pe(x)[0].detach().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Heatmap\n",
    "im = axes[0].imshow(pe_values.T, cmap='RdBu', aspect='auto')\n",
    "axes[0].set_xlabel('Position')\n",
    "axes[0].set_ylabel('Dimension')\n",
    "axes[0].set_title('Positional Encoding Heatmap')\n",
    "plt.colorbar(im, ax=axes[0])\n",
    "\n",
    "# íŠ¹ì • ì°¨ì›ì˜ ê°’\n",
    "for dim in [0, 1, 10, 20, 30]:\n",
    "    axes[1].plot(pe_values[:, dim], label=f'dim {dim}')\n",
    "\n",
    "axes[1].set_xlabel('Position')\n",
    "axes[1].set_ylabel('Value')\n",
    "axes[1].set_title('Positional Encoding by Dimension\\në‚®ì€ ì°¨ì›: ë¹ ë¥¸ ì£¼ê¸°, ë†’ì€ ì°¨ì›: ëŠë¦° ì£¼ê¸°')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. ìœ„ì¹˜ ê°„ ìœ ì‚¬ë„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìœ„ì¹˜ ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„\n",
    "pe_tensor = torch.tensor(pe_values)\n",
    "pe_norm = pe_tensor / pe_tensor.norm(dim=-1, keepdim=True)\n",
    "similarity = pe_norm @ pe_norm.T\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(similarity.numpy()[:50, :50], cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Position')\n",
    "plt.title('Position ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„\\nëŒ€ê°ì„ : ìê¸° ìì‹  (1.0)\\nê°€ê¹Œìš´ ìœ„ì¹˜: ë†’ì€ ìœ ì‚¬ë„')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Post-Quiz\n",
    "\n",
    "### Q1. í•™ìŠµëœ PE ëŒ€ì‹  Sin/Cosë¥¼ ì“°ëŠ” ì¥ì ì€?\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ: \n",
    "```\n",
    "\n",
    "### Q2. 10000ì´ë¼ëŠ” base ê°’ì˜ ì˜ë¯¸ëŠ”?\n",
    "```\n",
    "ë‹¹ì‹ ì˜ ë‹µ: \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ì •ë‹µ\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ‘‰ í´ë¦­í•˜ì—¬ ì •ë‹µ ë³´ê¸°</summary>\n",
    "\n",
    "### Pre-Quiz\n",
    "\n",
    "**Q1. Attentionì´ ìœ„ì¹˜ ì •ë³´ë¥¼ ìƒëŠ” ì´ìœ **\n",
    "> - Self-Attentionì€ ìˆœì„œì— **invariant** (permutation equivariant)\n",
    "> - ì…ë ¥ ìˆœì„œë¥¼ ë°”ê¿”ë„ ì¶œë ¥ë„ ê°™ì€ ìˆœì„œë¡œ ë°”ë€œ\n",
    "> - ë¬¸ì¥ì—ì„œ ë‹¨ì–´ ìˆœì„œ ì •ë³´ê°€ í•„ìˆ˜ì¸ë° ì´ë¥¼ ëª¨ë¦„\n",
    "\n",
    "**Q2. Sin/Cos PE ì¥ì **\n",
    "> - **ì™¸ì‚½ ê°€ëŠ¥**: í•™ìŠµ ë•Œë³´ë‹¤ ê¸´ ì‹œí€€ìŠ¤ë„ ì²˜ë¦¬\n",
    "> - **ìƒëŒ€ ìœ„ì¹˜**: PE(pos+k)ë¥¼ PE(pos)ì˜ ì„ í˜• í•¨ìˆ˜ë¡œ í‘œí˜„ ê°€ëŠ¥\n",
    "> - íŒŒë¼ë¯¸í„° ì—†ìŒ (ë©”ëª¨ë¦¬ íš¨ìœ¨)\n",
    "\n",
    "---\n",
    "\n",
    "### Post-Quiz\n",
    "\n",
    "**Q1. Sin/Cos vs Learned**\n",
    "> - ì™¸ì‚½ ê°€ëŠ¥ (í•™ìŠµ ì•ˆ ë³¸ ê¸´ ì‹œí€€ìŠ¤)\n",
    "> - ì‹¤ì œë¡œ ì„±ëŠ¥ì€ ë¹„ìŠ·í•¨\n",
    "> - BERTëŠ” learned, GPT-2ë„ learned ì‚¬ìš©\n",
    "\n",
    "**Q2. 10000ì˜ ì˜ë¯¸**\n",
    "> - ì£¼íŒŒìˆ˜ ë²”ìœ„ ê²°ì •\n",
    "> - ë‚®ì€ ì°¨ì›: ì§§ì€ ì£¼ê¸° (ìœ„ì¹˜ ë¯¼ê°)\n",
    "> - ë†’ì€ ì°¨ì›: ê¸´ ì£¼ê¸° (ìœ„ì¹˜ ëœ ë¯¼ê°)\n",
    "> - 10000 = ì‹¤í—˜ì ìœ¼ë¡œ ì¢‹ì€ ê°’\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## êµ¬í˜„ ì •ë‹µ\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ‘‰ í´ë¦­í•˜ì—¬ ì •ë‹µ ì½”ë“œ ë³´ê¸°</summary>\n",
    "\n",
    "```python\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        \n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len, :]\n",
    "        return self.dropout(x)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
