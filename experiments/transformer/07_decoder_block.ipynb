{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 07. Transformer Decoder Block êµ¬í˜„\n",
        "\n",
        "## í•™ìŠµ ëª©í‘œ\n",
        "- Causal Mask (Autoregressive Mask) ì´í•´ ë° êµ¬í˜„\n",
        "- Encoder-Decoder vs Decoder-only êµ¬ì¡° ì°¨ì´ ì´í•´\n",
        "- GPT ìŠ¤íƒ€ì¼ Decoder Block êµ¬í˜„\n",
        "\n",
        "## ê´€ë ¨ ë©´ì ‘ ì§ˆë¬¸\n",
        "- GPTëŠ” ì™œ Decoder-only êµ¬ì¡°ì¸ê°€ìš”?\n",
        "- Causal Maskê°€ ì™œ í•„ìš”í•œê°€ìš”?\n",
        "- Encoder-Decoderì™€ Decoder-onlyì˜ ì°¨ì´ì ì€?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Pre-Quiz\n",
        "\n",
        "### Q1. GPTê°€ Decoder-onlyë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ”?\n",
        "```\n",
        "ë‹¹ì‹ ì˜ ë‹µ: \n",
        "```\n",
        "\n",
        "### Q2. Causal Maskì˜ ì—­í• ì€?\n",
        "```\n",
        "ë‹¹ì‹ ì˜ ë‹µ: \n",
        "```\n",
        "\n",
        "### Q3. Encoder-Decoder (T5) vs Decoder-only (GPT)ì˜ ìš©ë„ ì°¨ì´ëŠ”?\n",
        "```\n",
        "ë‹¹ì‹ ì˜ ë‹µ: \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from typing import Optional\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# matplotlib í•œê¸€ ì„¤ì • (macOS)\n",
        "plt.rcParams['font.family'] = 'AppleGothic'\n",
        "plt.rcParams['axes.unicode_minus'] = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì´ì „ì— êµ¬í˜„í•œ ì»´í¬ë„ŒíŠ¸ë“¤ (ì œê³µ)\n",
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.bmm(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask, float('-inf'))\n",
        "    weights = F.softmax(scores, dim=-1)\n",
        "    return torch.bmm(weights, value), weights\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        \n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "        seq_q, seq_k = query.size(1), key.size(1)\n",
        "        \n",
        "        Q = self.W_q(query).view(batch_size, seq_q, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.W_k(key).view(batch_size, seq_k, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.W_v(value).view(batch_size, seq_k, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        \n",
        "        Q = Q.contiguous().view(batch_size * self.num_heads, seq_q, self.d_k)\n",
        "        K = K.contiguous().view(batch_size * self.num_heads, seq_k, self.d_k)\n",
        "        V = V.contiguous().view(batch_size * self.num_heads, seq_k, self.d_k)\n",
        "        \n",
        "        if mask is not None and mask.dim() == 2:\n",
        "            mask = mask.unsqueeze(0).expand(batch_size * self.num_heads, -1, -1)\n",
        "        \n",
        "        output, weights = scaled_dot_product_attention(Q, K, V, mask)\n",
        "        \n",
        "        output = output.view(batch_size, self.num_heads, seq_q, self.d_k)\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_q, self.d_model)\n",
        "        \n",
        "        return self.dropout(self.W_o(output)), weights.view(batch_size, self.num_heads, seq_q, seq_k)\n",
        "\n",
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff=None, dropout=0.1):\n",
        "        super().__init__()\n",
        "        if d_ff is None:\n",
        "            d_ff = d_model * 4\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.dropout(self.fc2(self.dropout(F.gelu(self.fc1(x)))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Causal Mask ì´í•´\n",
        "\n",
        "### Autoregressive ëª¨ë¸ì˜ í•µì‹¬\n",
        "GPT ê°™ì€ ëª¨ë¸ì€ ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•  ë•Œ **ë¯¸ë˜ í† í°ì„ ë³´ë©´ ì•ˆë¨**\n",
        "\n",
        "```\n",
        "ì…ë ¥: \"ë‚˜ëŠ” ë°¥ì„ ___\"\n",
        "ì˜ˆì¸¡: \"ë¨¹ì—ˆë‹¤\"\n",
        "\n",
        "\"ë°¥ì„\" ì˜ˆì¸¡ ì‹œ \"___\"ë¥¼ ë³´ë©´ ì¹˜íŒ…!\n",
        "```\n",
        "\n",
        "### Causal Mask êµ¬ì¡° (seq_len=4)\n",
        "```\n",
        "        Key positions\n",
        "Query   [0]  [1]  [2]  [3]\n",
        "[0]      O    X    X    X    â† 0ë²ˆ ìœ„ì¹˜ëŠ” 0ë²ˆë§Œ ë³¼ ìˆ˜ ìˆìŒ\n",
        "[1]      O    O    X    X    â† 1ë²ˆ ìœ„ì¹˜ëŠ” 0,1ë§Œ ë³¼ ìˆ˜ ìˆìŒ\n",
        "[2]      O    O    O    X    â† 2ë²ˆ ìœ„ì¹˜ëŠ” 0,1,2ë§Œ ë³¼ ìˆ˜ ìˆìŒ\n",
        "[3]      O    O    O    O    â† 3ë²ˆ ìœ„ì¹˜ëŠ” ëª¨ë‘ ë³¼ ìˆ˜ ìˆìŒ\n",
        "\n",
        "O = attend ê°€ëŠ¥ (False in mask)\n",
        "X = attend ë¶ˆê°€ (True in mask, -infë¡œ ì„¤ì •)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =================================================\n",
        "# TODO: Causal Mask ìƒì„± í•¨ìˆ˜ êµ¬í˜„\n",
        "# =================================================\n",
        "#\n",
        "# íŒíŠ¸:\n",
        "# - torch.triu(tensor, diagonal=1)ì„ ì‚¬ìš©í•˜ë©´ ìƒì‚¼ê° í–‰ë ¬ ìƒì„±\n",
        "# - diagonal=1: ëŒ€ê°ì„  í¬í•¨í•˜ì§€ ì•ŠìŒ (ëŒ€ê°ì„  ìœ„ë§Œ True)\n",
        "# - ê²°ê³¼ë¥¼ bool íƒ€ì…ìœ¼ë¡œ ë³€í™˜\n",
        "#\n",
        "# =================================================\n",
        "\n",
        "def create_causal_mask(seq_len: int, device: torch.device = None) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Causal (autoregressive) mask ìƒì„±\n",
        "    \n",
        "    Args:\n",
        "        seq_len: ì‹œí€€ìŠ¤ ê¸¸ì´\n",
        "        device: í…ì„œê°€ ìœ„ì¹˜í•  device\n",
        "        \n",
        "    Returns:\n",
        "        mask: (seq_len, seq_len) bool tensor\n",
        "              Trueì¸ ìœ„ì¹˜ëŠ” attentionì—ì„œ ì œì™¸ë¨\n",
        "              \n",
        "    ì˜ˆì‹œ (seq_len=4):\n",
        "        [[False,  True,  True,  True],\n",
        "         [False, False,  True,  True],\n",
        "         [False, False, False,  True],\n",
        "         [False, False, False, False]]\n",
        "    \"\"\"\n",
        "    # ============================================\n",
        "    # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
        "    # 1. ones í–‰ë ¬ ìƒì„± (seq_len, seq_len)\n",
        "    # 2. torch.triuë¡œ ìƒì‚¼ê° ì¶”ì¶œ (diagonal=1)\n",
        "    # 3. bool íƒ€ì…ìœ¼ë¡œ ë³€í™˜\n",
        "    # ============================================\n",
        "    \n",
        "    pass  # ì´ ì¤„ì„ ì§€ìš°ê³  êµ¬í˜„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============ í…ŒìŠ¤íŠ¸ ============\n",
        "def test_causal_mask():\n",
        "    mask = create_causal_mask(4)\n",
        "    \n",
        "    expected = torch.tensor([\n",
        "        [False,  True,  True,  True],\n",
        "        [False, False,  True,  True],\n",
        "        [False, False, False,  True],\n",
        "        [False, False, False, False]\n",
        "    ])\n",
        "    \n",
        "    assert torch.equal(mask, expected), f\"Mask ì˜¤ë¥˜:\\n{mask}\"\n",
        "    print(\"âœ… Causal Mask í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
        "    print(f\"\\nMask (seq_len=4):\\n{mask.int()}\")\n",
        "\n",
        "test_causal_mask()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Causal Mask ì‹œê°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Causal Mask ì‹œê°í™”\n",
        "def visualize_causal_mask(seq_len=8):\n",
        "    mask = create_causal_mask(seq_len)\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    \n",
        "    # 1. Mask ìì²´\n",
        "    im1 = axes[0].imshow(mask.int().numpy(), cmap='RdYlGn_r')\n",
        "    axes[0].set_title('Causal Mask\\n(ë¹¨ê°•=ë§ˆìŠ¤í‚¹, ì´ˆë¡=ë³¼ ìˆ˜ ìˆìŒ)')\n",
        "    axes[0].set_xlabel('Key Position')\n",
        "    axes[0].set_ylabel('Query Position')\n",
        "    plt.colorbar(im1, ax=axes[0])\n",
        "    \n",
        "    # 2. Attention ì ìš© í›„ (ì˜ˆì‹œ)\n",
        "    torch.manual_seed(42)\n",
        "    q = torch.randn(1, seq_len, 64)\n",
        "    k = torch.randn(1, seq_len, 64)\n",
        "    v = torch.randn(1, seq_len, 64)\n",
        "    \n",
        "    _, weights = scaled_dot_product_attention(q, k, v, mask.unsqueeze(0))\n",
        "    \n",
        "    im2 = axes[1].imshow(weights[0].numpy(), cmap='Blues')\n",
        "    axes[1].set_title('Attention Weights with Causal Mask\\n(ë¯¸ë˜ í† í° attention = 0)')\n",
        "    axes[1].set_xlabel('Key Position')\n",
        "    axes[1].set_ylabel('Query Position')\n",
        "    plt.colorbar(im2, ax=axes[1])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_causal_mask()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Decoder Block êµ¬í˜„\n",
        "\n",
        "### GPT ìŠ¤íƒ€ì¼ Decoder (Pre-LN)\n",
        "```\n",
        "x â†’ LayerNorm â†’ Masked Self-Attention â†’ + â†’ LayerNorm â†’ FFN â†’ + â†’ output\n",
        "                                        â†‘                     â†‘\n",
        "    â†‘___________________________________|   â†‘_________________|\n",
        "              (residual)                      (residual)\n",
        "```\n",
        "\n",
        "### Encoder Blockê³¼ì˜ ì°¨ì´ì \n",
        "| í•­ëª© | Encoder Block | Decoder Block (GPT) |\n",
        "|------|--------------|--------------------|\n",
        "| Mask | ì„ íƒì  (íŒ¨ë”©ìš©) | í•„ìˆ˜ (Causal) |\n",
        "| Cross-Attention | ì—†ìŒ | ì—†ìŒ (Decoder-only) |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =================================================\n",
        "# TODO: Decoder Block êµ¬í˜„ (Pre-LN, GPT ìŠ¤íƒ€ì¼)\n",
        "# =================================================\n",
        "#\n",
        "# Encoder Blockê³¼ ê±°ì˜ ë™ì¼í•˜ì§€ë§Œ:\n",
        "# - forwardì—ì„œ causal maskë¥¼ í•­ìƒ ìƒì„±í•˜ì—¬ ì ìš©\n",
        "# - ì¶”ê°€ maskê°€ ìˆìœ¼ë©´ OR ì—°ì‚°ìœ¼ë¡œ ê²°í•©\n",
        "#\n",
        "# =================================================\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        num_heads: int,\n",
        "        d_ff: int = None,\n",
        "        dropout: float = 0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \n",
        "        # ============================================\n",
        "        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
        "        # - attention: MultiHeadAttention\n",
        "        # - ffn: PositionWiseFeedForward\n",
        "        # - norm1, norm2: LayerNorm\n",
        "        # - dropout\n",
        "        # ============================================\n",
        "        \n",
        "        pass  # ì´ ì¤„ì„ ì§€ìš°ê³  êµ¬í˜„\n",
        "    \n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        mask: Optional[torch.Tensor] = None\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch, seq, d_model)\n",
        "            mask: ì¶”ê°€ mask (optional, causal maskì™€ ê²°í•©)\n",
        "        Returns:\n",
        "            output: (batch, seq, d_model)\n",
        "        \"\"\"\n",
        "        # ============================================\n",
        "        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
        "        # 1. causal mask ìƒì„± (xì˜ seq_len ì‚¬ìš©)\n",
        "        # 2. ì¶”ê°€ maskê°€ ìˆìœ¼ë©´ OR ì—°ì‚°ìœ¼ë¡œ ê²°í•©\n",
        "        # 3. Pre-LN + Attention + Residual\n",
        "        # 4. Pre-LN + FFN + Residual\n",
        "        # ============================================\n",
        "        \n",
        "        pass  # ì´ ì¤„ì„ ì§€ìš°ê³  êµ¬í˜„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============ í…ŒìŠ¤íŠ¸ ============\n",
        "def test_decoder_block():\n",
        "    batch, seq, d_model, num_heads = 2, 10, 64, 8\n",
        "    \n",
        "    decoder = DecoderBlock(d_model, num_heads)\n",
        "    x = torch.randn(batch, seq, d_model)\n",
        "    \n",
        "    output = decoder(x)\n",
        "    \n",
        "    assert output.shape == x.shape, f\"Shape ì˜¤ë¥˜: {output.shape}\"\n",
        "    print(\"âœ… Decoder Block í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
        "    print(f\"   Input: {x.shape} â†’ Output: {output.shape}\")\n",
        "\n",
        "test_decoder_block()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Transformer Decoder (ì „ì²´ ìŠ¤íƒ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =================================================\n",
        "# TODO: Transformer Decoder êµ¬í˜„\n",
        "# =================================================\n",
        "#\n",
        "# êµ¬ì¡°:\n",
        "# token_ids â†’ Embedding â†’ + PE â†’ Dropout â†’ DecoderBlockÃ—N â†’ LayerNorm â†’ Linear â†’ logits\n",
        "#\n",
        "# =================================================\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        d_model: int,\n",
        "        num_heads: int,\n",
        "        num_layers: int,\n",
        "        d_ff: int = None,\n",
        "        max_len: int = 512,\n",
        "        dropout: float = 0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \n",
        "        # ============================================\n",
        "        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
        "        # - token_embedding: nn.Embedding(vocab_size, d_model)\n",
        "        # - position_embedding: nn.Embedding(max_len, d_model)\n",
        "        # - dropout: nn.Dropout\n",
        "        # - layers: nn.ModuleList of DecoderBlock\n",
        "        # - norm: nn.LayerNorm (final)\n",
        "        # - lm_head: nn.Linear(d_model, vocab_size)\n",
        "        # ============================================\n",
        "        \n",
        "        pass  # ì´ ì¤„ì„ ì§€ìš°ê³  êµ¬í˜„\n",
        "    \n",
        "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch, seq) - token ids\n",
        "            mask: optional additional mask\n",
        "        Returns:\n",
        "            logits: (batch, seq, vocab_size)\n",
        "        \"\"\"\n",
        "        # ============================================\n",
        "        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”\n",
        "        # 1. token embedding\n",
        "        # 2. position ids ìƒì„± (0, 1, 2, ..., seq_len-1)\n",
        "        # 3. position embedding ë”í•˜ê¸°\n",
        "        # 4. dropout\n",
        "        # 5. decoder blocks í†µê³¼\n",
        "        # 6. final layer norm\n",
        "        # 7. lm_headë¡œ logits ìƒì„±\n",
        "        # ============================================\n",
        "        \n",
        "        pass  # ì´ ì¤„ì„ ì§€ìš°ê³  êµ¬í˜„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============ í…ŒìŠ¤íŠ¸ ============\n",
        "def test_transformer_decoder():\n",
        "    vocab_size = 1000\n",
        "    batch, seq, d_model, num_heads, num_layers = 2, 10, 64, 8, 4\n",
        "    \n",
        "    decoder = TransformerDecoder(\n",
        "        vocab_size=vocab_size,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        num_layers=num_layers\n",
        "    )\n",
        "    \n",
        "    # ëœë¤ í† í° ID ìƒì„±\n",
        "    token_ids = torch.randint(0, vocab_size, (batch, seq))\n",
        "    \n",
        "    logits = decoder(token_ids)\n",
        "    \n",
        "    assert logits.shape == (batch, seq, vocab_size), f\"Shape ì˜¤ë¥˜: {logits.shape}\"\n",
        "    print(\"âœ… Transformer Decoder í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
        "    print(f\"   Input: {token_ids.shape} â†’ Output: {logits.shape}\")\n",
        "    print(f\"   Total parameters: {sum(p.numel() for p in decoder.parameters()):,}\")\n",
        "\n",
        "test_transformer_decoder()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Encoder vs Decoder ë¹„êµ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encoder vs Decoder Attention íŒ¨í„´ ë¹„êµ\n",
        "def compare_encoder_decoder_attention():\n",
        "    seq_len = 8\n",
        "    d_model = 64\n",
        "    \n",
        "    torch.manual_seed(42)\n",
        "    x = torch.randn(1, seq_len, d_model)\n",
        "    \n",
        "    # Encoder (no mask)\n",
        "    q, k, v = x, x, x\n",
        "    _, encoder_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask=None\n",
        "    )\n",
        "    \n",
        "    # Decoder (causal mask)\n",
        "    causal_mask = create_causal_mask(seq_len).unsqueeze(0)\n",
        "    _, decoder_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask=causal_mask\n",
        "    )\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    \n",
        "    im1 = axes[0].imshow(encoder_weights[0].numpy(), cmap='Blues')\n",
        "    axes[0].set_title('Encoder (Bidirectional)\\nëª¨ë“  ìœ„ì¹˜ë¥¼ ë³¼ ìˆ˜ ìˆìŒ')\n",
        "    axes[0].set_xlabel('Key Position')\n",
        "    axes[0].set_ylabel('Query Position')\n",
        "    plt.colorbar(im1, ax=axes[0])\n",
        "    \n",
        "    im2 = axes[1].imshow(decoder_weights[0].numpy(), cmap='Blues')\n",
        "    axes[1].set_title('Decoder (Causal/Unidirectional)\\nê³¼ê±° ìœ„ì¹˜ë§Œ ë³¼ ìˆ˜ ìˆìŒ')\n",
        "    axes[1].set_xlabel('Key Position')\n",
        "    axes[1].set_ylabel('Query Position')\n",
        "    plt.colorbar(im2, ax=axes[1])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "compare_encoder_decoder_attention()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Post-Quiz\n",
        "\n",
        "### Q1. ìœ„ ì‹œê°í™”ì—ì„œ Decoder attentionì˜ ìƒì‚¼ê° ë¶€ë¶„ì´ 0ì¸ ì´ìœ ëŠ”?\n",
        "```\n",
        "ë‹¹ì‹ ì˜ ë‹µ: \n",
        "```\n",
        "\n",
        "### Q2. BERTëŠ” ì™œ Encoderë§Œ ì‚¬ìš©í•˜ë‚˜ìš”?\n",
        "```\n",
        "ë‹¹ì‹ ì˜ ë‹µ: \n",
        "```\n",
        "\n",
        "### Q3. T5 ê°™ì€ Encoder-Decoder ëª¨ë¸ì—ì„œ Cross-Attentionì€ ì–´ë””ì— ìœ„ì¹˜í•˜ë‚˜ìš”?\n",
        "```\n",
        "ë‹¹ì‹ ì˜ ë‹µ: \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## ì •ë‹µ\n",
        "\n",
        "<details>\n",
        "<summary>ğŸ‘‰ í´ë¦­í•˜ì—¬ ì •ë‹µ ì½”ë“œ ë³´ê¸°</summary>\n",
        "\n",
        "```python\n",
        "def create_causal_mask(seq_len: int, device: torch.device = None) -> torch.Tensor:\n",
        "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
        "    if device is not None:\n",
        "        mask = mask.to(device)\n",
        "    return mask\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff=None, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.ffn = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        seq_len = x.size(1)\n",
        "        \n",
        "        # Causal mask ìƒì„±\n",
        "        causal_mask = create_causal_mask(seq_len, x.device)\n",
        "        \n",
        "        # ì¶”ê°€ maskì™€ ê²°í•© (OR ì—°ì‚°)\n",
        "        if mask is not None:\n",
        "            combined_mask = causal_mask | mask\n",
        "        else:\n",
        "            combined_mask = causal_mask\n",
        "        \n",
        "        # Pre-LN + Attention + Residual\n",
        "        normed = self.norm1(x)\n",
        "        attn_out, _ = self.attention(normed, normed, normed, combined_mask)\n",
        "        x = x + self.dropout(attn_out)\n",
        "        \n",
        "        # Pre-LN + FFN + Residual\n",
        "        normed = self.norm2(x)\n",
        "        ffn_out = self.ffn(normed)\n",
        "        x = x + ffn_out\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, num_layers, \n",
        "                 d_ff=None, max_len=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.position_embedding = nn.Embedding(max_len, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderBlock(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        \n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        batch, seq_len = x.shape\n",
        "        \n",
        "        # Embeddings\n",
        "        tok_emb = self.token_embedding(x)\n",
        "        pos_ids = torch.arange(seq_len, device=x.device).unsqueeze(0)\n",
        "        pos_emb = self.position_embedding(pos_ids)\n",
        "        \n",
        "        x = self.dropout(tok_emb + pos_emb)\n",
        "        \n",
        "        # Decoder blocks\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        \n",
        "        # Final norm + projection\n",
        "        x = self.norm(x)\n",
        "        logits = self.lm_head(x)\n",
        "        \n",
        "        return logits\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>ğŸ‘‰ Pre-Quiz ì •ë‹µ</summary>\n",
        "\n",
        "### Q1. GPTê°€ Decoder-onlyë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ”?\n",
        "**í…ìŠ¤íŠ¸ ìƒì„±(ë‹¤ìŒ í† í° ì˜ˆì¸¡)ì— ìµœì í™”**ëœ êµ¬ì¡°ì´ê¸° ë•Œë¬¸. Encoderì˜ ì–‘ë°©í–¥ ì»¨í…ìŠ¤íŠ¸ëŠ” ìƒì„± íƒœìŠ¤í¬ì— ë¶ˆí•„ìš”í•˜ë©°, Causal maskë¡œ ë¯¸ë˜ í† í°ì„ ê°€ë¦¬ë©´ì„œ autoregressive ìƒì„±ì´ ê°€ëŠ¥.\n",
        "\n",
        "### Q2. Causal Maskì˜ ì—­í• ì€?\n",
        "**ë¯¸ë˜ í† í°ì— ëŒ€í•œ attentionì„ ì°¨ë‹¨**í•˜ì—¬ ëª¨ë¸ì´ ì¹˜íŒ…í•˜ì§€ ì•Šë„ë¡ í•¨. í•™ìŠµ ì‹œ teacher forcingìœ¼ë¡œ ì •ë‹µì„ ì£¼ì§€ë§Œ, ê° ìœ„ì¹˜ì—ì„œëŠ” ì´ì „ í† í°ë§Œ ì°¸ê³ í•˜ë„ë¡ ê°•ì œ.\n",
        "\n",
        "### Q3. Encoder-Decoder vs Decoder-only ìš©ë„ ì°¨ì´ëŠ”?\n",
        "- **Encoder-Decoder (T5, BART)**: ë²ˆì—­, ìš”ì•½ ë“± ì…ë ¥â†’ì¶œë ¥ ë³€í™˜ íƒœìŠ¤í¬ì— ì í•©. ì…ë ¥ ì „ì²´ë¥¼ ì–‘ë°©í–¥ìœ¼ë¡œ ì´í•´ í›„ ì¶œë ¥ ìƒì„±.\n",
        "- **Decoder-only (GPT)**: í…ìŠ¤íŠ¸ ìƒì„±, ëŒ€í™” ë“± ì—°ì† ìƒì„± íƒœìŠ¤í¬ì— ì í•©. í”„ë¡¬í”„íŠ¸ ì´ì–´ì“°ê¸° í˜•íƒœ.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>ğŸ‘‰ Post-Quiz ì •ë‹µ</summary>\n",
        "\n",
        "### Q1. ìƒì‚¼ê° ë¶€ë¶„ì´ 0ì¸ ì´ìœ \n",
        "Causal maskê°€ ìƒì‚¼ê° ë¶€ë¶„(ë¯¸ë˜ ìœ„ì¹˜)ì„ -infë¡œ ì„¤ì •í•˜ê¸° ë•Œë¬¸. softmax(-inf) = 0ì´ ë˜ì–´ ë¯¸ë˜ í† í°ì— ëŒ€í•œ attention weightê°€ 0.\n",
        "\n",
        "### Q2. BERTê°€ Encoderë§Œ ì‚¬ìš©í•˜ëŠ” ì´ìœ \n",
        "BERTëŠ” **ì–‘ë°©í–¥ ì»¨í…ìŠ¤íŠ¸ ì´í•´**ê°€ ëª©ì (MLM: Masked Language Modeling). ë¬¸ì¥ì˜ ì•ë’¤ë¥¼ ëª¨ë‘ ë³´ê³  ë¹ˆì¹¸ì„ ì±„ìš°ë¯€ë¡œ causal maskê°€ ì˜¤íˆë ¤ ì„±ëŠ¥ ì €í•˜. ìƒì„±ì´ ì•„ë‹Œ ë¶„ë¥˜/ì¶”ì¶œ íƒœìŠ¤í¬ì— ìµœì í™”.\n",
        "\n",
        "### Q3. Cross-Attention ìœ„ì¹˜\n",
        "Encoder-Decoder ëª¨ë¸ì˜ **Decoder Block ë‚´ë¶€**, Self-Attentionê³¼ FFN ì‚¬ì´ì— ìœ„ì¹˜. Encoder ì¶œë ¥(K, V)ê³¼ Decoder ìƒíƒœ(Q)ë¥¼ ì—°ê²°í•˜ì—¬ ì…ë ¥ ì •ë³´ë¥¼ ì°¸ì¡°.\n",
        "\n",
        "</details>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
