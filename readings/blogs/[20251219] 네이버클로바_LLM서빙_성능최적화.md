# 토큰 한 알의 질주: LLM 서빙의 모든 것 (2)

- **출처**: [CLOVA 기술 블로그](https://clova.ai/tech-blog/토큰-한-알의-질주-llm-서빙의-모든-것-2)
- **저자**: 네이버클라우드 CLOVA 팀
- **읽은 날짜**: 2025-12-19
- **태그**: #LLM #서빙 #KVCache #Goodput #성능최적화

## 핵심 내용

LLM 서빙 시스템의 성능 지표, 병목 지점, 최적화 전략을 체계적으로 정리.

## 1. 핵심 성능 지표

### Latency (지연 시간)
| 지표 | 설명 |
|------|------|
| **TTFT** | Time to First Token - 첫 토큰까지 시간 |
| **TPOT** | Time Per Output Token - 토큰당 생성 시간 |

```
총 응답 시간 = TTFT + (TPOT × 생성 토큰 수)
```

### Throughput (처리량)
- 초당 생성 토큰 수
- **Latency ↔ Throughput 트레이드오프** 존재

### Goodput (유효 처리량)
> "SLO(서비스 지연 기준)를 통과한 응답만 집계한 실제 유효 응답량"

- 실전 성능 측정의 핵심 지표
- 차트에서 **좌상단**(낮은 지연 + 높은 처리량)이 최적

## 2. KV Cache와 메모리 관리

### KV Cache란?
> "어텐션에서 계산한 K, V를 GPU 메모리에 저장 → 매 토큰마다 재계산 불필요"

### 메모리 사용량 공식
```
KV Cache = 배치크기 × 시퀀스길이 × 레이어수 × 히든사이즈 × 2 × 데이터타입크기
```

### 트레이드오프
- 배치 크기 ↑ → KV 캐시 ↑ → 시퀀스 길이 제한
- 시퀀스 길이 ↑ → 배치 크기 제한

## 3. 병목 지점 분석

| 단계 | 병목 유형 | 설명 |
|------|----------|------|
| **Prefill** | Compute-bound | GPU 코어의 계산 속도가 성능 결정 |
| **Decode** | Memory-bound | 메모리 I/O 속도가 성능 좌우 |

## 4. 병목 해결 전략

### 하드웨어
| 방법 | 효과 |
|------|------|
| HBM (고속 메모리) | 파라미터/KV 캐시 접근 속도 ↑ |
| Tensor Core | 행렬 연산 고속화 (Prefill 개선) |
| 다중 GPU | 메모리 대역폭 ↑ (but 통신 병목 주의) |

### 알고리즘
- **Speculative Decoding**: 소형 모델 초안 → 대형 모델 검증
- 조건부 계산 (Conditional Computation)
- 캐시 최적화 (PagedAttention 등)
- 디코딩 전략 조정

## 5. 최종 목표: 비용 효율성

> **"Tokens per Dollar"** - 달러당 처리 가능한 토큰 수 극대화

- 최고 성능 GPU ≠ 최적의 선택
- **Goodput 목표를 가장 저렴하게 달성**하는 조합 찾기

## 인상 깊은 부분

### Prefill vs Decode 병목 차이
- 같은 Transformer인데 **단계별로 병목이 다름**
- Prefill: 한 번에 많은 토큰 처리 → 연산량 큼 → Compute-bound
- Decode: 토큰 하나씩 → 가중치 읽기 반복 → Memory-bound

### Goodput의 실용성
- 단순 Throughput은 실전과 동떨어짐
- SLO 기반 Goodput이 **실제 사용자 경험** 반영

## 내 생각 / 적용점

### 배운 점
1. **TTFT vs TPOT 분리**해서 봐야 함
   - 첫 응답 빠른데 이후 느린 경우 / 반대 경우 구분
2. **KV 캐시 = 메모리 전쟁**
   - 긴 컨텍스트 원하면 배치 줄여야
3. **Goodput = 비즈니스 지표**
   - 기술 지표(Throughput)와 별개로 SLO 달성률 봐야

### 실무 연결
- vLLM, TGI 사용 시 메트릭 대시보드 구성
  - TTFT, TPOT, KV 캐시 사용률, SLO 달성률
- 배치 크기 vs 시퀀스 길이 트레이드오프 설정
- GPU 선택 시 메모리 대역폭도 고려

### 후속 학습
- [ ] PagedAttention 동작 원리
- [ ] vLLM 메트릭 모니터링 실습
- [ ] Flash Attention과의 관계
- [ ] 1편 내용도 읽기
